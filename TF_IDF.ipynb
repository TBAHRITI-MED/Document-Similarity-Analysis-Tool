{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8505776",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/mohamedtbh/Desktop/M1_IA/TF_IDF/Eval_Formation_Etranger_fr.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# tp avec le text en francais et en anglais \u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m \n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/mohamedtbh/Desktop/M1_IA/TF_IDF/Eval_Formation_Etranger_fr.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m      6\u001b[0m     content \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(content)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/mohamedtbh/Desktop/M1_IA/TF_IDF/Eval_Formation_Etranger_fr.txt'"
     ]
    }
   ],
   "source": [
    "# tp avec le text en francais et en anglais \n",
    "\n",
    "import re \n",
    "\n",
    "with open(\"/Users/mohamedtbh/Desktop/M1_IA/TF_IDF/Eval_Formation_Etranger_fr.txt\", \"r\") as file:\n",
    "    content = file.read()\n",
    "print(content)\n",
    "\n",
    "content_splited = re.split(r\"[.]\" , content)\n",
    "\n",
    "liste_of_sentences = list(content_splited)\n",
    "\n",
    "print(f\" the content splited :  {content_splited}\")\n",
    "\n",
    "\n",
    "list_of_tokens = []\n",
    "\n",
    "for sentence in liste_of_sentences:\n",
    "    tokens = sentence.split()\n",
    "    list_of_tokens.append(tokens) \n",
    "    \n",
    "print(list_of_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb46ce62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obama\n",
      "sentences obama : ['If there is anyone out there who still doubts that America is a place where all things are possible who still wonders if the dream of our founders is alive in our time who still questions the power of our democracy tonight is your answer', 'Its the answer told by lines that stretched around schools and churches in numbers this nation has never seen by people who waited three hours and four hours many for the very first time in their lives because they believed that this time must be different that their voice could be that difference', 'Its the answer spoken by young and old rich and poor Democrat and Republican black white Latino Asian Native American gay straight disabled and not disabled  Americans who sent a message to the world that we have never been a collection of red states and blue states we are and always will be the United States of America', 'Its the answer that led those who have been told for so long by so many to be cynical and fearful and doubtful of what we can achieve to put their hands on the arc of history and bend it once more toward the hope of a better day', 'Its been a long time coming but tonight because of what we did on this day in this election at this defining moment change has come to America', 'I just received a very gracious call from Sen', 'McCain', 'He fought long and hard in this campaign and hes fought even longer and harder for the country he loves', 'He has endured sacrifices for America that most of us cannot begin to imagine and we are better off for the service rendered by this brave and selfless leader', 'I congratulate him and Gov', 'Palin for all they have achieved and I look forward to working with them to renew this nations promise in the months ahead', 'I want to thank my partner in this journey a man who campaigned from his heart and spoke for the men and women he grew up with on the streets of Scranton and rode with on that train home to Delaware the vicepresidentelect of the United States Joe Biden', 'I would not be standing here tonight without the unyielding support of my best friend for the last  years the rock of our family and the love of my life our nations next first lady Michelle Obama', 'Sasha and Malia I love you both so much and you have earned the new puppy thats coming with us to the White House', 'And while shes no longer with us I know my grandmother is watching along with the family that made me who I am', 'I miss them tonight and know that my debt to them is beyond measure', 'To my campaign manager David Plouffe my chief strategist David Axelrod and the best campaign team ever assembled in the history of politics  you made this happen and I am forever grateful for what youve sacrificed to get it done', 'But above all I will never forget who this victory truly belongs to  it belongs to you', 'I was never the likeliest candidate for this office', 'We didnt start with much money or many endorsements', 'Our campaign was not hatched in the halls of Washington  it began in the backyards of Des Moines and the living rooms of Concord and the front porches of Charleston', 'It was built by working men and women who dug into what little savings they had to give  and  and  to this cause', 'It grew strength from the young people who rejected the myth of their generations apathy who left their homes and their families for jobs that offered little pay and less sleep from the notsoyoung people who braved the bitter cold and scorching heat to knock on the doors of perfect strangers from the millions of Americans who volunteered and organized and proved that more than two centuries later a government of the people by the people and for the people has not perished from this earth', 'This is your victory', 'I know you didnt do this just to win an election and I know you didnt do it for me', 'You did it because you understand the enormity of the task that lies ahead', 'For even as we celebrate tonight we know the challenges that tomorrow will bring are the greatest of our lifetime  two wars a planet in peril the worst financial crisis in a century', 'Even as we stand here tonight we know there are brave Americans waking up in the deserts of Iraq and the mountains of Afghanistan to risk their lives for us', 'There are mothers and fathers who will lie awake after their children fall asleep and wonder how theyll make the mortgage or pay their doctors bills or save enough for college', 'There is new energy to harness and new jobs to be created new schools to build and threats to meet and alliances to repair', 'The road ahead will be long', 'Our climb will be steep', 'We may not get there in one year or even one term but America  I have never been more hopeful than I am tonight that we will get there', 'I promise you We as a people will get there', 'There will be setbacks and false starts', 'There are many who wont agree with every decision or policy I make as president and we know that government cant solve every problem', 'But I will always be honest with you about the challenges we face', 'I will listen to you especially when we disagree', 'And above all I will ask you join in the work of remaking this nation the only way its been done in America for  years  block by block brick by brick callused hand by callused hand', 'What began  months ago in the depths of winter must not end on this autumn night', 'This victory alone is not the change we seek  it is only the chance for us to make that change', 'And that cannot happen if we go back to the way things were', 'It cannot happen without you', 'So let us summon a new spirit of patriotism of service and responsibility where each of us resolves to pitch in and work harder and look after not only ourselves but each other', 'Let us remember that if this financial crisis taught us anything its that we cannot have a thriving Wall Street while Main Street suffers', 'In this country we rise or fall as one nation  as one people', 'Let us resist the temptation to fall back on the same partisanship and pettiness and immaturity that has poisoned our politics for so long', 'Let us remember that it was a man from this state who first carried the banner of the Republican Party to the White House  a party founded on the values of selfreliance individual liberty and national unity', 'Those are values we all share and while the Democratic Party has won a great victory tonight we do so with a measure of humility and determination to heal the divides that have held back our progress', 'As Lincoln said to a nation far more divided than ours We are not enemies but friends', 'Though passion may have strained it must not break our bonds of affection And to those Americans whose support I have yet to earn I may not have won your vote but I hear your voices I need your help and I will be your president too', 'And to all those watching tonight from beyond our shores from parliaments and palaces to those who are huddled around radios in the forgotten corners of our world  our stories are singular but our destiny is shared and a new dawn of American leadership is at hand', 'To those who would tear this world down We will defeat you', 'To those who seek peace and security We support you', 'And to all those who have wondered if Americas beacon still burns as bright Tonight we proved once more that the true strength of our nation comes not from the might of our arms or the scale of our wealth but from the enduring power of our ideals democracy liberty opportunity and unyielding hope', 'For that is the true genius of America  that America can change', 'Our union can be perfected', 'And what we have already achieved gives us hope for what we can and must achieve tomorrow', 'This election had many firsts and many stories that will be told for generations', 'But one thats on my mind tonight is about a woman who cast her ballot in Atlanta', 'Shes a lot like the millions of others who stood in line to make their voice heard in this election except for one thing Ann Nixon Cooper is  years old', 'She was born just a generation past slavery a time when there were no cars on the road or planes in the sky when someone like her couldnt vote for two reasons  because she was a woman and because of the color of her skin', 'And tonight I think about all that shes seen throughout her century in America  the heartache and the hope the struggle and the progress the times we were told that we cant and the people who pressed on with that American creed Yes we can', 'At a time when womens voices were silenced and their hopes dismissed she lived to see them stand up and speak out and reach for the ballot', 'Yes we can', 'When there was despair in the Dust Bowl and depression across the land she saw a nation conquer fear itself with a New Deal new jobs and a new sense of common purpose', 'Yes we can', 'When the bombs fell on our harbor and tyranny threatened the world she was there to witness a generation rise to greatness and a democracy was saved', 'Yes we can', 'She was there for the buses in Montgomery the hoses in Birmingham a bridge in Selma and a preacher from Atlanta who told a people that We Shall Overcome Yes we can', 'A man touched down on the moon a wall came down in Berlin a world was connected by our own science and imagination', 'And this year in this election she touched her finger to a screen and cast her vote because after  years in America through the best of times and the darkest of hours she knows how America can change', 'Yes we can', 'America we have come so far', 'We have seen so much', 'But there is so much more to do', 'So tonight let us ask ourselves If our children should live to see the next century if my daughters should be so lucky to live as long as Ann Nixon Cooper what change will they see', 'What progress will we have made', 'This is our chance to answer that call', 'This is our moment', 'This is our time  to put our people back to work and open doors of opportunity for our kids to restore prosperity and promote the cause of peace to reclaim the American Dream and reaffirm that fundamental truth that out of many we are one that while we breathe we hope and where we are met with cynicism and doubt and those who tell us that we cant we will respond with that timeless creed that sums up the spirit of a people Yes we can', 'Thank you God bless you and may God bless the United States of America']\n",
      "tokens obama : ['land', 'americans', 'buses', 'prosperity', 'get', 'enough', 'enduring', 'bitter', 'main', 'homes', 'college', 'love', 'tonight', 'journey', 'comes', 'anyone', 'preacher', 'working', 'to', 'sense', 'nation', 'montgomery', 'came', 'join', 'doubt', 'stand', 'challenges', 'defining', 'temptation', 'corners', 'united', 'science', 'victory', 'american', 'though', 'hatched', 'fearful', 'myth', 'mountains', 'century', 'opportunity', 'day', 'progress', 'grew', 'beyond', 'summon', 'wonders', 'threatened', 'that', 'toward', 'streets', 'received', 'true', 'fear', 'vicepresidentelect', 'campaigned', 'would', 'term', 'across', 'creed', 'bend', 'later', 'even', 'asleep', 'heat', 'millions', 'forever', 'common', 'bills', 'country', 'time', 'just', 'joe', 'government', 'about', 'spirit', 'cooper', 'dug', 'who', 'responsibility', 'won', 'should', 'sums', 'in', 'them', 'after', 'or', 'thing', 'had', 'for', 'deal', 'from', 'wondered', 'little', 'selfless', 'along', 'peace', 'your', 'celebrate', 'ourselves', 'false', 'starts', 'dawn', 'achieve', 'you', 'halls', 'down', 'ann', 'without', 'miss', 'nixon', 'alive', 'out', 'rich', 'him', 'wall', 'greatness', 'men', 'up', 'i', 'sky', 'promise', 'likeliest', 'gracious', 'enemies', 'depths', 'rode', 'work', 'pressed', 'there', 'fall', 'earned', 'huddled', 'beacon', 'except', 'bombs', 'when', 'callused', 'pettiness', 'lot', 'led', 'answer', 'breathe', 'end', 'god', 'hopes', 'tyranny', 'only', 'saw', 'lived', 'atlanta', 'house', 'how', 'liberty', 'democrat', 'conquer', 'alliances', 'above', 'have', 'struggle', 'imagine', 'off', 'nations', 'des', 'told', 'built', 'street', 'ballot', 'radios', 'lady', 'if', 'palin', 'national', 'their', 'all', 'organized', 'enormity', 'while', 'do', 'shes', 'past', 'back', 'block', 'unity', 'could', 'people', 'lie', 'state', 'questions', 'itself', 'lines', 'unyielding', 'strength', 'whose', 'most', 'womens', 'bless', 'election', 'david', 'he', 'its', 'cold', 'ahead', 'bowl', 'face', 'heart', 'doctors', 'sacrifices', 'change', 'cause', 'planet', 'build', 'hear', 'own', 'let', 'destiny', 'pitch', 'her', 'volunteered', 'patriotism', 'truth', 'reaffirm', 'made', 'win', 'despair', 'scranton', 'as', 'where', 'darkest', 'kids', 'created', 'happen', 'on', 'apathy', 'agree', 'give', 'selfreliance', 'here', 'friends', 'leadership', 'axelrod', 'tear', 'proved', 'night', 'than', 'biden', 'children', 'singular', 'wonder', 'bridge', 'months', 'divided', 'cynicism', 'same', 'founders', 'shall', 'latino', 'manager', 'restore', 'others', 'notsoyoung', 'bring', 'policy', 'disabled', 'sen', 'best', 'especially', 'selma', 'hand', 'berlin', 'once', 'power', 'leader', 'puppy', 'train', 'young', 'native', 'each', 'perfected', 'left', 'doubts', 'carried', 'team', 'stretched', 'churches', 'a', 'believed', 'year', 'determination', 'genius', 'threats', 'but', 'imagination', 'begin', 'blue', 'immaturity', 'wealth', 'doors', 'partner', 'assembled', 'might', 'better', 'moon', 'we', 'rock', 'meet', 'measure', 'offered', 'crisis', 'iraq', 'problem', 'service', 'may', 'forward', 'can', 'still', 'resist', 'different', 'rise', 'be', 'waking', 'spoken', 'very', 'cast', 'yes', 'respond', 'individual', 'schools', 'many', 'hopeful', 'both', 'heard', 'cars', 'passion', 'loves', 'solve', 'me', 'one', 'perished', 'jobs', 'poor', 'rejected', 'earth', 'resolves', 'will', 'backyards', 'stood', 'those', 'call', 'cannot', 'lies', 'things', 'voice', 'want', 'through', 'next', 'chief', 'vote', 'hoses', 'braved', 'lifetime', 'she', 'always', 'connected', 'someone', 'dream', 'fell', 'worst', 'affection', 'look', 'way', 'around', 'shores', 'woman', 'two', 'fundamental', 'because', 'anything', 'years', 'overcome', 'four', 'collection', 'centuries', 'an', 'gay', 'savings', 'screen', 'met', 'know', 'line', 'like', 'strained', 'generations', 'alone', 'family', 'remember', 'grateful', 'financial', 'straight', 'americas', 'new', 'gives', 'moment', 'long', 'were', 'charleston', 'other', 'brave', 'share', 'greatest', 'ideals', 'security', 'democratic', 'red', 'arc', 'rendered', 'start', 'more', 'so', 'repair', 'hours', 'renew', 'thank', 'values', 'far', 'black', 'task', 'first', 'asian', 'campaign', 'poisoned', 'brick', 'man', 'into', 'heartache', 'endured', 'states', 'didnt', 'already', 'live', 'ours', 'strategist', 'every', 'chance', 'lincoln', 'saved', 'help', 'doubtful', 'living', 'what', 'decision', 'am', 'times', 'scale', 'shared', 'debt', 'arms', 'washington', 'hard', 'need', 'firsts', 'was', 'they', 'burns', 'scorching', 'dust', 'come', 'obama', 'coming', 'world', 'possible', 'mccain', 'watching', 'families', 'banner', 'harbor', 'life', 'perfect', 'think', 'not', 'plouffe', 'great', 'earn', 'fought', 'gov', 'make', 'done', 'hands', 'grandmother', 'go', 'congratulate', 'my', 'save', 'wars', 'awake', 'spoke', 'depression', 'never', 'listen', 'knock', 'thriving', 'climb', 'throughout', 'ask', 'divides', 'concord', 'tomorrow', 'theyll', 'home', 'been', 'bonds', 'stories', 'numbers', 'our', 'heal', 'defeat', 'mind', 'speak', 'republican', 'place', 'hope', 'belongs', 'voices', 'reasons', 'president', 'us', 'bright', 'three', 'timeless', 'it', 'winter', 'color', 'with', 'said', 'steep', 'wont', 'america', 'friend', 'waited', 'michelle', 'setbacks', 'thats', 'too', 'born', 'longer', 'mortgage', 'no', 'promote', 'founded', 'began', 'are', 'forget', 'sleep', 'peril', 'sasha', 'break', 'sacrificed', 'birmingham', 'history', 'parliaments', 'less', 'lives', 'politics', 'of', 'and', 'deserts', 'slavery', 'moines', 'planes', 'youve', 'silenced', 'daughters', 'humility', 'much', 'rooms', 'office', 'support', 'risk', 'party', 'difference', 'endorsements', 'forgotten', 'cynical', 'seen', 'hes', 'message', 'open', 'standing', 'union', 'tell', 'did', 'truly', 'reclaim', 'see', 'purpose', 'women', 'afghanistan', 'finger', 'energy', 'dismissed', 'old', 'autumn', 'porches', 'strangers', 'yet', 'ago', 'candidate', 'must', 'suffers', 'harness', 'put', 'at', 'understand', 'harder', 'seek', 'road', 'honest', 'democracy', 'malia', 'front', 'held', 'pay', 'partisanship', 'witness', 'the', 'knows', 'this', 'touched', 'achieved', 'lucky', 'generation', 'ever', 'his', 'by', 'palaces', 'skin', 'is', 'fathers', 'disagree', 'delaware', 'remaking', 'cant', 'couldnt', 'reach', 'white', 'taught', 'money', 'sent', 'last', 'mothers', 'has']\n",
      "82\n",
      "651\n"
     ]
    }
   ],
   "source": [
    "# tp pour le text de obama et chiraq \n",
    "\n",
    "import re\n",
    "\n",
    "def split_text_into_sentences_and_tokens(text):\n",
    "    # Split the text into sentences using punctuation (.,!?)\n",
    "    text = text.replace('�', '')\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text.strip())\n",
    "    \n",
    "    # Remove punctuation from sentences\n",
    "    sentences_no_punctuation = [re.sub(r'[.,!-?;\\'\"]', '', sentence) for sentence in sentences]\n",
    "    \n",
    "    # Create a set to store tokens and avoid duplicates\n",
    "    token_set = set()\n",
    "    \n",
    "    for sentence in sentences_no_punctuation:\n",
    "        # Extract words only, excluding punctuation\n",
    "        tokens = re.findall(r'\\b\\w+\\b', sentence)\n",
    "        token_set.update(token.lower() for token in tokens)\n",
    "    \n",
    "    # Convert the set to a list\n",
    "    tokens = list(token_set)\n",
    "    \n",
    "    return sentences_no_punctuation, tokens\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "obama=\"\"\"If there is anyone out there who still doubts that America is a place where all things are possible; who still wonders if the dream of our founders is alive in our time; who still questions the power of our democracy, tonight is your answer.\n",
    "It's the answer told by lines that stretched around schools and churches in numbers this nation has never seen; by people who waited three hours and four hours, many for the very first time in their lives, because they believed that this time must be different; that their voice could be that difference.\n",
    "It's the answer spoken by young and old, rich and poor, Democrat and Republican, black, white, Latino, Asian, Native American, gay, straight, disabled and not disabled  Americans who sent a message to the world that we have never been a collection of red states and blue states; we are, and always will be, the United States of America.\n",
    "It's the answer that led those who have been told for so long by so many to be cynical, and fearful, and doubtful of what we can achieve to put their hands on the arc of history and bend it once more toward the hope of a better day.\n",
    "It's been a long time coming, but tonight, because of what we did on this day, in this election, at this defining moment, change has come to America.\n",
    "I just received a very gracious call from Sen. McCain. He fought long and hard in this campaign, and he's fought even longer and harder for the country he loves. He has endured sacrifices for America that most of us cannot begin to imagine, and we are better off for the service rendered by this brave and selfless leader. I congratulate him and Gov. Palin for all they have achieved, and I look forward to working with them to renew this nation's promise in the months ahead.\n",
    "I want to thank my partner in this journey, a man who campaigned from his heart and spoke for the men and women he grew up with on the streets of Scranton and rode with on that train home to Delaware, the vice-president-elect of the United States, Joe Biden.\n",
    "I would not be standing here tonight without the unyielding support of my best friend for the last 16 years, the rock of our family and the love of my life, our nation's next first lady, Michelle Obama. Sasha and Malia, I love you both so much, and you have earned the new puppy that's coming with us to the White House. And while she's no longer with us, I know my grandmother is watching, along with the family that made me who I am. I miss them tonight, and know that my debt to them is beyond measure.\n",
    "To my campaign manager, David Plouffe; my chief strategist, David Axelrod; and the best campaign team ever assembled in the history of politics  you made this happen, and I am forever grateful for what you've sacrificed to get it done.\n",
    "But above all, I will never forget who this victory truly belongs to  it belongs to you.\n",
    "I was never the likeliest candidate for this office. We didn't start with much money or many endorsements. Our campaign was not hatched in the halls of Washington  it began in the backyards of Des Moines and the living rooms of Concord and the front porches of Charleston.\n",
    "It was built by working men and women who dug into what little savings they had to give $5 and $10 and $20 to this cause. It grew strength from the young people who rejected the myth of their generation's apathy; who left their homes and their families for jobs that offered little pay and less sleep; from the not-so-young people who braved the bitter cold and scorching heat to knock on the doors of perfect strangers; from the millions of Americans who volunteered and organized, and proved that more than two centuries later, a government of the people, by the people and for the people has not perished from this earth. This is your victory.\n",
    "I know you didn't do this just to win an election, and I know you didn't do it for me. You did it because you understand the enormity of the task that lies ahead. For even as we celebrate tonight, we know the challenges that tomorrow will bring are the greatest of our lifetime  two wars, a planet in peril, the worst financial crisis in a century. Even as we stand here tonight, we know there are brave Americans waking up in the deserts of Iraq and the mountains of Afghanistan to risk their lives for us. There are mothers and fathers who will lie awake after their children fall asleep and wonder how they'll make the mortgage, or pay their doctor's bills, or save enough for college. There is new energy to harness and new jobs to be created; new schools to build and threats to meet and alliances to repair.\n",
    "The road ahead will be long. Our climb will be steep. We may not get there in one year, or even one term, but America  I have never been more hopeful than I am tonight that we will get there. I promise you: We as a people will get there.\n",
    "There will be setbacks and false starts. There are many who won't agree with every decision or policy I make as president, and we know that government can't solve every problem. But I will always be honest with you about the challenges we face. I will listen to you, especially when we disagree. And, above all, I will ask you join in the work of remaking this nation the only way it's been done in America for 221 years  block by block, brick by brick, callused hand by callused hand.\n",
    "What began 21 months ago in the depths of winter must not end on this autumn night. This victory alone is not the change we seek  it is only the chance for us to make that change. And that cannot happen if we go back to the way things were. It cannot happen without you.\n",
    "So let us summon a new spirit of patriotism; of service and responsibility where each of us resolves to pitch in and work harder and look after not only ourselves, but each other. Let us remember that if this financial crisis taught us anything, it's that we cannot have a thriving Wall Street while Main Street suffers. In this country, we rise or fall as one nation  as one people.\n",
    "Let us resist the temptation to fall back on the same partisanship and pettiness and immaturity that has poisoned our politics for so long. Let us remember that it was a man from this state who first carried the banner of the Republican Party to the White House  a party founded on the values of self-reliance, individual liberty and national unity. Those are values we all share, and while the Democratic Party has won a great victory tonight, we do so with a measure of humility and determination to heal the divides that have held back our progress.\n",
    "As Lincoln said to a nation far more divided than ours, \"We are not enemies, but friends... Though passion may have strained, it must not break our bonds of affection.\" And, to those Americans whose support I have yet to earn, I may not have won your vote, but I hear your voices, I need your help, and I will be your president, too.\n",
    "And to all those watching tonight from beyond our shores, from parliaments and palaces to those who are huddled around radios in the forgotten corners of our world  our stories are singular, but our destiny is shared, and a new dawn of American leadership is at hand. To those who would tear this world down: We will defeat you. To those who seek peace and security: We support you. And to all those who have wondered if America's beacon still burns as bright: Tonight, we proved once more that the true strength of our nation comes not from the might of our arms or the scale of our wealth, but from the enduring power of our ideals: democracy, liberty, opportunity and unyielding hope.\n",
    "For that is the true genius of America  that America can change. Our union can be perfected. And what we have already achieved gives us hope for what we can and must achieve tomorrow.\n",
    "This election had many firsts and many stories that will be told for generations. But one that's on my mind tonight is about a woman who cast her ballot in Atlanta. She's a lot like the millions of others who stood in line to make their voice heard in this election, except for one thing: Ann Nixon Cooper is 106 years old.\n",
    "She was born just a generation past slavery; a time when there were no cars on the road or planes in the sky; when someone like her couldn't vote for two reasons  because she was a woman and because of the color of her skin.\n",
    "And tonight, I think about all that she's seen throughout her century in America  the heartache and the hope; the struggle and the progress; the times we were told that we can't and the people who pressed on with that American creed: Yes, we can.\n",
    "At a time when women's voices were silenced and their hopes dismissed, she lived to see them stand up and speak out and reach for the ballot. Yes, we can.\n",
    "When there was despair in the Dust Bowl and depression across the land, she saw a nation conquer fear itself with a New Deal, new jobs and a new sense of common purpose. Yes, we can.\n",
    "When the bombs fell on our harbor and tyranny threatened the world, she was there to witness a generation rise to greatness and a democracy was saved. Yes, we can.\n",
    "She was there for the buses in Montgomery, the hoses in Birmingham, a bridge in Selma and a preacher from Atlanta who told a people that \"We Shall Overcome.\" Yes, we can.\n",
    "A man touched down on the moon, a wall came down in Berlin, a world was connected by our own science and imagination. And this year, in this election, she touched her finger to a screen and cast her vote, because after 106 years in America, through the best of times and the darkest of hours, she knows how America can change. Yes, we can.\n",
    "America, we have come so far. We have seen so much. But there is so much more to do. So tonight, let us ask ourselves: If our children should live to see the next century; if my daughters should be so lucky to live as long as Ann Nixon Cooper, what change will they see? What progress will we have made?\n",
    "This is our chance to answer that call. This is our moment. This is our time  to put our people back to work and open doors of opportunity for our kids; to restore prosperity and promote the cause of peace; to reclaim the American Dream and reaffirm that fundamental truth that out of many, we are one; that while we breathe, we hope, and where we are met with cynicism, and doubt, and those who tell us that we can't, we will respond with that timeless creed that sums up the spirit of a people: Yes, we can.\n",
    "Thank you, God bless you, and may God bless the United States of America.\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "chiraq = \"\"\"\n",
    "La confiance que vous venez de me témoigner, je veux y répondre en m'engageant dans l'action avec détermination.\n",
    "Mes chers compatriotes de métropole, d'outre-mer et de l'étranger,\n",
    "Nous venons de vivre un temps de grave inquiétude pour la Nation.\n",
    "Mais ce soir, dans un grand élan la France a réaffirmé son attachement aux valeurs de la République.\n",
    "Je salue la France, fidèle à elle-même, fidèle à ses grands idéaux, fidèle à sa vocation universelle et humaniste.\n",
    "Je salue la France qui, comme toujours dans les moments difficiles, sait se retrouver sur l'essentiel. Je salue les Françaises et les Français épris de solidarité et de liberté, soucieux de s'ouvrir à l'Europe et au monde, tournés vers l'avenir.\n",
    "J'ai entendu et compris votre appel pour que la République vive, pour que la Nation se rassemble, pour que la politique change. Tout dans l'action qui sera conduite, devra répondre à cet appel et s'inspirer d'une exigence de service et d'écoute pour chaque Française et chaque Français.\n",
    "Ce soir, je veux vous dire aussi mon émotion et le sentiment que j'ai de la responsabilité qui m'incombe.\n",
    "Votre choix d'aujourd'hui est un choix fondateur, un choix qui renouvelle notre pacte républicain. Ce choix m'oblige comme il oblige chaque responsable de notre pays. Chacun mesure bien, à l'aune de notre histoire, la force de ce moment exceptionnel.\n",
    "Votre décision, vous l'avez prise en conscience, en dépassant les clivages traditionnels, et, pour certains d'entre vous, en allant au-delà même de vos préférences personnelles ou politiques.\n",
    "La confiance que vous venez de me témoigner, je veux y répondre en m'engageant dans l'action avec détermination.\n",
    "Président de tous les Français, je veux y répondre dans un esprit de rassemblement. Je veux mettre la République au service de tous. Je veux que les valeurs de liberté, d'égalité et de fraternité reprennent toute leur place dans la vie de chacune et de chacun d'entre nous.\n",
    "La liberté, c'est la sécurité, la lutte contre la violence, le refus de l'impunité. Faire reculer l'insécurité est la première priorité de l'Etat pour les temps à venir.\n",
    "La liberté, c'est aussi la reconnaissance du travail et du mérite, la réduction des charges et des impôts.\n",
    "L'égalité, c'est le refus de toute discrimination, ce sont les mêmes droits et les mêmes devoirs pour tous.\n",
    "La fraternité, c'est sauvegarder les retraites. C'est aider les familles à jouer pleinement leur rôle. C'est faire en sorte que personne n'éprouve plus le sentiment d'être laissé pour compte.\n",
    "La France, forte de sa cohésion sociale et de son dynamisme économique, portera en Europe et dans le monde l'ambition de la paix, des libertés et de la solidarité.\n",
    "Dans les prochains jours, je mettrai en place un gouvernement de mission, un gouvernement qui aura pour seule tâche de répondre à vos préoccupations et d'apporter des solutions à des problèmes trop longtemps négligés. Son premier devoir sera de rétablir l'autorité de l'Etat pour répondre à l'exigence de sécurité, et de mettre la France sur un nouveau chemin de croissance et d'emploi.\n",
    "C'est par une action forte et déterminée, c'est par la solidarité de la Nation, c'est par l'efficacité des résultats obtenus, que nous pourrons lutter contre l'intolérance, faire reculer l'extrémisme, garantir la vitalité de notre démocratie. Cette exigence s'impose à chacun d'entre nous. Elle impliquera, au cours des prochaines années, vigilance et mobilisation de la part de tous.\n",
    "Mes chers compatriotes,\n",
    "Le mandat que vous m'avez confié, je l'exercerai dans un esprit d'ouverture et de concorde, avec pour exigence l'unité de la République, la cohésion de la Nation et le respect de l'autorité de l'Etat.\n",
    "Les jours que nous venons de vivre ont ranimé la vigueur nationale, la vigueur de l'idéal démocratique français. Ils ont exprimé une autre idée de la politique, une autre idée de la citoyenneté.\n",
    "Chacune et chacun d'entre vous, conscient de ses responsabilités, par un choix de liberté, a contribué, ce soir, à forger le destin de la France.\n",
    "Il y a là un espoir qui ne demande qu'à grandir, un espoir que je veux servir.\n",
    "Vive la République !\n",
    "Vive la France !\n",
    "\"\"\"\n",
    "\n",
    "sentences_no_punctuation, tokens = split_text_into_sentences_and_tokens(obama)\n",
    "\n",
    "print('Obama')\n",
    "print(f\"sentences obama : {sentences_no_punctuation}\")\n",
    "print(f\"tokens obama : {tokens}\")\n",
    "\n",
    "\n",
    "print(len(sentences_no_punctuation))\n",
    "print(len(tokens))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d121cef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHIRAQ\n",
      "sentences chiraq : ['La confiance que vous venez de me témoigner je veux y répondre en mengageant dans laction avec détermination', 'Mes chers compatriotes de métropole doutremer et de létranger\\nNous venons de vivre un temps de grave inquiétude pour la Nation', 'Mais ce soir dans un grand élan la France a réaffirmé son attachement aux valeurs de la République', 'Je salue la France fidèle à ellemême fidèle à ses grands idéaux fidèle à sa vocation universelle et humaniste', 'Je salue la France qui comme toujours dans les moments difficiles sait se retrouver sur lessentiel', 'Je salue les Françaises et les Français épris de solidarité et de liberté soucieux de souvrir à lEurope et au monde tournés vers lavenir', 'Jai entendu et compris votre appel pour que la République vive pour que la Nation se rassemble pour que la politique change', 'Tout dans laction qui sera conduite devra répondre à cet appel et sinspirer dune exigence de service et découte pour chaque Française et chaque Français', 'Ce soir je veux vous dire aussi mon émotion et le sentiment que jai de la responsabilité qui mincombe', 'Votre choix daujourdhui est un choix fondateur un choix qui renouvelle notre pacte républicain', 'Ce choix moblige comme il oblige chaque responsable de notre pays', 'Chacun mesure bien à laune de notre histoire la force de ce moment exceptionnel', 'Votre décision vous lavez prise en conscience en dépassant les clivages traditionnels et pour certains dentre vous en allant audelà même de vos préférences personnelles ou politiques', 'La confiance que vous venez de me témoigner je veux y répondre en mengageant dans laction avec détermination', 'Président de tous les Français je veux y répondre dans un esprit de rassemblement', 'Je veux mettre la République au service de tous', 'Je veux que les valeurs de liberté dégalité et de fraternité reprennent toute leur place dans la vie de chacune et de chacun dentre nous', 'La liberté cest la sécurité la lutte contre la violence le refus de limpunité', 'Faire reculer linsécurité est la première priorité de lEtat pour les temps à venir', 'La liberté cest aussi la reconnaissance du travail et du mérite la réduction des charges et des impôts', 'Légalité cest le refus de toute discrimination ce sont les mêmes droits et les mêmes devoirs pour tous', 'La fraternité cest sauvegarder les retraites', 'Cest aider les familles à jouer pleinement leur rôle', 'Cest faire en sorte que personne néprouve plus le sentiment dêtre laissé pour compte', 'La France forte de sa cohésion sociale et de son dynamisme économique portera en Europe et dans le monde lambition de la paix des libertés et de la solidarité', 'Dans les prochains jours je mettrai en place un gouvernement de mission un gouvernement qui aura pour seule tâche de répondre à vos préoccupations et dapporter des solutions à des problèmes trop longtemps négligés', 'Son premier devoir sera de rétablir lautorité de lEtat pour répondre à lexigence de sécurité et de mettre la France sur un nouveau chemin de croissance et demploi', 'Cest par une action forte et déterminée cest par la solidarité de la Nation cest par lefficacité des résultats obtenus que nous pourrons lutter contre lintolérance faire reculer lextrémisme garantir la vitalité de notre démocratie', 'Cette exigence simpose à chacun dentre nous', 'Elle impliquera au cours des prochaines années vigilance et mobilisation de la part de tous', 'Mes chers compatriotes\\nLe mandat que vous mavez confié je lexercerai dans un esprit douverture et de concorde avec pour exigence lunité de la République la cohésion de la Nation et le respect de lautorité de lEtat', 'Les jours que nous venons de vivre ont ranimé la vigueur nationale la vigueur de lidéal démocratique français', 'Ils ont exprimé une autre idée de la politique une autre idée de la citoyenneté', 'Chacune et chacun dentre vous conscient de ses responsabilités par un choix de liberté a contribué ce soir à forger le destin de la France', 'Il y a là un espoir qui ne demande quà grandir un espoir que je veux servir', 'Vive la République ', 'Vive la France ']\n",
      "tokens chiraq : ['réduction', 'douverture', 'droits', 'simpose', 'un', 'tournés', 'mettre', 'me', 'solutions', 'cours', 'années', 'laissé', 'mérite', 'la', 'émotion', 'mais', 'son', 'moblige', 'lutte', 'faire', 'responsabilité', 'retrouver', 'ne', 'lunité', 'idée', 'linsécurité', 'appel', 'sauvegarder', 'toujours', 'portera', 'confiance', 'daujourdhui', 'lexercerai', 'chacune', 'pour', 'trop', 'déterminée', 'répondre', 'elle', 'dire', 'lefficacité', 'nationale', 'nation', 'difficiles', 'épris', 'aider', 'nouveau', 'vive', 'au', 'prochains', 'demande', 'françaises', 'liberté', 'mon', 'refus', 'vos', 'soir', 'sera', 'fraternité', 'place', 'cohésion', 'discrimination', 'sociale', 'grandir', 'grand', 'jouer', 'mavez', 'une', 'rassemble', 'compte', 'tous', 'grave', 'reprennent', 'vivre', 'sont', 'chemin', 'vigueur', 'chers', 'change', 'cest', 'valeurs', 'du', 'politiques', 'rassemblement', 'monde', 'longtemps', 'personne', 'premier', 'priorité', 'forte', 'sait', 'légalité', 'tâche', 'dans', 'par', 'fondateur', 'ont', 'paix', 'aussi', 'espoir', 'attachement', 'préférences', 'action', 'républicain', 'pacte', 'exprimé', 'pourrons', 'devoirs', 'lavenir', 'mesure', 'comme', 'citoyenneté', 'charges', 'réaffirmé', 'contre', 'gouvernement', 'compris', 'démocratie', 'vers', 'mes', 'leur', 'cette', 'se', 'seule', 'et', 'certains', 'dentre', 'chaque', 'mettrai', 'vocation', 'vigilance', 'lautorité', 'à', 'moment', 'française', 'néprouve', 'servir', 'ses', 'exceptionnel', 'force', 'concorde', 'conscience', 'grands', 'reconnaissance', 'croissance', 'politique', 'contribué', 'demploi', 'témoigner', 'même', 'veux', 'laune', 'français', 'les', 'de', 'nous', 'oblige', 'mincombe', 'part', 'démocratique', 'responsable', 'devra', 'sinspirer', 'sécurité', 'négligés', 'conduite', 'lutter', 'compatriotes', 'pays', 'le', 'en', 'esprit', 'ranimé', 'destin', 'solidarité', 'notre', 'mengageant', 'soucieux', 'détermination', 'histoire', 'venez', 'lextrémisme', 'dégalité', 'inquiétude', 'personnelles', 'lintolérance', 'choix', 'jours', 'avec', 'dépassant', 'létranger', 'clivages', 'rétablir', 'dune', 'allant', 'lexigence', 'entendu', 'traditionnels', 'y', 'temps', 'idéaux', 'qui', 'reculer', 'europe', 'sa', 'préoccupations', 'problèmes', 'lidéal', 'conscient', 'souvrir', 'responsabilités', 'plus', 'mêmes', 'résultats', 'exigence', 'aux', 'prochaines', 'dapporter', 'a', 'impliquera', 'mobilisation', 'aura', 'sentiment', 'je', 'pleinement', 'autre', 'économique', 'vitalité', 'bien', 'humaniste', 'lessentiel', 'forger', 'mission', 'première', 'république', 'obtenus', 'garantir', 'cet', 'sur', 'découte', 'il', 'dêtre', 'dynamisme', 'doutremer', 'audelà', 'jai', 'ce', 'limpunité', 'président', 'libertés', 'tout', 'que', 'travail', 'moments', 'mandat', 'ou', 'ils', 'respect', 'service', 'leurope', 'métropole', 'venons', 'élan', 'lambition', 'france', 'votre', 'laction', 'est', 'violence', 'familles', 'retraites', 'impôts', 'prise', 'devoir', 'vous', 'chacun', 'quà', 'venir', 'sorte', 'là', 'universelle', 'rôle', 'fidèle', 'confié', 'letat', 'lavez', 'décision', 'toute', 'des', 'salue', 'ellemême', 'renouvelle', 'vie']\n",
      "37\n",
      "293\n"
     ]
    }
   ],
   "source": [
    "sentences_no_punctuation, tokens = split_text_into_sentences_and_tokens(chiraq)\n",
    "\n",
    "print('CHIRAQ')\n",
    "print(f\"sentences chiraq : {sentences_no_punctuation}\")\n",
    "print(f\"tokens chiraq : {tokens}\")\n",
    "\n",
    "\n",
    "print(len(sentences_no_punctuation))\n",
    "print(len(tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a28d173b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Doc 1  Doc 2  Doc 3  Doc 4  Doc 5  Doc 6  Doc 7  Doc 8  Doc 9  Doc 10  \\\n",
      "Doc 1     0.0   35.0   31.0   32.0   29.0   35.0   33.0   35.0   27.0    33.0   \n",
      "Doc 2    35.0    0.0   32.0   33.0   34.0   36.0   30.0   36.0   36.0    30.0   \n",
      "Doc 3    31.0   32.0    0.0   29.0   28.0   36.0   30.0   38.0   30.0    28.0   \n",
      "Doc 4    32.0   33.0   29.0    0.0   25.0   29.0   29.0   37.0   31.0    29.0   \n",
      "Doc 5    29.0   34.0   28.0   25.0    0.0   30.0   30.0   36.0   32.0    28.0   \n",
      "Doc 6    35.0   36.0   36.0   29.0   30.0    0.0   36.0   34.0   36.0    34.0   \n",
      "Doc 7    33.0   30.0   30.0   29.0   30.0   36.0    0.0   36.0   28.0    28.0   \n",
      "Doc 8    35.0   36.0   38.0   37.0   36.0   34.0   36.0    0.0   40.0    34.0   \n",
      "Doc 9    27.0   36.0   30.0   31.0   32.0   36.0   28.0   40.0    0.0    32.0   \n",
      "Doc 10   33.0   30.0   28.0   29.0   28.0   34.0   28.0   34.0   32.0     0.0   \n",
      "Doc 11   27.0   30.0   24.0   27.0   26.0   30.0   28.0   32.0   26.0    20.0   \n",
      "Doc 12   28.0   29.0   25.0   26.0   27.0   29.0   29.0   33.0   29.0    25.0   \n",
      "Doc 13   39.0   38.0   42.0   39.0   40.0   38.0   38.0   42.0   42.0    36.0   \n",
      "Doc 14    0.0   35.0   31.0   32.0   29.0   35.0   33.0   35.0   27.0    33.0   \n",
      "Doc 15   21.0   30.0   24.0   27.0   24.0   26.0   30.0   30.0   28.0    24.0   \n",
      "Doc 16   21.0   26.0   20.0   21.0   22.0   24.0   22.0   30.0   22.0    22.0   \n",
      "Doc 17   30.0   33.0   31.0   32.0   31.0   33.0   33.0   39.0   31.0    33.0   \n",
      "Doc 18   27.0   28.0   26.0   27.0   26.0   28.0   28.0   34.0   28.0    24.0   \n",
      "Doc 19   29.0   26.0   28.0   27.0   26.0   28.0   28.0   32.0   32.0    26.0   \n",
      "Doc 20   32.0   31.0   29.0   26.0   29.0   31.0   27.0   37.0   29.0    25.0   \n",
      "Doc 21   34.0   31.0   31.0   32.0   31.0   31.0   31.0   35.0   31.0    29.0   \n",
      "Doc 22   25.0   26.0   22.0   21.0   20.0   26.0   22.0   32.0   26.0    18.0   \n",
      "Doc 23   30.0   31.0   27.0   24.0   25.0   27.0   27.0   33.0   31.0    21.0   \n",
      "Doc 24   33.0   34.0   34.0   33.0   34.0   38.0   30.0   38.0   32.0    26.0   \n",
      "Doc 25   33.0   36.0   30.0   31.0   32.0   32.0   36.0   40.0   36.0    36.0   \n",
      "Doc 26   40.0   41.0   41.0   40.0   39.0   41.0   43.0   39.0   43.0    37.0   \n",
      "Doc 27   37.0   32.0   32.0   33.0   34.0   38.0   36.0   32.0   40.0    34.0   \n",
      "Doc 28   40.0   39.0   43.0   42.0   43.0   43.0   39.0   45.0   43.0    39.0   \n",
      "Doc 29   29.0   26.0   26.0   23.0   26.0   26.0   26.0   26.0   30.0    20.0   \n",
      "Doc 30   30.0   29.0   27.0   24.0   29.0   29.0   27.0   35.0   29.0    27.0   \n",
      "Doc 31   34.0   31.0   39.0   42.0   41.0   45.0   37.0   43.0   37.0    41.0   \n",
      "Doc 32   28.0   25.0   29.0   30.0   27.0   29.0   29.0   35.0   31.0    29.0   \n",
      "Doc 33   26.0   27.0   23.0   24.0   25.0   29.0   23.0   31.0   27.0    23.0   \n",
      "Doc 34   38.0   35.0   27.0   30.0   37.0   37.0   37.0   41.0   31.0    31.0   \n",
      "Doc 35   28.0   35.0   29.0   28.0   29.0   33.0   31.0   37.0   29.0    25.0   \n",
      "Doc 36   21.0   22.0   16.0   17.0   18.0   24.0   14.0   28.0   22.0    16.0   \n",
      "Doc 37   21.0   22.0   16.0   15.0   16.0   24.0   16.0   28.0   22.0    16.0   \n",
      "\n",
      "        ...  Doc 28  Doc 29  Doc 30  Doc 31  Doc 32  Doc 33  Doc 34  Doc 35  \\\n",
      "Doc 1   ...    40.0    29.0    30.0    34.0    28.0    26.0    38.0    28.0   \n",
      "Doc 2   ...    39.0    26.0    29.0    31.0    25.0    27.0    35.0    35.0   \n",
      "Doc 3   ...    43.0    26.0    27.0    39.0    29.0    23.0    27.0    29.0   \n",
      "Doc 4   ...    42.0    23.0    24.0    42.0    30.0    24.0    30.0    28.0   \n",
      "Doc 5   ...    43.0    26.0    29.0    41.0    27.0    25.0    37.0    29.0   \n",
      "Doc 6   ...    43.0    26.0    29.0    45.0    29.0    29.0    37.0    33.0   \n",
      "Doc 7   ...    39.0    26.0    27.0    37.0    29.0    23.0    37.0    31.0   \n",
      "Doc 8   ...    45.0    26.0    35.0    43.0    35.0    31.0    41.0    37.0   \n",
      "Doc 9   ...    43.0    30.0    29.0    37.0    31.0    27.0    31.0    29.0   \n",
      "Doc 10  ...    39.0    20.0    27.0    41.0    29.0    23.0    31.0    25.0   \n",
      "Doc 11  ...    37.0    20.0    23.0    39.0    25.0    19.0    29.0    25.0   \n",
      "Doc 12  ...    36.0    19.0    24.0    40.0    24.0    20.0    28.0    28.0   \n",
      "Doc 13  ...    51.0    32.0    35.0    45.0    37.0    35.0    41.0    43.0   \n",
      "Doc 14  ...    40.0    29.0    30.0    34.0    28.0    26.0    38.0    28.0   \n",
      "Doc 15  ...    41.0    22.0    23.0    35.0    23.0    21.0    33.0    21.0   \n",
      "Doc 16  ...    35.0    18.0    15.0    33.0    21.0    15.0    29.0    21.0   \n",
      "Doc 17  ...    42.0    23.0    30.0    40.0    28.0    28.0    30.0    32.0   \n",
      "Doc 18  ...    31.0    22.0    23.0    37.0    23.0    19.0    29.0    29.0   \n",
      "Doc 19  ...    33.0    22.0    25.0    37.0    23.0    21.0    33.0    29.0   \n",
      "Doc 20  ...    34.0    23.0    22.0    42.0    28.0    22.0    32.0    30.0   \n",
      "Doc 21  ...    38.0    27.0    26.0    40.0    28.0    26.0    34.0    34.0   \n",
      "Doc 22  ...    31.0    16.0    19.0    37.0    19.0    15.0    29.0    23.0   \n",
      "Doc 23  ...    36.0    17.0    24.0    42.0    24.0    20.0    32.0    24.0   \n",
      "Doc 24  ...    39.0    24.0    31.0    41.0    31.0    27.0    37.0    31.0   \n",
      "Doc 25  ...    39.0    32.0    29.0    41.0    33.0    29.0    37.0    39.0   \n",
      "Doc 26  ...    54.0    35.0    38.0    48.0    40.0    38.0    44.0    38.0   \n",
      "Doc 27  ...    47.0    28.0    33.0    37.0    35.0    31.0    35.0    37.0   \n",
      "Doc 28  ...     0.0    37.0    36.0    50.0    36.0    34.0    46.0    44.0   \n",
      "Doc 29  ...    37.0     0.0    23.0    37.0    23.0    19.0    25.0    23.0   \n",
      "Doc 30  ...    36.0    23.0     0.0    40.0    26.0    20.0    32.0    30.0   \n",
      "Doc 31  ...    50.0    37.0    40.0     0.0    40.0    38.0    42.0    42.0   \n",
      "Doc 32  ...    36.0    23.0    26.0    40.0     0.0    20.0    36.0    30.0   \n",
      "Doc 33  ...    34.0    19.0    20.0    38.0    20.0     0.0    30.0    26.0   \n",
      "Doc 34  ...    46.0    25.0    32.0    42.0    36.0    30.0     0.0    34.0   \n",
      "Doc 35  ...    44.0    23.0    30.0    42.0    30.0    26.0    34.0     0.0   \n",
      "Doc 36  ...    31.0    12.0    15.0    31.0    17.0    11.0    25.0    19.0   \n",
      "Doc 37  ...    31.0    12.0    15.0    33.0    17.0    11.0    23.0    19.0   \n",
      "\n",
      "        Doc 36  Doc 37  \n",
      "Doc 1     21.0    21.0  \n",
      "Doc 2     22.0    22.0  \n",
      "Doc 3     16.0    16.0  \n",
      "Doc 4     17.0    15.0  \n",
      "Doc 5     18.0    16.0  \n",
      "Doc 6     24.0    24.0  \n",
      "Doc 7     14.0    16.0  \n",
      "Doc 8     28.0    28.0  \n",
      "Doc 9     22.0    22.0  \n",
      "Doc 10    16.0    16.0  \n",
      "Doc 11    14.0    14.0  \n",
      "Doc 12    15.0    15.0  \n",
      "Doc 13    30.0    30.0  \n",
      "Doc 14    21.0    21.0  \n",
      "Doc 15    16.0    16.0  \n",
      "Doc 16     8.0    10.0  \n",
      "Doc 17    23.0    23.0  \n",
      "Doc 18    14.0    14.0  \n",
      "Doc 19    16.0    16.0  \n",
      "Doc 20    15.0    15.0  \n",
      "Doc 21    21.0    21.0  \n",
      "Doc 22     8.0     8.0  \n",
      "Doc 23    13.0    13.0  \n",
      "Doc 24    20.0    20.0  \n",
      "Doc 25    24.0    22.0  \n",
      "Doc 26    33.0    33.0  \n",
      "Doc 27    26.0    24.0  \n",
      "Doc 28    31.0    31.0  \n",
      "Doc 29    12.0    12.0  \n",
      "Doc 30    15.0    15.0  \n",
      "Doc 31    31.0    33.0  \n",
      "Doc 32    17.0    17.0  \n",
      "Doc 33    11.0    11.0  \n",
      "Doc 34    25.0    23.0  \n",
      "Doc 35    19.0    19.0  \n",
      "Doc 36     0.0     2.0  \n",
      "Doc 37     2.0     0.0  \n",
      "\n",
      "[37 rows x 37 columns]\n",
      "        Doc 1  Doc 2  Doc 3  Doc 4  Doc 5  Doc 6  Doc 7  Doc 8  Doc 9  Doc 10  \\\n",
      "Doc 1     0.0   38.0   32.0   36.0   29.0   41.0   39.0   39.0   27.0    36.0   \n",
      "Doc 2    38.0    0.0   36.0   40.0   37.0   41.0   39.0   43.0   39.0    36.0   \n",
      "Doc 3    32.0   36.0    0.0   34.0   29.0   43.0   35.0   43.0   31.0    32.0   \n",
      "Doc 4    36.0   40.0   34.0    0.0   29.0   39.0   39.0   45.0   35.0    36.0   \n",
      "Doc 5    29.0   37.0   29.0   29.0    0.0   36.0   36.0   40.0   32.0    31.0   \n",
      "Doc 6    41.0   41.0   43.0   39.0   36.0    0.0   48.0   40.0   42.0    43.0   \n",
      "Doc 7    39.0   39.0   35.0   39.0   36.0   48.0    0.0   46.0   34.0    37.0   \n",
      "Doc 8    39.0   43.0   43.0   45.0   40.0   40.0   46.0    0.0   44.0    41.0   \n",
      "Doc 9    27.0   39.0   31.0   35.0   32.0   42.0   34.0   44.0    0.0    35.0   \n",
      "Doc 10   36.0   36.0   32.0   36.0   31.0   43.0   37.0   41.0   35.0     0.0   \n",
      "Doc 11   28.0   34.0   26.0   32.0   27.0   37.0   35.0   37.0   27.0    24.0   \n",
      "Doc 12   29.0   31.0   27.0   31.0   28.0   34.0   36.0   38.0   30.0    29.0   \n",
      "Doc 13   42.0   44.0   46.0   46.0   43.0   47.0   47.0   49.0   45.0    42.0   \n",
      "Doc 14    0.0   38.0   32.0   36.0   29.0   41.0   39.0   39.0   27.0    36.0   \n",
      "Doc 15   22.0   32.0   26.0   32.0   25.0   31.0   37.0   35.0   29.0    28.0   \n",
      "Doc 16   21.0   29.0   21.0   25.0   22.0   30.0   28.0   34.0   22.0    25.0   \n",
      "Doc 17   35.0   35.0   37.0   41.0   36.0   38.0   44.0   44.0   36.0    41.0   \n",
      "Doc 18   30.0   34.0   28.0   34.0   29.0   37.0   33.0   41.0   31.0    30.0   \n",
      "Doc 19   30.0   30.0   30.0   32.0   27.0   33.0   35.0   37.0   33.0    30.0   \n",
      "Doc 20   37.0   39.0   33.0   35.0   34.0   40.0   34.0   44.0   34.0    33.0   \n",
      "Doc 21   36.0   36.0   34.0   38.0   33.0   37.0   39.0   41.0   33.0    34.0   \n",
      "Doc 22   25.0   29.0   23.0   25.0   20.0   32.0   28.0   36.0   26.0    21.0   \n",
      "Doc 23   30.0   34.0   28.0   28.0   25.0   33.0   33.0   37.0   31.0    24.0   \n",
      "Doc 24   33.0   37.0   35.0   37.0   34.0   44.0   36.0   42.0   32.0    29.0   \n",
      "Doc 25   40.0   40.0   36.0   42.0   39.0   37.0   45.0   47.0   43.0    46.0   \n",
      "Doc 26   45.0   47.0   47.0   47.0   44.0   50.0   54.0   48.0   48.0    43.0   \n",
      "Doc 27   44.0   36.0   40.0   44.0   41.0   43.0   49.0   41.0   47.0    44.0   \n",
      "Doc 28   51.0   51.0   53.0   57.0   54.0   56.0   52.0   60.0   54.0    53.0   \n",
      "Doc 29   29.0   29.0   27.0   27.0   26.0   32.0   32.0   30.0   30.0    23.0   \n",
      "Doc 30   31.0   31.0   29.0   29.0   30.0   34.0   34.0   40.0   30.0    31.0   \n",
      "Doc 31   45.0   39.0   49.0   57.0   52.0   54.0   50.0   56.0   48.0    55.0   \n",
      "Doc 32   31.0   29.0   31.0   37.0   30.0   36.0   36.0   42.0   34.0    35.0   \n",
      "Doc 33   31.0   33.0   27.0   33.0   30.0   38.0   32.0   40.0   32.0    31.0   \n",
      "Doc 34   40.0   36.0   30.0   36.0   39.0   41.0   45.0   47.0   33.0    36.0   \n",
      "Doc 35   30.0   40.0   32.0   34.0   31.0   41.0   39.0   43.0   31.0    28.0   \n",
      "Doc 36   21.0   25.0   17.0   21.0   18.0   30.0   20.0   32.0   22.0    19.0   \n",
      "Doc 37   21.0   25.0   17.0   19.0   16.0   30.0   22.0   32.0   22.0    19.0   \n",
      "\n",
      "        ...  Doc 28  Doc 29  Doc 30  Doc 31  Doc 32  Doc 33  Doc 34  Doc 35  \\\n",
      "Doc 1   ...    51.0    29.0    31.0    45.0    31.0    31.0    40.0    30.0   \n",
      "Doc 2   ...    51.0    29.0    31.0    39.0    29.0    33.0    36.0    40.0   \n",
      "Doc 3   ...    53.0    27.0    29.0    49.0    31.0    27.0    30.0    32.0   \n",
      "Doc 4   ...    57.0    27.0    29.0    57.0    37.0    33.0    36.0    34.0   \n",
      "Doc 5   ...    54.0    26.0    30.0    52.0    30.0    30.0    39.0    31.0   \n",
      "Doc 6   ...    56.0    32.0    34.0    54.0    36.0    38.0    41.0    41.0   \n",
      "Doc 7   ...    52.0    32.0    34.0    50.0    36.0    32.0    45.0    39.0   \n",
      "Doc 8   ...    60.0    30.0    40.0    56.0    42.0    40.0    47.0    43.0   \n",
      "Doc 9   ...    54.0    30.0    30.0    48.0    34.0    32.0    33.0    31.0   \n",
      "Doc 10  ...    53.0    23.0    31.0    55.0    35.0    31.0    36.0    28.0   \n",
      "Doc 11  ...    49.0    21.0    25.0    51.0    29.0    25.0    32.0    28.0   \n",
      "Doc 12  ...    46.0    20.0    24.0    50.0    26.0    24.0    29.0    31.0   \n",
      "Doc 13  ...    65.0    35.0    39.0    59.0    43.0    43.0    46.0    48.0   \n",
      "Doc 14  ...    51.0    29.0    31.0    45.0    31.0    31.0    40.0    30.0   \n",
      "Doc 15  ...    51.0    23.0    23.0    45.0    25.0    25.0    34.0    24.0   \n",
      "Doc 16  ...    46.0    18.0    16.0    44.0    24.0    20.0    31.0    23.0   \n",
      "Doc 17  ...    56.0    28.0    34.0    48.0    34.0    36.0    33.0    39.0   \n",
      "Doc 18  ...    41.0    25.0    27.0    47.0    27.0    25.0    34.0    34.0   \n",
      "Doc 19  ...    43.0    23.0    27.0    47.0    27.0    27.0    36.0    32.0   \n",
      "Doc 20  ...    46.0    28.0    28.0    52.0    34.0    30.0    39.0    37.0   \n",
      "Doc 21  ...    51.0    29.0    29.0    53.0    33.0    33.0    38.0    38.0   \n",
      "Doc 22  ...    42.0    16.0    20.0    48.0    22.0    20.0    31.0    25.0   \n",
      "Doc 23  ...    47.0    17.0    25.0    53.0    27.0    25.0    34.0    26.0   \n",
      "Doc 24  ...    50.0    24.0    32.0    52.0    34.0    32.0    39.0    33.0   \n",
      "Doc 25  ...    51.0    39.0    35.0    47.0    39.0    37.0    42.0    48.0   \n",
      "Doc 26  ...    68.0    40.0    42.0    62.0    46.0    46.0    49.0    43.0   \n",
      "Doc 27  ...    59.0    35.0    39.0    41.0    43.0    41.0    40.0    46.0   \n",
      "Doc 28  ...     0.0    48.0    46.0    62.0    46.0    46.0    57.0    57.0   \n",
      "Doc 29  ...    48.0     0.0    24.0    48.0    26.0    24.0    27.0    25.0   \n",
      "Doc 30  ...    46.0    24.0     0.0    50.0    28.0    24.0    33.0    33.0   \n",
      "Doc 31  ...    62.0    48.0    50.0     0.0    50.0    50.0    51.0    55.0   \n",
      "Doc 32  ...    46.0    26.0    28.0    50.0     0.0    24.0    39.0    35.0   \n",
      "Doc 33  ...    46.0    24.0    24.0    50.0    24.0     0.0    35.0    33.0   \n",
      "Doc 34  ...    57.0    27.0    33.0    51.0    39.0    35.0     0.0    38.0   \n",
      "Doc 35  ...    57.0    25.0    33.0    55.0    35.0    33.0    38.0     0.0   \n",
      "Doc 36  ...    42.0    12.0    16.0    42.0    20.0    16.0    27.0    21.0   \n",
      "Doc 37  ...    42.0    12.0    16.0    44.0    20.0    16.0    25.0    21.0   \n",
      "\n",
      "        Doc 36  Doc 37  \n",
      "Doc 1     21.0    21.0  \n",
      "Doc 2     25.0    25.0  \n",
      "Doc 3     17.0    17.0  \n",
      "Doc 4     21.0    19.0  \n",
      "Doc 5     18.0    16.0  \n",
      "Doc 6     30.0    30.0  \n",
      "Doc 7     20.0    22.0  \n",
      "Doc 8     32.0    32.0  \n",
      "Doc 9     22.0    22.0  \n",
      "Doc 10    19.0    19.0  \n",
      "Doc 11    15.0    15.0  \n",
      "Doc 12    16.0    16.0  \n",
      "Doc 13    33.0    33.0  \n",
      "Doc 14    21.0    21.0  \n",
      "Doc 15    17.0    17.0  \n",
      "Doc 16     8.0    10.0  \n",
      "Doc 17    28.0    28.0  \n",
      "Doc 18    17.0    17.0  \n",
      "Doc 19    17.0    17.0  \n",
      "Doc 20    20.0    20.0  \n",
      "Doc 21    23.0    23.0  \n",
      "Doc 22     8.0     8.0  \n",
      "Doc 23    13.0    13.0  \n",
      "Doc 24    20.0    20.0  \n",
      "Doc 25    31.0    29.0  \n",
      "Doc 26    38.0    38.0  \n",
      "Doc 27    33.0    31.0  \n",
      "Doc 28    42.0    42.0  \n",
      "Doc 29    12.0    12.0  \n",
      "Doc 30    16.0    16.0  \n",
      "Doc 31    42.0    44.0  \n",
      "Doc 32    20.0    20.0  \n",
      "Doc 33    16.0    16.0  \n",
      "Doc 34    27.0    25.0  \n",
      "Doc 35    21.0    21.0  \n",
      "Doc 36     0.0     2.0  \n",
      "Doc 37     2.0     0.0  \n",
      "\n",
      "[37 rows x 37 columns]\n",
      "           Doc 1     Doc 2     Doc 3     Doc 4     Doc 5     Doc 6     Doc 7  \\\n",
      "Doc 1   0.000000  5.916080  5.567764  5.656854  5.385165  5.916080  5.744563   \n",
      "Doc 2   5.916080  0.000000  5.656854  5.744563  5.830952  6.000000  5.477226   \n",
      "Doc 3   5.567764  5.656854  0.000000  5.385165  5.291503  6.000000  5.477226   \n",
      "Doc 4   5.656854  5.744563  5.385165  0.000000  5.000000  5.385165  5.385165   \n",
      "Doc 5   5.385165  5.830952  5.291503  5.000000  0.000000  5.477226  5.477226   \n",
      "Doc 6   5.916080  6.000000  6.000000  5.385165  5.477226  0.000000  6.000000   \n",
      "Doc 7   5.744563  5.477226  5.477226  5.385165  5.477226  6.000000  0.000000   \n",
      "Doc 8   5.916080  6.000000  6.164414  6.082763  6.000000  5.830952  6.000000   \n",
      "Doc 9   5.196152  6.000000  5.477226  5.567764  5.656854  6.000000  5.291503   \n",
      "Doc 10  5.744563  5.477226  5.291503  5.385165  5.291503  5.830952  5.291503   \n",
      "Doc 11  5.196152  5.477226  4.898979  5.196152  5.099020  5.477226  5.291503   \n",
      "Doc 12  5.291503  5.385165  5.000000  5.099020  5.196152  5.385165  5.385165   \n",
      "Doc 13  6.244998  6.164414  6.480741  6.244998  6.324555  6.164414  6.164414   \n",
      "Doc 14  0.000000  5.916080  5.567764  5.656854  5.385165  5.916080  5.744563   \n",
      "Doc 15  4.582576  5.477226  4.898979  5.196152  4.898979  5.099020  5.477226   \n",
      "Doc 16  4.582576  5.099020  4.472136  4.582576  4.690416  4.898979  4.690416   \n",
      "Doc 17  5.477226  5.744563  5.567764  5.656854  5.567764  5.744563  5.744563   \n",
      "Doc 18  5.196152  5.291503  5.099020  5.196152  5.099020  5.291503  5.291503   \n",
      "Doc 19  5.385165  5.099020  5.291503  5.196152  5.099020  5.291503  5.291503   \n",
      "Doc 20  5.656854  5.567764  5.385165  5.099020  5.385165  5.567764  5.196152   \n",
      "Doc 21  5.830952  5.567764  5.567764  5.656854  5.567764  5.567764  5.567764   \n",
      "Doc 22  5.000000  5.099020  4.690416  4.582576  4.472136  5.099020  4.690416   \n",
      "Doc 23  5.477226  5.567764  5.196152  4.898979  5.000000  5.196152  5.196152   \n",
      "Doc 24  5.744563  5.830952  5.830952  5.744563  5.830952  6.164414  5.477226   \n",
      "Doc 25  5.744563  6.000000  5.477226  5.567764  5.656854  5.656854  6.000000   \n",
      "Doc 26  6.324555  6.403124  6.403124  6.324555  6.244998  6.403124  6.557439   \n",
      "Doc 27  6.082763  5.656854  5.656854  5.744563  5.830952  6.164414  6.000000   \n",
      "Doc 28  6.324555  6.244998  6.557439  6.480741  6.557439  6.557439  6.244998   \n",
      "Doc 29  5.385165  5.099020  5.099020  4.795832  5.099020  5.099020  5.099020   \n",
      "Doc 30  5.477226  5.385165  5.196152  4.898979  5.385165  5.385165  5.196152   \n",
      "Doc 31  5.830952  5.567764  6.244998  6.480741  6.403124  6.708204  6.082763   \n",
      "Doc 32  5.291503  5.000000  5.385165  5.477226  5.196152  5.385165  5.385165   \n",
      "Doc 33  5.099020  5.196152  4.795832  4.898979  5.000000  5.385165  4.795832   \n",
      "Doc 34  6.164414  5.916080  5.196152  5.477226  6.082763  6.082763  6.082763   \n",
      "Doc 35  5.291503  5.916080  5.385165  5.291503  5.385165  5.744563  5.567764   \n",
      "Doc 36  4.582576  4.690416  4.000000  4.123106  4.242641  4.898979  3.741657   \n",
      "Doc 37  4.582576  4.690416  4.000000  3.872983  4.000000  4.898979  4.000000   \n",
      "\n",
      "           Doc 8     Doc 9    Doc 10  ...    Doc 28    Doc 29    Doc 30  \\\n",
      "Doc 1   5.916080  5.196152  5.744563  ...  6.324555  5.385165  5.477226   \n",
      "Doc 2   6.000000  6.000000  5.477226  ...  6.244998  5.099020  5.385165   \n",
      "Doc 3   6.164414  5.477226  5.291503  ...  6.557439  5.099020  5.196152   \n",
      "Doc 4   6.082763  5.567764  5.385165  ...  6.480741  4.795832  4.898979   \n",
      "Doc 5   6.000000  5.656854  5.291503  ...  6.557439  5.099020  5.385165   \n",
      "Doc 6   5.830952  6.000000  5.830952  ...  6.557439  5.099020  5.385165   \n",
      "Doc 7   6.000000  5.291503  5.291503  ...  6.244998  5.099020  5.196152   \n",
      "Doc 8   0.000000  6.324555  5.830952  ...  6.708204  5.099020  5.916080   \n",
      "Doc 9   6.324555  0.000000  5.656854  ...  6.557439  5.477226  5.385165   \n",
      "Doc 10  5.830952  5.656854  0.000000  ...  6.244998  4.472136  5.196152   \n",
      "Doc 11  5.656854  5.099020  4.472136  ...  6.082763  4.472136  4.795832   \n",
      "Doc 12  5.744563  5.385165  5.000000  ...  6.000000  4.358899  4.898979   \n",
      "Doc 13  6.480741  6.480741  6.000000  ...  7.141428  5.656854  5.916080   \n",
      "Doc 14  5.916080  5.196152  5.744563  ...  6.324555  5.385165  5.477226   \n",
      "Doc 15  5.477226  5.291503  4.898979  ...  6.403124  4.690416  4.795832   \n",
      "Doc 16  5.477226  4.690416  4.690416  ...  5.916080  4.242641  3.872983   \n",
      "Doc 17  6.244998  5.567764  5.744563  ...  6.480741  4.795832  5.477226   \n",
      "Doc 18  5.830952  5.291503  4.898979  ...  5.567764  4.690416  4.795832   \n",
      "Doc 19  5.656854  5.656854  5.099020  ...  5.744563  4.690416  5.000000   \n",
      "Doc 20  6.082763  5.385165  5.000000  ...  5.830952  4.795832  4.690416   \n",
      "Doc 21  5.916080  5.567764  5.385165  ...  6.164414  5.196152  5.099020   \n",
      "Doc 22  5.656854  5.099020  4.242641  ...  5.567764  4.000000  4.358899   \n",
      "Doc 23  5.744563  5.567764  4.582576  ...  6.000000  4.123106  4.898979   \n",
      "Doc 24  6.164414  5.656854  5.099020  ...  6.244998  4.898979  5.567764   \n",
      "Doc 25  6.324555  6.000000  6.000000  ...  6.244998  5.656854  5.385165   \n",
      "Doc 26  6.244998  6.557439  6.082763  ...  7.348469  5.916080  6.164414   \n",
      "Doc 27  5.656854  6.324555  5.830952  ...  6.855655  5.291503  5.744563   \n",
      "Doc 28  6.708204  6.557439  6.244998  ...  0.000000  6.082763  6.000000   \n",
      "Doc 29  5.099020  5.477226  4.472136  ...  6.082763  0.000000  4.795832   \n",
      "Doc 30  5.916080  5.385165  5.196152  ...  6.000000  4.795832  0.000000   \n",
      "Doc 31  6.557439  6.082763  6.403124  ...  7.071068  6.082763  6.324555   \n",
      "Doc 32  5.916080  5.567764  5.385165  ...  6.000000  4.795832  5.099020   \n",
      "Doc 33  5.567764  5.196152  4.795832  ...  5.830952  4.358899  4.472136   \n",
      "Doc 34  6.403124  5.567764  5.567764  ...  6.782330  5.000000  5.656854   \n",
      "Doc 35  6.082763  5.385165  5.000000  ...  6.633250  4.795832  5.477226   \n",
      "Doc 36  5.291503  4.690416  4.000000  ...  5.567764  3.464102  3.872983   \n",
      "Doc 37  5.291503  4.690416  4.000000  ...  5.567764  3.464102  3.872983   \n",
      "\n",
      "          Doc 31    Doc 32    Doc 33    Doc 34    Doc 35    Doc 36    Doc 37  \n",
      "Doc 1   5.830952  5.291503  5.099020  6.164414  5.291503  4.582576  4.582576  \n",
      "Doc 2   5.567764  5.000000  5.196152  5.916080  5.916080  4.690416  4.690416  \n",
      "Doc 3   6.244998  5.385165  4.795832  5.196152  5.385165  4.000000  4.000000  \n",
      "Doc 4   6.480741  5.477226  4.898979  5.477226  5.291503  4.123106  3.872983  \n",
      "Doc 5   6.403124  5.196152  5.000000  6.082763  5.385165  4.242641  4.000000  \n",
      "Doc 6   6.708204  5.385165  5.385165  6.082763  5.744563  4.898979  4.898979  \n",
      "Doc 7   6.082763  5.385165  4.795832  6.082763  5.567764  3.741657  4.000000  \n",
      "Doc 8   6.557439  5.916080  5.567764  6.403124  6.082763  5.291503  5.291503  \n",
      "Doc 9   6.082763  5.567764  5.196152  5.567764  5.385165  4.690416  4.690416  \n",
      "Doc 10  6.403124  5.385165  4.795832  5.567764  5.000000  4.000000  4.000000  \n",
      "Doc 11  6.244998  5.000000  4.358899  5.385165  5.000000  3.741657  3.741657  \n",
      "Doc 12  6.324555  4.898979  4.472136  5.291503  5.291503  3.872983  3.872983  \n",
      "Doc 13  6.708204  6.082763  5.916080  6.403124  6.557439  5.477226  5.477226  \n",
      "Doc 14  5.830952  5.291503  5.099020  6.164414  5.291503  4.582576  4.582576  \n",
      "Doc 15  5.916080  4.795832  4.582576  5.744563  4.582576  4.000000  4.000000  \n",
      "Doc 16  5.744563  4.582576  3.872983  5.385165  4.582576  2.828427  3.162278  \n",
      "Doc 17  6.324555  5.291503  5.291503  5.477226  5.656854  4.795832  4.795832  \n",
      "Doc 18  6.082763  4.795832  4.358899  5.385165  5.385165  3.741657  3.741657  \n",
      "Doc 19  6.082763  4.795832  4.582576  5.744563  5.385165  4.000000  4.000000  \n",
      "Doc 20  6.480741  5.291503  4.690416  5.656854  5.477226  3.872983  3.872983  \n",
      "Doc 21  6.324555  5.291503  5.099020  5.830952  5.830952  4.582576  4.582576  \n",
      "Doc 22  6.082763  4.358899  3.872983  5.385165  4.795832  2.828427  2.828427  \n",
      "Doc 23  6.480741  4.898979  4.472136  5.656854  4.898979  3.605551  3.605551  \n",
      "Doc 24  6.403124  5.567764  5.196152  6.082763  5.567764  4.472136  4.472136  \n",
      "Doc 25  6.403124  5.744563  5.385165  6.082763  6.244998  4.898979  4.690416  \n",
      "Doc 26  6.928203  6.324555  6.164414  6.633250  6.164414  5.744563  5.744563  \n",
      "Doc 27  6.082763  5.916080  5.567764  5.916080  6.082763  5.099020  4.898979  \n",
      "Doc 28  7.071068  6.000000  5.830952  6.782330  6.633250  5.567764  5.567764  \n",
      "Doc 29  6.082763  4.795832  4.358899  5.000000  4.795832  3.464102  3.464102  \n",
      "Doc 30  6.324555  5.099020  4.472136  5.656854  5.477226  3.872983  3.872983  \n",
      "Doc 31  0.000000  6.324555  6.164414  6.480741  6.480741  5.567764  5.744563  \n",
      "Doc 32  6.324555  0.000000  4.472136  6.000000  5.477226  4.123106  4.123106  \n",
      "Doc 33  6.164414  4.472136  0.000000  5.477226  5.099020  3.316625  3.316625  \n",
      "Doc 34  6.480741  6.000000  5.477226  0.000000  5.830952  5.000000  4.795832  \n",
      "Doc 35  6.480741  5.477226  5.099020  5.830952  0.000000  4.358899  4.358899  \n",
      "Doc 36  5.567764  4.123106  3.316625  5.000000  4.358899  0.000000  1.414214  \n",
      "Doc 37  5.744563  4.123106  3.316625  4.795832  4.358899  1.414214  0.000000  \n",
      "\n",
      "[37 rows x 37 columns]\n",
      "           Doc 1     Doc 2     Doc 3      Doc 4     Doc 5     Doc 6     Doc 7  \\\n",
      "Doc 1   0.000000  6.633250  5.656854   6.928203  5.385165  7.141428  7.000000   \n",
      "Doc 2   6.633250  0.000000  6.480741   8.000000  7.000000  6.708204  7.810250   \n",
      "Doc 3   5.656854  6.480741  0.000000   6.782330  5.385165  7.549834  6.855655   \n",
      "Doc 4   6.928203  8.000000  6.782330   0.000000  6.403124  7.681146  8.062258   \n",
      "Doc 5   5.385165  7.000000  5.385165   6.403124  0.000000  6.928203  7.071068   \n",
      "Doc 6   7.141428  6.708204  7.549834   7.681146  6.928203  0.000000  8.831761   \n",
      "Doc 7   7.000000  7.810250  6.855655   8.062258  7.071068  8.831761  0.000000   \n",
      "Doc 8   7.000000  7.280110  7.416198   7.681146  7.071068  6.928203  8.124038   \n",
      "Doc 9   5.196152  6.708204  5.567764   6.855655  5.656854  7.071068  6.633250   \n",
      "Doc 10  6.633250  7.348469  6.324555   7.483315  6.244998  8.185353  7.937254   \n",
      "Doc 11  5.477226  6.480741  5.477226   6.782330  5.385165  7.141428  7.416198   \n",
      "Doc 12  5.385165  5.744563  5.196152   6.403124  5.477226  6.480741  7.211103   \n",
      "Doc 13  6.633250  7.615773  7.483315   8.124038  7.141428  7.681146  8.306624   \n",
      "Doc 14  0.000000  6.633250  5.656854   6.928203  5.385165  7.141428  7.000000   \n",
      "Doc 15  4.690416  5.830952  5.291503   6.782330  5.196152  6.244998  7.549834   \n",
      "Doc 16  4.582576  5.916080  4.582576   6.082763  4.690416  6.480741  6.480741   \n",
      "Doc 17  6.708204  5.916080  6.855655   8.185353  7.211103  6.480741  8.246211   \n",
      "Doc 18  6.000000  6.782330  5.477226   7.211103  5.916080  7.681146  6.708204   \n",
      "Doc 19  5.477226  6.000000  5.656854   6.480741  5.196152  6.403124  6.855655   \n",
      "Doc 20  6.708204  7.549834  6.244998   7.280110  6.480741  7.745967  7.071068   \n",
      "Doc 21  6.324555  6.782330  6.324555   7.348469  5.916080  6.557439  7.549834   \n",
      "Doc 22  5.000000  6.403124  4.795832   6.082763  4.472136  6.782330  6.480741   \n",
      "Doc 23  5.477226  6.782330  5.477226   6.000000  5.000000  6.855655  7.141428   \n",
      "Doc 24  5.744563  7.000000  6.082763   7.000000  5.830952  7.745967  6.782330   \n",
      "Doc 25  7.348469  6.633250  6.928203   8.366600  7.681146  6.708204  8.426150   \n",
      "Doc 26  7.280110  7.416198  7.416198   7.810250  7.348469  7.745967  8.831761   \n",
      "Doc 27  7.745967  6.164414  7.745967   8.831761  8.062258  6.855655  9.219544   \n",
      "Doc 28  8.544004  8.660254  8.774964   9.848858  8.831761  9.165151  9.273618   \n",
      "Doc 29  5.385165  6.403124  5.385165   5.916080  5.099020  6.928203  7.071068   \n",
      "Doc 30  5.567764  5.744563  5.385165   6.557439  5.656854  6.324555  7.071068   \n",
      "Doc 31  8.306624  7.000000  8.774964  10.246951  9.165151  8.246211  9.380832   \n",
      "Doc 32  5.744563  5.744563  5.744563   7.280110  5.830952  6.782330  6.928203   \n",
      "Doc 33  6.082763  6.403124  5.744563   7.280110  6.164414  7.483315  7.211103   \n",
      "Doc 34  6.480741  6.000000  5.656854   7.071068  6.708204  6.855655  8.062258   \n",
      "Doc 35  5.830952  7.348469  6.000000   6.782330  5.916080  7.810250  7.549834   \n",
      "Doc 36  4.582576  6.082763  4.123106   5.744563  4.242641  6.782330  5.830952   \n",
      "Doc 37  4.582576  6.082763  4.123106   5.567764  4.000000  6.782330  6.000000   \n",
      "\n",
      "           Doc 8     Doc 9     Doc 10  ...     Doc 28    Doc 29    Doc 30  \\\n",
      "Doc 1   7.000000  5.196152   6.633250  ...   8.544004  5.385165  5.567764   \n",
      "Doc 2   7.280110  6.708204   7.348469  ...   8.660254  6.403124  5.744563   \n",
      "Doc 3   7.416198  5.567764   6.324555  ...   8.774964  5.385165  5.385165   \n",
      "Doc 4   7.681146  6.855655   7.483315  ...   9.848858  5.916080  6.557439   \n",
      "Doc 5   7.071068  5.656854   6.244998  ...   8.831761  5.099020  5.656854   \n",
      "Doc 6   6.928203  7.071068   8.185353  ...   9.165151  6.928203  6.324555   \n",
      "Doc 7   8.124038  6.633250   7.937254  ...   9.273618  7.071068  7.071068   \n",
      "Doc 8   0.000000  7.071068   7.549834  ...   9.591663  6.164414  6.782330   \n",
      "Doc 9   7.071068  0.000000   6.557439  ...   8.944272  5.477226  5.477226   \n",
      "Doc 10  7.549834  6.557439   0.000000  ...   9.433981  5.567764  6.403124   \n",
      "Doc 11  6.855655  5.385165   5.477226  ...   9.000000  4.795832  5.196152   \n",
      "Doc 12  6.928203  5.477226   6.244998  ...   8.246211  4.690416  4.898979   \n",
      "Doc 13  7.810250  7.141428   7.615773  ...   9.949874  6.557439  6.855655   \n",
      "Doc 14  7.000000  5.196152   6.633250  ...   8.544004  5.385165  5.567764   \n",
      "Doc 15  6.708204  5.385165   6.000000  ...   9.000000  5.000000  4.795832   \n",
      "Doc 16  6.633250  4.690416   5.744563  ...   8.485281  4.242641  4.000000   \n",
      "Doc 17  7.211103  6.633250   7.937254  ...   9.273618  6.480741  6.164414   \n",
      "Doc 18  7.937254  6.082763   7.071068  ...   7.280110  6.082763  5.744563   \n",
      "Doc 19  6.855655  5.916080   6.324555  ...   7.681146  5.000000  5.385165   \n",
      "Doc 20  7.615773  6.324555   7.280110  ...   8.124038  6.324555  5.830952   \n",
      "Doc 21  7.141428  6.082763   6.782330  ...   8.544004  5.744563  5.744563   \n",
      "Doc 22  6.782330  5.099020   5.385165  ...   7.874008  4.000000  4.690416   \n",
      "Doc 23  6.855655  5.567764   5.656854  ...   8.426150  4.123106  5.196152   \n",
      "Doc 24  7.071068  5.656854   6.082763  ...   8.602325  4.898979  5.830952   \n",
      "Doc 25  7.937254  7.280110   8.831761  ...   8.660254  7.937254  6.403124   \n",
      "Doc 26  7.615773  7.483315   7.549834  ...  10.198039  6.928203  6.928203   \n",
      "Doc 27  7.549834  8.062258   8.831761  ...   9.219544  7.937254  7.141428   \n",
      "Doc 28  9.591663  8.944272   9.433981  ...   0.000000  8.944272  8.485281   \n",
      "Doc 29  6.164414  5.477226   5.567764  ...   8.944272  0.000000  5.099020   \n",
      "Doc 30  6.782330  5.477226   6.403124  ...   8.485281  5.099020  0.000000   \n",
      "Doc 31  9.165151  8.602325  10.148892  ...   9.380832  9.486833  8.485281   \n",
      "Doc 32  7.483315  6.000000   7.000000  ...   8.246211  5.656854  5.477226   \n",
      "Doc 33  7.483315  6.164414   7.000000  ...   8.602325  5.830952  5.477226   \n",
      "Doc 34  7.280110  5.916080   6.633250  ...   8.888194  5.744563  5.744563   \n",
      "Doc 35  7.549834  5.916080   6.000000  ...   9.643651  5.385165  6.244998   \n",
      "Doc 36  6.480741  4.690416   5.196152  ...   8.366600  3.464102  4.242641   \n",
      "Doc 37  6.480741  4.690416   5.196152  ...   8.366600  3.464102  4.242641   \n",
      "\n",
      "           Doc 31    Doc 32    Doc 33    Doc 34    Doc 35    Doc 36    Doc 37  \n",
      "Doc 1    8.306624  5.744563  6.082763  6.480741  5.830952  4.582576  4.582576  \n",
      "Doc 2    7.000000  5.744563  6.403124  6.000000  7.348469  6.082763  6.082763  \n",
      "Doc 3    8.774964  5.744563  5.744563  5.656854  6.000000  4.123106  4.123106  \n",
      "Doc 4   10.246951  7.280110  7.280110  7.071068  6.782330  5.744563  5.567764  \n",
      "Doc 5    9.165151  5.830952  6.164414  6.708204  5.916080  4.242641  4.000000  \n",
      "Doc 6    8.246211  6.782330  7.483315  6.855655  7.810250  6.782330  6.782330  \n",
      "Doc 7    9.380832  6.928203  7.211103  8.062258  7.549834  5.830952  6.000000  \n",
      "Doc 8    9.165151  7.483315  7.483315  7.280110  7.549834  6.480741  6.480741  \n",
      "Doc 9    8.602325  6.000000  6.164414  5.916080  5.916080  4.690416  4.690416  \n",
      "Doc 10  10.148892  7.000000  7.000000  6.633250  6.000000  5.196152  5.196152  \n",
      "Doc 11   9.327379  5.916080  5.916080  6.000000  5.830952  4.123106  4.123106  \n",
      "Doc 12   8.246211  5.291503  5.477226  5.385165  6.082763  4.242641  4.242641  \n",
      "Doc 13   9.539392  7.416198  7.681146  7.348469  7.745967  6.403124  6.403124  \n",
      "Doc 14   8.306624  5.744563  6.082763  6.480741  5.830952  4.582576  4.582576  \n",
      "Doc 15   8.544004  5.385165  5.744563  5.830952  5.291503  4.358899  4.358899  \n",
      "Doc 16   8.602325  5.099020  5.099020  5.744563  5.196152  2.828427  3.162278  \n",
      "Doc 17   8.000000  6.480741  6.928203  5.744563  7.681146  6.633250  6.633250  \n",
      "Doc 18   8.185353  5.567764  5.744563  6.480741  7.071068  4.795832  4.795832  \n",
      "Doc 19   8.185353  5.385165  5.916080  6.324555  6.164414  4.358899  4.358899  \n",
      "Doc 20   9.486833  6.633250  6.633250  7.141428  7.280110  5.291503  5.291503  \n",
      "Doc 21   9.000000  6.244998  6.708204  6.633250  6.782330  5.196152  5.196152  \n",
      "Doc 22   9.273618  5.099020  5.291503  6.082763  5.385165  2.828427  2.828427  \n",
      "Doc 23   9.746794  5.744563  5.916080  6.324555  5.477226  3.605551  3.605551  \n",
      "Doc 24   9.591663  6.324555  6.480741  6.708204  6.082763  4.472136  4.472136  \n",
      "Doc 25   7.280110  7.000000  7.141428  6.782330  8.717798  7.141428  7.000000  \n",
      "Doc 26   9.695360  7.615773  7.874008  7.280110  7.141428  6.928203  6.928203  \n",
      "Doc 27   6.708204  7.416198  7.810250  6.928203  8.717798  7.810250  7.681146  \n",
      "Doc 28   9.380832  8.246211  8.602325  8.888194  9.643651  8.366600  8.366600  \n",
      "Doc 29   9.486833  5.656854  5.830952  5.744563  5.385165  3.464102  3.464102  \n",
      "Doc 30   8.485281  5.477226  5.477226  5.744563  6.244998  4.242641  4.242641  \n",
      "Doc 31   0.000000  8.246211  8.831761  8.185353  9.949874  8.944272  9.055385  \n",
      "Doc 32   8.246211  0.000000  5.656854  6.403124  6.708204  4.898979  4.898979  \n",
      "Doc 33   8.831761  5.656854  0.000000  6.403124  6.855655  4.898979  4.898979  \n",
      "Doc 34   8.185353  6.403124  6.403124  0.000000  6.782330  5.744563  5.567764  \n",
      "Doc 35   9.949874  6.708204  6.855655  6.782330  0.000000  5.000000  5.000000  \n",
      "Doc 36   8.944272  4.898979  4.898979  5.744563  5.000000  0.000000  1.414214  \n",
      "Doc 37   9.055385  4.898979  4.898979  5.567764  5.000000  1.414214  0.000000  \n",
      "\n",
      "[37 rows x 37 columns]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "# 1. Diviser le texte en phrases\n",
    "def split_into_sentences(text):\n",
    "    # Utilisation d'une expression régulière pour diviser les phrases sur les points, points d'exclamation, points d'interrogation\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text.strip())\n",
    "    return sentences\n",
    "\n",
    "# 2. Prétraitement du texte\n",
    "def preprocess_text(sentences):\n",
    "    unique_tokens = set()  # Utiliser un ensemble pour les mots uniques\n",
    "    for sentence in sentences:\n",
    "        # Nettoyer et tokeniser\n",
    "        tokens = re.findall(r'\\b\\w+\\b', sentence.lower())\n",
    "        unique_tokens.update(tokens)  # Ajouter les mots uniques\n",
    "    return sorted(unique_tokens)  # Retourner une liste triée\n",
    "\n",
    "# 3. Créer la matrice binaire et la matrice d'occurrences\n",
    "def create_matrices(sentences, unique_tokens):\n",
    "    binary_matrix = []\n",
    "    occurrence_matrix = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        # Tokeniser la phrase\n",
    "        tokens = re.findall(r'\\b\\w+\\b', sentence.lower())\n",
    "        \n",
    "        # Créer une ligne pour la matrice binaire\n",
    "        binary_row = [1 if token in tokens else 0 for token in unique_tokens]\n",
    "        binary_matrix.append(binary_row)\n",
    "        \n",
    "        # Créer une ligne pour la matrice d'occurrences\n",
    "        occurrence_row = [tokens.count(token) for token in unique_tokens]\n",
    "        occurrence_matrix.append(occurrence_row)\n",
    "\n",
    "    return np.array(binary_matrix), np.array(occurrence_matrix)\n",
    "\n",
    "# 4. Calculer la distance de Manhattan\n",
    "def calculate_manhattan_distance(matrix):\n",
    "    return squareform(pdist(matrix, metric='cityblock'))\n",
    "\n",
    "# 5. Calculer la distance euclidienne\n",
    "def calculate_euclidean_distance(matrix):\n",
    "    return squareform(pdist(matrix, metric='euclidean'))\n",
    "\n",
    "# Exemple de texte\n",
    "\n",
    "\n",
    "# Traitement des phrases\n",
    "sentences = split_into_sentences(chiraq)\n",
    "unique_tokens = preprocess_text(sentences)\n",
    "\n",
    "# Création des matrices\n",
    "binary_matrix, occurrence_matrix = create_matrices(sentences, unique_tokens)\n",
    "\n",
    "# Calculer les distances de Manhattan\n",
    "binary_distance_manhattan = calculate_manhattan_distance(binary_matrix)\n",
    "occurrence_distance_manhattan = calculate_manhattan_distance(occurrence_matrix)\n",
    "\n",
    "# Calculer les distances euclidiennes\n",
    "binary_distance_euclidean = calculate_euclidean_distance(binary_matrix)\n",
    "occurrence_distance_euclidean = calculate_euclidean_distance(occurrence_matrix)\n",
    "\n",
    "# Créer des DataFrames pour afficher les distances\n",
    "binary_distance_manhattan_df = pd.DataFrame(binary_distance_manhattan, \n",
    "                                             columns=[f'Doc {i+1}' for i in range(len(sentences))],\n",
    "                                             index=[f'Doc {i+1}' for i in range(len(sentences))])\n",
    "\n",
    "occurrence_distance_manhattan_df = pd.DataFrame(occurrence_distance_manhattan, \n",
    "                                                columns=[f'Doc {i+1}' for i in range(len(sentences))],\n",
    "                                                index=[f'Doc {i+1}' for i in range(len(sentences))])\n",
    "\n",
    "binary_distance_euclidean_df = pd.DataFrame(binary_distance_euclidean, \n",
    "                                             columns=[f'Doc {i+1}' for i in range(len(sentences))],\n",
    "                                             index=[f'Doc {i+1}' for i in range(len(sentences))])\n",
    "\n",
    "occurrence_distance_euclidean_df = pd.DataFrame(occurrence_distance_euclidean, \n",
    "                                                columns=[f'Doc {i+1}' for i in range(len(sentences))],\n",
    "                                                index=[f'Doc {i+1}' for i in range(len(sentences))])\n",
    "\n",
    "# Affichage des DataFrames pour vérifier les résultats\n",
    "print(binary_distance_manhattan_df)\n",
    "print(occurrence_distance_manhattan_df)\n",
    "print(binary_distance_euclidean_df)\n",
    "print(occurrence_distance_euclidean_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82326196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrice Binaire (Bag of Words) :\n",
      "[[0 1 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 1 0]\n",
      " [1 0 0 ... 0 0 0]\n",
      " ...\n",
      " [1 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Afficher les résultats\n",
    "print(\"Matrice Binaire (Bag of Words) :\")\n",
    "print(binary_matrix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "825e545b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Matrice d'Occurrences (Bag of Words) :\n",
      "[[0 1 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 1 0]\n",
      " [1 0 0 ... 0 0 0]\n",
      " ...\n",
      " [1 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nMatrice d'Occurrences (Bag of Words) :\")\n",
    "print(occurrence_matrix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af47a96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarity_matrix(distance_matrix):\n",
    "    max_distance = np.max(distance_matrix)\n",
    "    return 1 - (distance_matrix / max_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4780c809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrice Binaire (Bag of Words) :\n",
      "[[0 1 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 1 0]\n",
      " [1 0 0 ... 0 0 0]\n",
      " ...\n",
      " [1 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Afficher les résultats\n",
    "print(\"Matrice Binaire (Bag of Words) :\")\n",
    "print(binary_matrix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3eb82a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Matrice de Distance de Manhattan (Binaire) :\n",
      "        Doc 1  Doc 2  Doc 3  Doc 4  Doc 5  Doc 6  Doc 7  Doc 8  Doc 9  Doc 10  \\\n",
      "Doc 1     0.0   35.0   31.0   32.0   29.0   35.0   33.0   35.0   27.0    33.0   \n",
      "Doc 2    35.0    0.0   32.0   33.0   34.0   36.0   30.0   36.0   36.0    30.0   \n",
      "Doc 3    31.0   32.0    0.0   29.0   28.0   36.0   30.0   38.0   30.0    28.0   \n",
      "Doc 4    32.0   33.0   29.0    0.0   25.0   29.0   29.0   37.0   31.0    29.0   \n",
      "Doc 5    29.0   34.0   28.0   25.0    0.0   30.0   30.0   36.0   32.0    28.0   \n",
      "Doc 6    35.0   36.0   36.0   29.0   30.0    0.0   36.0   34.0   36.0    34.0   \n",
      "Doc 7    33.0   30.0   30.0   29.0   30.0   36.0    0.0   36.0   28.0    28.0   \n",
      "Doc 8    35.0   36.0   38.0   37.0   36.0   34.0   36.0    0.0   40.0    34.0   \n",
      "Doc 9    27.0   36.0   30.0   31.0   32.0   36.0   28.0   40.0    0.0    32.0   \n",
      "Doc 10   33.0   30.0   28.0   29.0   28.0   34.0   28.0   34.0   32.0     0.0   \n",
      "Doc 11   27.0   30.0   24.0   27.0   26.0   30.0   28.0   32.0   26.0    20.0   \n",
      "Doc 12   28.0   29.0   25.0   26.0   27.0   29.0   29.0   33.0   29.0    25.0   \n",
      "Doc 13   39.0   38.0   42.0   39.0   40.0   38.0   38.0   42.0   42.0    36.0   \n",
      "Doc 14    0.0   35.0   31.0   32.0   29.0   35.0   33.0   35.0   27.0    33.0   \n",
      "Doc 15   21.0   30.0   24.0   27.0   24.0   26.0   30.0   30.0   28.0    24.0   \n",
      "Doc 16   21.0   26.0   20.0   21.0   22.0   24.0   22.0   30.0   22.0    22.0   \n",
      "Doc 17   30.0   33.0   31.0   32.0   31.0   33.0   33.0   39.0   31.0    33.0   \n",
      "Doc 18   27.0   28.0   26.0   27.0   26.0   28.0   28.0   34.0   28.0    24.0   \n",
      "Doc 19   29.0   26.0   28.0   27.0   26.0   28.0   28.0   32.0   32.0    26.0   \n",
      "Doc 20   32.0   31.0   29.0   26.0   29.0   31.0   27.0   37.0   29.0    25.0   \n",
      "Doc 21   34.0   31.0   31.0   32.0   31.0   31.0   31.0   35.0   31.0    29.0   \n",
      "Doc 22   25.0   26.0   22.0   21.0   20.0   26.0   22.0   32.0   26.0    18.0   \n",
      "Doc 23   30.0   31.0   27.0   24.0   25.0   27.0   27.0   33.0   31.0    21.0   \n",
      "Doc 24   33.0   34.0   34.0   33.0   34.0   38.0   30.0   38.0   32.0    26.0   \n",
      "Doc 25   33.0   36.0   30.0   31.0   32.0   32.0   36.0   40.0   36.0    36.0   \n",
      "Doc 26   40.0   41.0   41.0   40.0   39.0   41.0   43.0   39.0   43.0    37.0   \n",
      "Doc 27   37.0   32.0   32.0   33.0   34.0   38.0   36.0   32.0   40.0    34.0   \n",
      "Doc 28   40.0   39.0   43.0   42.0   43.0   43.0   39.0   45.0   43.0    39.0   \n",
      "Doc 29   29.0   26.0   26.0   23.0   26.0   26.0   26.0   26.0   30.0    20.0   \n",
      "Doc 30   30.0   29.0   27.0   24.0   29.0   29.0   27.0   35.0   29.0    27.0   \n",
      "Doc 31   34.0   31.0   39.0   42.0   41.0   45.0   37.0   43.0   37.0    41.0   \n",
      "Doc 32   28.0   25.0   29.0   30.0   27.0   29.0   29.0   35.0   31.0    29.0   \n",
      "Doc 33   26.0   27.0   23.0   24.0   25.0   29.0   23.0   31.0   27.0    23.0   \n",
      "Doc 34   38.0   35.0   27.0   30.0   37.0   37.0   37.0   41.0   31.0    31.0   \n",
      "Doc 35   28.0   35.0   29.0   28.0   29.0   33.0   31.0   37.0   29.0    25.0   \n",
      "Doc 36   21.0   22.0   16.0   17.0   18.0   24.0   14.0   28.0   22.0    16.0   \n",
      "Doc 37   21.0   22.0   16.0   15.0   16.0   24.0   16.0   28.0   22.0    16.0   \n",
      "\n",
      "        ...  Doc 28  Doc 29  Doc 30  Doc 31  Doc 32  Doc 33  Doc 34  Doc 35  \\\n",
      "Doc 1   ...    40.0    29.0    30.0    34.0    28.0    26.0    38.0    28.0   \n",
      "Doc 2   ...    39.0    26.0    29.0    31.0    25.0    27.0    35.0    35.0   \n",
      "Doc 3   ...    43.0    26.0    27.0    39.0    29.0    23.0    27.0    29.0   \n",
      "Doc 4   ...    42.0    23.0    24.0    42.0    30.0    24.0    30.0    28.0   \n",
      "Doc 5   ...    43.0    26.0    29.0    41.0    27.0    25.0    37.0    29.0   \n",
      "Doc 6   ...    43.0    26.0    29.0    45.0    29.0    29.0    37.0    33.0   \n",
      "Doc 7   ...    39.0    26.0    27.0    37.0    29.0    23.0    37.0    31.0   \n",
      "Doc 8   ...    45.0    26.0    35.0    43.0    35.0    31.0    41.0    37.0   \n",
      "Doc 9   ...    43.0    30.0    29.0    37.0    31.0    27.0    31.0    29.0   \n",
      "Doc 10  ...    39.0    20.0    27.0    41.0    29.0    23.0    31.0    25.0   \n",
      "Doc 11  ...    37.0    20.0    23.0    39.0    25.0    19.0    29.0    25.0   \n",
      "Doc 12  ...    36.0    19.0    24.0    40.0    24.0    20.0    28.0    28.0   \n",
      "Doc 13  ...    51.0    32.0    35.0    45.0    37.0    35.0    41.0    43.0   \n",
      "Doc 14  ...    40.0    29.0    30.0    34.0    28.0    26.0    38.0    28.0   \n",
      "Doc 15  ...    41.0    22.0    23.0    35.0    23.0    21.0    33.0    21.0   \n",
      "Doc 16  ...    35.0    18.0    15.0    33.0    21.0    15.0    29.0    21.0   \n",
      "Doc 17  ...    42.0    23.0    30.0    40.0    28.0    28.0    30.0    32.0   \n",
      "Doc 18  ...    31.0    22.0    23.0    37.0    23.0    19.0    29.0    29.0   \n",
      "Doc 19  ...    33.0    22.0    25.0    37.0    23.0    21.0    33.0    29.0   \n",
      "Doc 20  ...    34.0    23.0    22.0    42.0    28.0    22.0    32.0    30.0   \n",
      "Doc 21  ...    38.0    27.0    26.0    40.0    28.0    26.0    34.0    34.0   \n",
      "Doc 22  ...    31.0    16.0    19.0    37.0    19.0    15.0    29.0    23.0   \n",
      "Doc 23  ...    36.0    17.0    24.0    42.0    24.0    20.0    32.0    24.0   \n",
      "Doc 24  ...    39.0    24.0    31.0    41.0    31.0    27.0    37.0    31.0   \n",
      "Doc 25  ...    39.0    32.0    29.0    41.0    33.0    29.0    37.0    39.0   \n",
      "Doc 26  ...    54.0    35.0    38.0    48.0    40.0    38.0    44.0    38.0   \n",
      "Doc 27  ...    47.0    28.0    33.0    37.0    35.0    31.0    35.0    37.0   \n",
      "Doc 28  ...     0.0    37.0    36.0    50.0    36.0    34.0    46.0    44.0   \n",
      "Doc 29  ...    37.0     0.0    23.0    37.0    23.0    19.0    25.0    23.0   \n",
      "Doc 30  ...    36.0    23.0     0.0    40.0    26.0    20.0    32.0    30.0   \n",
      "Doc 31  ...    50.0    37.0    40.0     0.0    40.0    38.0    42.0    42.0   \n",
      "Doc 32  ...    36.0    23.0    26.0    40.0     0.0    20.0    36.0    30.0   \n",
      "Doc 33  ...    34.0    19.0    20.0    38.0    20.0     0.0    30.0    26.0   \n",
      "Doc 34  ...    46.0    25.0    32.0    42.0    36.0    30.0     0.0    34.0   \n",
      "Doc 35  ...    44.0    23.0    30.0    42.0    30.0    26.0    34.0     0.0   \n",
      "Doc 36  ...    31.0    12.0    15.0    31.0    17.0    11.0    25.0    19.0   \n",
      "Doc 37  ...    31.0    12.0    15.0    33.0    17.0    11.0    23.0    19.0   \n",
      "\n",
      "        Doc 36  Doc 37  \n",
      "Doc 1     21.0    21.0  \n",
      "Doc 2     22.0    22.0  \n",
      "Doc 3     16.0    16.0  \n",
      "Doc 4     17.0    15.0  \n",
      "Doc 5     18.0    16.0  \n",
      "Doc 6     24.0    24.0  \n",
      "Doc 7     14.0    16.0  \n",
      "Doc 8     28.0    28.0  \n",
      "Doc 9     22.0    22.0  \n",
      "Doc 10    16.0    16.0  \n",
      "Doc 11    14.0    14.0  \n",
      "Doc 12    15.0    15.0  \n",
      "Doc 13    30.0    30.0  \n",
      "Doc 14    21.0    21.0  \n",
      "Doc 15    16.0    16.0  \n",
      "Doc 16     8.0    10.0  \n",
      "Doc 17    23.0    23.0  \n",
      "Doc 18    14.0    14.0  \n",
      "Doc 19    16.0    16.0  \n",
      "Doc 20    15.0    15.0  \n",
      "Doc 21    21.0    21.0  \n",
      "Doc 22     8.0     8.0  \n",
      "Doc 23    13.0    13.0  \n",
      "Doc 24    20.0    20.0  \n",
      "Doc 25    24.0    22.0  \n",
      "Doc 26    33.0    33.0  \n",
      "Doc 27    26.0    24.0  \n",
      "Doc 28    31.0    31.0  \n",
      "Doc 29    12.0    12.0  \n",
      "Doc 30    15.0    15.0  \n",
      "Doc 31    31.0    33.0  \n",
      "Doc 32    17.0    17.0  \n",
      "Doc 33    11.0    11.0  \n",
      "Doc 34    25.0    23.0  \n",
      "Doc 35    19.0    19.0  \n",
      "Doc 36     0.0     2.0  \n",
      "Doc 37     2.0     0.0  \n",
      "\n",
      "[37 rows x 37 columns]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nMatrice de Distance de Manhattan (Binaire) :\")\n",
    "print(binary_distance_manhattan_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8b95054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Matrice de Distance de Manhattan (Occurrences) :\n",
      "        Doc 1  Doc 2  Doc 3  Doc 4  Doc 5  Doc 6  Doc 7  Doc 8  Doc 9  Doc 10  \\\n",
      "Doc 1     0.0   38.0   32.0   36.0   29.0   41.0   39.0   39.0   27.0    36.0   \n",
      "Doc 2    38.0    0.0   36.0   40.0   37.0   41.0   39.0   43.0   39.0    36.0   \n",
      "Doc 3    32.0   36.0    0.0   34.0   29.0   43.0   35.0   43.0   31.0    32.0   \n",
      "Doc 4    36.0   40.0   34.0    0.0   29.0   39.0   39.0   45.0   35.0    36.0   \n",
      "Doc 5    29.0   37.0   29.0   29.0    0.0   36.0   36.0   40.0   32.0    31.0   \n",
      "Doc 6    41.0   41.0   43.0   39.0   36.0    0.0   48.0   40.0   42.0    43.0   \n",
      "Doc 7    39.0   39.0   35.0   39.0   36.0   48.0    0.0   46.0   34.0    37.0   \n",
      "Doc 8    39.0   43.0   43.0   45.0   40.0   40.0   46.0    0.0   44.0    41.0   \n",
      "Doc 9    27.0   39.0   31.0   35.0   32.0   42.0   34.0   44.0    0.0    35.0   \n",
      "Doc 10   36.0   36.0   32.0   36.0   31.0   43.0   37.0   41.0   35.0     0.0   \n",
      "Doc 11   28.0   34.0   26.0   32.0   27.0   37.0   35.0   37.0   27.0    24.0   \n",
      "Doc 12   29.0   31.0   27.0   31.0   28.0   34.0   36.0   38.0   30.0    29.0   \n",
      "Doc 13   42.0   44.0   46.0   46.0   43.0   47.0   47.0   49.0   45.0    42.0   \n",
      "Doc 14    0.0   38.0   32.0   36.0   29.0   41.0   39.0   39.0   27.0    36.0   \n",
      "Doc 15   22.0   32.0   26.0   32.0   25.0   31.0   37.0   35.0   29.0    28.0   \n",
      "Doc 16   21.0   29.0   21.0   25.0   22.0   30.0   28.0   34.0   22.0    25.0   \n",
      "Doc 17   35.0   35.0   37.0   41.0   36.0   38.0   44.0   44.0   36.0    41.0   \n",
      "Doc 18   30.0   34.0   28.0   34.0   29.0   37.0   33.0   41.0   31.0    30.0   \n",
      "Doc 19   30.0   30.0   30.0   32.0   27.0   33.0   35.0   37.0   33.0    30.0   \n",
      "Doc 20   37.0   39.0   33.0   35.0   34.0   40.0   34.0   44.0   34.0    33.0   \n",
      "Doc 21   36.0   36.0   34.0   38.0   33.0   37.0   39.0   41.0   33.0    34.0   \n",
      "Doc 22   25.0   29.0   23.0   25.0   20.0   32.0   28.0   36.0   26.0    21.0   \n",
      "Doc 23   30.0   34.0   28.0   28.0   25.0   33.0   33.0   37.0   31.0    24.0   \n",
      "Doc 24   33.0   37.0   35.0   37.0   34.0   44.0   36.0   42.0   32.0    29.0   \n",
      "Doc 25   40.0   40.0   36.0   42.0   39.0   37.0   45.0   47.0   43.0    46.0   \n",
      "Doc 26   45.0   47.0   47.0   47.0   44.0   50.0   54.0   48.0   48.0    43.0   \n",
      "Doc 27   44.0   36.0   40.0   44.0   41.0   43.0   49.0   41.0   47.0    44.0   \n",
      "Doc 28   51.0   51.0   53.0   57.0   54.0   56.0   52.0   60.0   54.0    53.0   \n",
      "Doc 29   29.0   29.0   27.0   27.0   26.0   32.0   32.0   30.0   30.0    23.0   \n",
      "Doc 30   31.0   31.0   29.0   29.0   30.0   34.0   34.0   40.0   30.0    31.0   \n",
      "Doc 31   45.0   39.0   49.0   57.0   52.0   54.0   50.0   56.0   48.0    55.0   \n",
      "Doc 32   31.0   29.0   31.0   37.0   30.0   36.0   36.0   42.0   34.0    35.0   \n",
      "Doc 33   31.0   33.0   27.0   33.0   30.0   38.0   32.0   40.0   32.0    31.0   \n",
      "Doc 34   40.0   36.0   30.0   36.0   39.0   41.0   45.0   47.0   33.0    36.0   \n",
      "Doc 35   30.0   40.0   32.0   34.0   31.0   41.0   39.0   43.0   31.0    28.0   \n",
      "Doc 36   21.0   25.0   17.0   21.0   18.0   30.0   20.0   32.0   22.0    19.0   \n",
      "Doc 37   21.0   25.0   17.0   19.0   16.0   30.0   22.0   32.0   22.0    19.0   \n",
      "\n",
      "        ...  Doc 28  Doc 29  Doc 30  Doc 31  Doc 32  Doc 33  Doc 34  Doc 35  \\\n",
      "Doc 1   ...    51.0    29.0    31.0    45.0    31.0    31.0    40.0    30.0   \n",
      "Doc 2   ...    51.0    29.0    31.0    39.0    29.0    33.0    36.0    40.0   \n",
      "Doc 3   ...    53.0    27.0    29.0    49.0    31.0    27.0    30.0    32.0   \n",
      "Doc 4   ...    57.0    27.0    29.0    57.0    37.0    33.0    36.0    34.0   \n",
      "Doc 5   ...    54.0    26.0    30.0    52.0    30.0    30.0    39.0    31.0   \n",
      "Doc 6   ...    56.0    32.0    34.0    54.0    36.0    38.0    41.0    41.0   \n",
      "Doc 7   ...    52.0    32.0    34.0    50.0    36.0    32.0    45.0    39.0   \n",
      "Doc 8   ...    60.0    30.0    40.0    56.0    42.0    40.0    47.0    43.0   \n",
      "Doc 9   ...    54.0    30.0    30.0    48.0    34.0    32.0    33.0    31.0   \n",
      "Doc 10  ...    53.0    23.0    31.0    55.0    35.0    31.0    36.0    28.0   \n",
      "Doc 11  ...    49.0    21.0    25.0    51.0    29.0    25.0    32.0    28.0   \n",
      "Doc 12  ...    46.0    20.0    24.0    50.0    26.0    24.0    29.0    31.0   \n",
      "Doc 13  ...    65.0    35.0    39.0    59.0    43.0    43.0    46.0    48.0   \n",
      "Doc 14  ...    51.0    29.0    31.0    45.0    31.0    31.0    40.0    30.0   \n",
      "Doc 15  ...    51.0    23.0    23.0    45.0    25.0    25.0    34.0    24.0   \n",
      "Doc 16  ...    46.0    18.0    16.0    44.0    24.0    20.0    31.0    23.0   \n",
      "Doc 17  ...    56.0    28.0    34.0    48.0    34.0    36.0    33.0    39.0   \n",
      "Doc 18  ...    41.0    25.0    27.0    47.0    27.0    25.0    34.0    34.0   \n",
      "Doc 19  ...    43.0    23.0    27.0    47.0    27.0    27.0    36.0    32.0   \n",
      "Doc 20  ...    46.0    28.0    28.0    52.0    34.0    30.0    39.0    37.0   \n",
      "Doc 21  ...    51.0    29.0    29.0    53.0    33.0    33.0    38.0    38.0   \n",
      "Doc 22  ...    42.0    16.0    20.0    48.0    22.0    20.0    31.0    25.0   \n",
      "Doc 23  ...    47.0    17.0    25.0    53.0    27.0    25.0    34.0    26.0   \n",
      "Doc 24  ...    50.0    24.0    32.0    52.0    34.0    32.0    39.0    33.0   \n",
      "Doc 25  ...    51.0    39.0    35.0    47.0    39.0    37.0    42.0    48.0   \n",
      "Doc 26  ...    68.0    40.0    42.0    62.0    46.0    46.0    49.0    43.0   \n",
      "Doc 27  ...    59.0    35.0    39.0    41.0    43.0    41.0    40.0    46.0   \n",
      "Doc 28  ...     0.0    48.0    46.0    62.0    46.0    46.0    57.0    57.0   \n",
      "Doc 29  ...    48.0     0.0    24.0    48.0    26.0    24.0    27.0    25.0   \n",
      "Doc 30  ...    46.0    24.0     0.0    50.0    28.0    24.0    33.0    33.0   \n",
      "Doc 31  ...    62.0    48.0    50.0     0.0    50.0    50.0    51.0    55.0   \n",
      "Doc 32  ...    46.0    26.0    28.0    50.0     0.0    24.0    39.0    35.0   \n",
      "Doc 33  ...    46.0    24.0    24.0    50.0    24.0     0.0    35.0    33.0   \n",
      "Doc 34  ...    57.0    27.0    33.0    51.0    39.0    35.0     0.0    38.0   \n",
      "Doc 35  ...    57.0    25.0    33.0    55.0    35.0    33.0    38.0     0.0   \n",
      "Doc 36  ...    42.0    12.0    16.0    42.0    20.0    16.0    27.0    21.0   \n",
      "Doc 37  ...    42.0    12.0    16.0    44.0    20.0    16.0    25.0    21.0   \n",
      "\n",
      "        Doc 36  Doc 37  \n",
      "Doc 1     21.0    21.0  \n",
      "Doc 2     25.0    25.0  \n",
      "Doc 3     17.0    17.0  \n",
      "Doc 4     21.0    19.0  \n",
      "Doc 5     18.0    16.0  \n",
      "Doc 6     30.0    30.0  \n",
      "Doc 7     20.0    22.0  \n",
      "Doc 8     32.0    32.0  \n",
      "Doc 9     22.0    22.0  \n",
      "Doc 10    19.0    19.0  \n",
      "Doc 11    15.0    15.0  \n",
      "Doc 12    16.0    16.0  \n",
      "Doc 13    33.0    33.0  \n",
      "Doc 14    21.0    21.0  \n",
      "Doc 15    17.0    17.0  \n",
      "Doc 16     8.0    10.0  \n",
      "Doc 17    28.0    28.0  \n",
      "Doc 18    17.0    17.0  \n",
      "Doc 19    17.0    17.0  \n",
      "Doc 20    20.0    20.0  \n",
      "Doc 21    23.0    23.0  \n",
      "Doc 22     8.0     8.0  \n",
      "Doc 23    13.0    13.0  \n",
      "Doc 24    20.0    20.0  \n",
      "Doc 25    31.0    29.0  \n",
      "Doc 26    38.0    38.0  \n",
      "Doc 27    33.0    31.0  \n",
      "Doc 28    42.0    42.0  \n",
      "Doc 29    12.0    12.0  \n",
      "Doc 30    16.0    16.0  \n",
      "Doc 31    42.0    44.0  \n",
      "Doc 32    20.0    20.0  \n",
      "Doc 33    16.0    16.0  \n",
      "Doc 34    27.0    25.0  \n",
      "Doc 35    21.0    21.0  \n",
      "Doc 36     0.0     2.0  \n",
      "Doc 37     2.0     0.0  \n",
      "\n",
      "[37 rows x 37 columns]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nMatrice de Distance de Manhattan (Occurrences) :\")\n",
    "print(occurrence_distance_manhattan_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f7606dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Matrice de Distance Euclidienne (Binaire) :\n",
      "           Doc 1     Doc 2     Doc 3     Doc 4     Doc 5     Doc 6     Doc 7  \\\n",
      "Doc 1   0.000000  5.916080  5.567764  5.656854  5.385165  5.916080  5.744563   \n",
      "Doc 2   5.916080  0.000000  5.656854  5.744563  5.830952  6.000000  5.477226   \n",
      "Doc 3   5.567764  5.656854  0.000000  5.385165  5.291503  6.000000  5.477226   \n",
      "Doc 4   5.656854  5.744563  5.385165  0.000000  5.000000  5.385165  5.385165   \n",
      "Doc 5   5.385165  5.830952  5.291503  5.000000  0.000000  5.477226  5.477226   \n",
      "Doc 6   5.916080  6.000000  6.000000  5.385165  5.477226  0.000000  6.000000   \n",
      "Doc 7   5.744563  5.477226  5.477226  5.385165  5.477226  6.000000  0.000000   \n",
      "Doc 8   5.916080  6.000000  6.164414  6.082763  6.000000  5.830952  6.000000   \n",
      "Doc 9   5.196152  6.000000  5.477226  5.567764  5.656854  6.000000  5.291503   \n",
      "Doc 10  5.744563  5.477226  5.291503  5.385165  5.291503  5.830952  5.291503   \n",
      "Doc 11  5.196152  5.477226  4.898979  5.196152  5.099020  5.477226  5.291503   \n",
      "Doc 12  5.291503  5.385165  5.000000  5.099020  5.196152  5.385165  5.385165   \n",
      "Doc 13  6.244998  6.164414  6.480741  6.244998  6.324555  6.164414  6.164414   \n",
      "Doc 14  0.000000  5.916080  5.567764  5.656854  5.385165  5.916080  5.744563   \n",
      "Doc 15  4.582576  5.477226  4.898979  5.196152  4.898979  5.099020  5.477226   \n",
      "Doc 16  4.582576  5.099020  4.472136  4.582576  4.690416  4.898979  4.690416   \n",
      "Doc 17  5.477226  5.744563  5.567764  5.656854  5.567764  5.744563  5.744563   \n",
      "Doc 18  5.196152  5.291503  5.099020  5.196152  5.099020  5.291503  5.291503   \n",
      "Doc 19  5.385165  5.099020  5.291503  5.196152  5.099020  5.291503  5.291503   \n",
      "Doc 20  5.656854  5.567764  5.385165  5.099020  5.385165  5.567764  5.196152   \n",
      "Doc 21  5.830952  5.567764  5.567764  5.656854  5.567764  5.567764  5.567764   \n",
      "Doc 22  5.000000  5.099020  4.690416  4.582576  4.472136  5.099020  4.690416   \n",
      "Doc 23  5.477226  5.567764  5.196152  4.898979  5.000000  5.196152  5.196152   \n",
      "Doc 24  5.744563  5.830952  5.830952  5.744563  5.830952  6.164414  5.477226   \n",
      "Doc 25  5.744563  6.000000  5.477226  5.567764  5.656854  5.656854  6.000000   \n",
      "Doc 26  6.324555  6.403124  6.403124  6.324555  6.244998  6.403124  6.557439   \n",
      "Doc 27  6.082763  5.656854  5.656854  5.744563  5.830952  6.164414  6.000000   \n",
      "Doc 28  6.324555  6.244998  6.557439  6.480741  6.557439  6.557439  6.244998   \n",
      "Doc 29  5.385165  5.099020  5.099020  4.795832  5.099020  5.099020  5.099020   \n",
      "Doc 30  5.477226  5.385165  5.196152  4.898979  5.385165  5.385165  5.196152   \n",
      "Doc 31  5.830952  5.567764  6.244998  6.480741  6.403124  6.708204  6.082763   \n",
      "Doc 32  5.291503  5.000000  5.385165  5.477226  5.196152  5.385165  5.385165   \n",
      "Doc 33  5.099020  5.196152  4.795832  4.898979  5.000000  5.385165  4.795832   \n",
      "Doc 34  6.164414  5.916080  5.196152  5.477226  6.082763  6.082763  6.082763   \n",
      "Doc 35  5.291503  5.916080  5.385165  5.291503  5.385165  5.744563  5.567764   \n",
      "Doc 36  4.582576  4.690416  4.000000  4.123106  4.242641  4.898979  3.741657   \n",
      "Doc 37  4.582576  4.690416  4.000000  3.872983  4.000000  4.898979  4.000000   \n",
      "\n",
      "           Doc 8     Doc 9    Doc 10  ...    Doc 28    Doc 29    Doc 30  \\\n",
      "Doc 1   5.916080  5.196152  5.744563  ...  6.324555  5.385165  5.477226   \n",
      "Doc 2   6.000000  6.000000  5.477226  ...  6.244998  5.099020  5.385165   \n",
      "Doc 3   6.164414  5.477226  5.291503  ...  6.557439  5.099020  5.196152   \n",
      "Doc 4   6.082763  5.567764  5.385165  ...  6.480741  4.795832  4.898979   \n",
      "Doc 5   6.000000  5.656854  5.291503  ...  6.557439  5.099020  5.385165   \n",
      "Doc 6   5.830952  6.000000  5.830952  ...  6.557439  5.099020  5.385165   \n",
      "Doc 7   6.000000  5.291503  5.291503  ...  6.244998  5.099020  5.196152   \n",
      "Doc 8   0.000000  6.324555  5.830952  ...  6.708204  5.099020  5.916080   \n",
      "Doc 9   6.324555  0.000000  5.656854  ...  6.557439  5.477226  5.385165   \n",
      "Doc 10  5.830952  5.656854  0.000000  ...  6.244998  4.472136  5.196152   \n",
      "Doc 11  5.656854  5.099020  4.472136  ...  6.082763  4.472136  4.795832   \n",
      "Doc 12  5.744563  5.385165  5.000000  ...  6.000000  4.358899  4.898979   \n",
      "Doc 13  6.480741  6.480741  6.000000  ...  7.141428  5.656854  5.916080   \n",
      "Doc 14  5.916080  5.196152  5.744563  ...  6.324555  5.385165  5.477226   \n",
      "Doc 15  5.477226  5.291503  4.898979  ...  6.403124  4.690416  4.795832   \n",
      "Doc 16  5.477226  4.690416  4.690416  ...  5.916080  4.242641  3.872983   \n",
      "Doc 17  6.244998  5.567764  5.744563  ...  6.480741  4.795832  5.477226   \n",
      "Doc 18  5.830952  5.291503  4.898979  ...  5.567764  4.690416  4.795832   \n",
      "Doc 19  5.656854  5.656854  5.099020  ...  5.744563  4.690416  5.000000   \n",
      "Doc 20  6.082763  5.385165  5.000000  ...  5.830952  4.795832  4.690416   \n",
      "Doc 21  5.916080  5.567764  5.385165  ...  6.164414  5.196152  5.099020   \n",
      "Doc 22  5.656854  5.099020  4.242641  ...  5.567764  4.000000  4.358899   \n",
      "Doc 23  5.744563  5.567764  4.582576  ...  6.000000  4.123106  4.898979   \n",
      "Doc 24  6.164414  5.656854  5.099020  ...  6.244998  4.898979  5.567764   \n",
      "Doc 25  6.324555  6.000000  6.000000  ...  6.244998  5.656854  5.385165   \n",
      "Doc 26  6.244998  6.557439  6.082763  ...  7.348469  5.916080  6.164414   \n",
      "Doc 27  5.656854  6.324555  5.830952  ...  6.855655  5.291503  5.744563   \n",
      "Doc 28  6.708204  6.557439  6.244998  ...  0.000000  6.082763  6.000000   \n",
      "Doc 29  5.099020  5.477226  4.472136  ...  6.082763  0.000000  4.795832   \n",
      "Doc 30  5.916080  5.385165  5.196152  ...  6.000000  4.795832  0.000000   \n",
      "Doc 31  6.557439  6.082763  6.403124  ...  7.071068  6.082763  6.324555   \n",
      "Doc 32  5.916080  5.567764  5.385165  ...  6.000000  4.795832  5.099020   \n",
      "Doc 33  5.567764  5.196152  4.795832  ...  5.830952  4.358899  4.472136   \n",
      "Doc 34  6.403124  5.567764  5.567764  ...  6.782330  5.000000  5.656854   \n",
      "Doc 35  6.082763  5.385165  5.000000  ...  6.633250  4.795832  5.477226   \n",
      "Doc 36  5.291503  4.690416  4.000000  ...  5.567764  3.464102  3.872983   \n",
      "Doc 37  5.291503  4.690416  4.000000  ...  5.567764  3.464102  3.872983   \n",
      "\n",
      "          Doc 31    Doc 32    Doc 33    Doc 34    Doc 35    Doc 36    Doc 37  \n",
      "Doc 1   5.830952  5.291503  5.099020  6.164414  5.291503  4.582576  4.582576  \n",
      "Doc 2   5.567764  5.000000  5.196152  5.916080  5.916080  4.690416  4.690416  \n",
      "Doc 3   6.244998  5.385165  4.795832  5.196152  5.385165  4.000000  4.000000  \n",
      "Doc 4   6.480741  5.477226  4.898979  5.477226  5.291503  4.123106  3.872983  \n",
      "Doc 5   6.403124  5.196152  5.000000  6.082763  5.385165  4.242641  4.000000  \n",
      "Doc 6   6.708204  5.385165  5.385165  6.082763  5.744563  4.898979  4.898979  \n",
      "Doc 7   6.082763  5.385165  4.795832  6.082763  5.567764  3.741657  4.000000  \n",
      "Doc 8   6.557439  5.916080  5.567764  6.403124  6.082763  5.291503  5.291503  \n",
      "Doc 9   6.082763  5.567764  5.196152  5.567764  5.385165  4.690416  4.690416  \n",
      "Doc 10  6.403124  5.385165  4.795832  5.567764  5.000000  4.000000  4.000000  \n",
      "Doc 11  6.244998  5.000000  4.358899  5.385165  5.000000  3.741657  3.741657  \n",
      "Doc 12  6.324555  4.898979  4.472136  5.291503  5.291503  3.872983  3.872983  \n",
      "Doc 13  6.708204  6.082763  5.916080  6.403124  6.557439  5.477226  5.477226  \n",
      "Doc 14  5.830952  5.291503  5.099020  6.164414  5.291503  4.582576  4.582576  \n",
      "Doc 15  5.916080  4.795832  4.582576  5.744563  4.582576  4.000000  4.000000  \n",
      "Doc 16  5.744563  4.582576  3.872983  5.385165  4.582576  2.828427  3.162278  \n",
      "Doc 17  6.324555  5.291503  5.291503  5.477226  5.656854  4.795832  4.795832  \n",
      "Doc 18  6.082763  4.795832  4.358899  5.385165  5.385165  3.741657  3.741657  \n",
      "Doc 19  6.082763  4.795832  4.582576  5.744563  5.385165  4.000000  4.000000  \n",
      "Doc 20  6.480741  5.291503  4.690416  5.656854  5.477226  3.872983  3.872983  \n",
      "Doc 21  6.324555  5.291503  5.099020  5.830952  5.830952  4.582576  4.582576  \n",
      "Doc 22  6.082763  4.358899  3.872983  5.385165  4.795832  2.828427  2.828427  \n",
      "Doc 23  6.480741  4.898979  4.472136  5.656854  4.898979  3.605551  3.605551  \n",
      "Doc 24  6.403124  5.567764  5.196152  6.082763  5.567764  4.472136  4.472136  \n",
      "Doc 25  6.403124  5.744563  5.385165  6.082763  6.244998  4.898979  4.690416  \n",
      "Doc 26  6.928203  6.324555  6.164414  6.633250  6.164414  5.744563  5.744563  \n",
      "Doc 27  6.082763  5.916080  5.567764  5.916080  6.082763  5.099020  4.898979  \n",
      "Doc 28  7.071068  6.000000  5.830952  6.782330  6.633250  5.567764  5.567764  \n",
      "Doc 29  6.082763  4.795832  4.358899  5.000000  4.795832  3.464102  3.464102  \n",
      "Doc 30  6.324555  5.099020  4.472136  5.656854  5.477226  3.872983  3.872983  \n",
      "Doc 31  0.000000  6.324555  6.164414  6.480741  6.480741  5.567764  5.744563  \n",
      "Doc 32  6.324555  0.000000  4.472136  6.000000  5.477226  4.123106  4.123106  \n",
      "Doc 33  6.164414  4.472136  0.000000  5.477226  5.099020  3.316625  3.316625  \n",
      "Doc 34  6.480741  6.000000  5.477226  0.000000  5.830952  5.000000  4.795832  \n",
      "Doc 35  6.480741  5.477226  5.099020  5.830952  0.000000  4.358899  4.358899  \n",
      "Doc 36  5.567764  4.123106  3.316625  5.000000  4.358899  0.000000  1.414214  \n",
      "Doc 37  5.744563  4.123106  3.316625  4.795832  4.358899  1.414214  0.000000  \n",
      "\n",
      "[37 rows x 37 columns]\n",
      "\n",
      "Matrice de Distance Euclidienne (Occurrences) :\n",
      "           Doc 1     Doc 2     Doc 3      Doc 4     Doc 5     Doc 6     Doc 7  \\\n",
      "Doc 1   0.000000  6.633250  5.656854   6.928203  5.385165  7.141428  7.000000   \n",
      "Doc 2   6.633250  0.000000  6.480741   8.000000  7.000000  6.708204  7.810250   \n",
      "Doc 3   5.656854  6.480741  0.000000   6.782330  5.385165  7.549834  6.855655   \n",
      "Doc 4   6.928203  8.000000  6.782330   0.000000  6.403124  7.681146  8.062258   \n",
      "Doc 5   5.385165  7.000000  5.385165   6.403124  0.000000  6.928203  7.071068   \n",
      "Doc 6   7.141428  6.708204  7.549834   7.681146  6.928203  0.000000  8.831761   \n",
      "Doc 7   7.000000  7.810250  6.855655   8.062258  7.071068  8.831761  0.000000   \n",
      "Doc 8   7.000000  7.280110  7.416198   7.681146  7.071068  6.928203  8.124038   \n",
      "Doc 9   5.196152  6.708204  5.567764   6.855655  5.656854  7.071068  6.633250   \n",
      "Doc 10  6.633250  7.348469  6.324555   7.483315  6.244998  8.185353  7.937254   \n",
      "Doc 11  5.477226  6.480741  5.477226   6.782330  5.385165  7.141428  7.416198   \n",
      "Doc 12  5.385165  5.744563  5.196152   6.403124  5.477226  6.480741  7.211103   \n",
      "Doc 13  6.633250  7.615773  7.483315   8.124038  7.141428  7.681146  8.306624   \n",
      "Doc 14  0.000000  6.633250  5.656854   6.928203  5.385165  7.141428  7.000000   \n",
      "Doc 15  4.690416  5.830952  5.291503   6.782330  5.196152  6.244998  7.549834   \n",
      "Doc 16  4.582576  5.916080  4.582576   6.082763  4.690416  6.480741  6.480741   \n",
      "Doc 17  6.708204  5.916080  6.855655   8.185353  7.211103  6.480741  8.246211   \n",
      "Doc 18  6.000000  6.782330  5.477226   7.211103  5.916080  7.681146  6.708204   \n",
      "Doc 19  5.477226  6.000000  5.656854   6.480741  5.196152  6.403124  6.855655   \n",
      "Doc 20  6.708204  7.549834  6.244998   7.280110  6.480741  7.745967  7.071068   \n",
      "Doc 21  6.324555  6.782330  6.324555   7.348469  5.916080  6.557439  7.549834   \n",
      "Doc 22  5.000000  6.403124  4.795832   6.082763  4.472136  6.782330  6.480741   \n",
      "Doc 23  5.477226  6.782330  5.477226   6.000000  5.000000  6.855655  7.141428   \n",
      "Doc 24  5.744563  7.000000  6.082763   7.000000  5.830952  7.745967  6.782330   \n",
      "Doc 25  7.348469  6.633250  6.928203   8.366600  7.681146  6.708204  8.426150   \n",
      "Doc 26  7.280110  7.416198  7.416198   7.810250  7.348469  7.745967  8.831761   \n",
      "Doc 27  7.745967  6.164414  7.745967   8.831761  8.062258  6.855655  9.219544   \n",
      "Doc 28  8.544004  8.660254  8.774964   9.848858  8.831761  9.165151  9.273618   \n",
      "Doc 29  5.385165  6.403124  5.385165   5.916080  5.099020  6.928203  7.071068   \n",
      "Doc 30  5.567764  5.744563  5.385165   6.557439  5.656854  6.324555  7.071068   \n",
      "Doc 31  8.306624  7.000000  8.774964  10.246951  9.165151  8.246211  9.380832   \n",
      "Doc 32  5.744563  5.744563  5.744563   7.280110  5.830952  6.782330  6.928203   \n",
      "Doc 33  6.082763  6.403124  5.744563   7.280110  6.164414  7.483315  7.211103   \n",
      "Doc 34  6.480741  6.000000  5.656854   7.071068  6.708204  6.855655  8.062258   \n",
      "Doc 35  5.830952  7.348469  6.000000   6.782330  5.916080  7.810250  7.549834   \n",
      "Doc 36  4.582576  6.082763  4.123106   5.744563  4.242641  6.782330  5.830952   \n",
      "Doc 37  4.582576  6.082763  4.123106   5.567764  4.000000  6.782330  6.000000   \n",
      "\n",
      "           Doc 8     Doc 9     Doc 10  ...     Doc 28    Doc 29    Doc 30  \\\n",
      "Doc 1   7.000000  5.196152   6.633250  ...   8.544004  5.385165  5.567764   \n",
      "Doc 2   7.280110  6.708204   7.348469  ...   8.660254  6.403124  5.744563   \n",
      "Doc 3   7.416198  5.567764   6.324555  ...   8.774964  5.385165  5.385165   \n",
      "Doc 4   7.681146  6.855655   7.483315  ...   9.848858  5.916080  6.557439   \n",
      "Doc 5   7.071068  5.656854   6.244998  ...   8.831761  5.099020  5.656854   \n",
      "Doc 6   6.928203  7.071068   8.185353  ...   9.165151  6.928203  6.324555   \n",
      "Doc 7   8.124038  6.633250   7.937254  ...   9.273618  7.071068  7.071068   \n",
      "Doc 8   0.000000  7.071068   7.549834  ...   9.591663  6.164414  6.782330   \n",
      "Doc 9   7.071068  0.000000   6.557439  ...   8.944272  5.477226  5.477226   \n",
      "Doc 10  7.549834  6.557439   0.000000  ...   9.433981  5.567764  6.403124   \n",
      "Doc 11  6.855655  5.385165   5.477226  ...   9.000000  4.795832  5.196152   \n",
      "Doc 12  6.928203  5.477226   6.244998  ...   8.246211  4.690416  4.898979   \n",
      "Doc 13  7.810250  7.141428   7.615773  ...   9.949874  6.557439  6.855655   \n",
      "Doc 14  7.000000  5.196152   6.633250  ...   8.544004  5.385165  5.567764   \n",
      "Doc 15  6.708204  5.385165   6.000000  ...   9.000000  5.000000  4.795832   \n",
      "Doc 16  6.633250  4.690416   5.744563  ...   8.485281  4.242641  4.000000   \n",
      "Doc 17  7.211103  6.633250   7.937254  ...   9.273618  6.480741  6.164414   \n",
      "Doc 18  7.937254  6.082763   7.071068  ...   7.280110  6.082763  5.744563   \n",
      "Doc 19  6.855655  5.916080   6.324555  ...   7.681146  5.000000  5.385165   \n",
      "Doc 20  7.615773  6.324555   7.280110  ...   8.124038  6.324555  5.830952   \n",
      "Doc 21  7.141428  6.082763   6.782330  ...   8.544004  5.744563  5.744563   \n",
      "Doc 22  6.782330  5.099020   5.385165  ...   7.874008  4.000000  4.690416   \n",
      "Doc 23  6.855655  5.567764   5.656854  ...   8.426150  4.123106  5.196152   \n",
      "Doc 24  7.071068  5.656854   6.082763  ...   8.602325  4.898979  5.830952   \n",
      "Doc 25  7.937254  7.280110   8.831761  ...   8.660254  7.937254  6.403124   \n",
      "Doc 26  7.615773  7.483315   7.549834  ...  10.198039  6.928203  6.928203   \n",
      "Doc 27  7.549834  8.062258   8.831761  ...   9.219544  7.937254  7.141428   \n",
      "Doc 28  9.591663  8.944272   9.433981  ...   0.000000  8.944272  8.485281   \n",
      "Doc 29  6.164414  5.477226   5.567764  ...   8.944272  0.000000  5.099020   \n",
      "Doc 30  6.782330  5.477226   6.403124  ...   8.485281  5.099020  0.000000   \n",
      "Doc 31  9.165151  8.602325  10.148892  ...   9.380832  9.486833  8.485281   \n",
      "Doc 32  7.483315  6.000000   7.000000  ...   8.246211  5.656854  5.477226   \n",
      "Doc 33  7.483315  6.164414   7.000000  ...   8.602325  5.830952  5.477226   \n",
      "Doc 34  7.280110  5.916080   6.633250  ...   8.888194  5.744563  5.744563   \n",
      "Doc 35  7.549834  5.916080   6.000000  ...   9.643651  5.385165  6.244998   \n",
      "Doc 36  6.480741  4.690416   5.196152  ...   8.366600  3.464102  4.242641   \n",
      "Doc 37  6.480741  4.690416   5.196152  ...   8.366600  3.464102  4.242641   \n",
      "\n",
      "           Doc 31    Doc 32    Doc 33    Doc 34    Doc 35    Doc 36    Doc 37  \n",
      "Doc 1    8.306624  5.744563  6.082763  6.480741  5.830952  4.582576  4.582576  \n",
      "Doc 2    7.000000  5.744563  6.403124  6.000000  7.348469  6.082763  6.082763  \n",
      "Doc 3    8.774964  5.744563  5.744563  5.656854  6.000000  4.123106  4.123106  \n",
      "Doc 4   10.246951  7.280110  7.280110  7.071068  6.782330  5.744563  5.567764  \n",
      "Doc 5    9.165151  5.830952  6.164414  6.708204  5.916080  4.242641  4.000000  \n",
      "Doc 6    8.246211  6.782330  7.483315  6.855655  7.810250  6.782330  6.782330  \n",
      "Doc 7    9.380832  6.928203  7.211103  8.062258  7.549834  5.830952  6.000000  \n",
      "Doc 8    9.165151  7.483315  7.483315  7.280110  7.549834  6.480741  6.480741  \n",
      "Doc 9    8.602325  6.000000  6.164414  5.916080  5.916080  4.690416  4.690416  \n",
      "Doc 10  10.148892  7.000000  7.000000  6.633250  6.000000  5.196152  5.196152  \n",
      "Doc 11   9.327379  5.916080  5.916080  6.000000  5.830952  4.123106  4.123106  \n",
      "Doc 12   8.246211  5.291503  5.477226  5.385165  6.082763  4.242641  4.242641  \n",
      "Doc 13   9.539392  7.416198  7.681146  7.348469  7.745967  6.403124  6.403124  \n",
      "Doc 14   8.306624  5.744563  6.082763  6.480741  5.830952  4.582576  4.582576  \n",
      "Doc 15   8.544004  5.385165  5.744563  5.830952  5.291503  4.358899  4.358899  \n",
      "Doc 16   8.602325  5.099020  5.099020  5.744563  5.196152  2.828427  3.162278  \n",
      "Doc 17   8.000000  6.480741  6.928203  5.744563  7.681146  6.633250  6.633250  \n",
      "Doc 18   8.185353  5.567764  5.744563  6.480741  7.071068  4.795832  4.795832  \n",
      "Doc 19   8.185353  5.385165  5.916080  6.324555  6.164414  4.358899  4.358899  \n",
      "Doc 20   9.486833  6.633250  6.633250  7.141428  7.280110  5.291503  5.291503  \n",
      "Doc 21   9.000000  6.244998  6.708204  6.633250  6.782330  5.196152  5.196152  \n",
      "Doc 22   9.273618  5.099020  5.291503  6.082763  5.385165  2.828427  2.828427  \n",
      "Doc 23   9.746794  5.744563  5.916080  6.324555  5.477226  3.605551  3.605551  \n",
      "Doc 24   9.591663  6.324555  6.480741  6.708204  6.082763  4.472136  4.472136  \n",
      "Doc 25   7.280110  7.000000  7.141428  6.782330  8.717798  7.141428  7.000000  \n",
      "Doc 26   9.695360  7.615773  7.874008  7.280110  7.141428  6.928203  6.928203  \n",
      "Doc 27   6.708204  7.416198  7.810250  6.928203  8.717798  7.810250  7.681146  \n",
      "Doc 28   9.380832  8.246211  8.602325  8.888194  9.643651  8.366600  8.366600  \n",
      "Doc 29   9.486833  5.656854  5.830952  5.744563  5.385165  3.464102  3.464102  \n",
      "Doc 30   8.485281  5.477226  5.477226  5.744563  6.244998  4.242641  4.242641  \n",
      "Doc 31   0.000000  8.246211  8.831761  8.185353  9.949874  8.944272  9.055385  \n",
      "Doc 32   8.246211  0.000000  5.656854  6.403124  6.708204  4.898979  4.898979  \n",
      "Doc 33   8.831761  5.656854  0.000000  6.403124  6.855655  4.898979  4.898979  \n",
      "Doc 34   8.185353  6.403124  6.403124  0.000000  6.782330  5.744563  5.567764  \n",
      "Doc 35   9.949874  6.708204  6.855655  6.782330  0.000000  5.000000  5.000000  \n",
      "Doc 36   8.944272  4.898979  4.898979  5.744563  5.000000  0.000000  1.414214  \n",
      "Doc 37   9.055385  4.898979  4.898979  5.567764  5.000000  1.414214  0.000000  \n",
      "\n",
      "[37 rows x 37 columns]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nMatrice de Distance Euclidienne (Binaire) :\")\n",
    "print(binary_distance_euclidean_df)\n",
    "\n",
    "print(\"\\nMatrice de Distance Euclidienne (Occurrences) :\")\n",
    "print(occurrence_distance_euclidean_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b8a2778c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrice de similarité (Binaire - Manhattan):\n",
      "[[1.         0.35185185 0.42592593 ... 0.48148148 0.61111111 0.61111111]\n",
      " [0.35185185 1.         0.40740741 ... 0.35185185 0.59259259 0.59259259]\n",
      " [0.42592593 0.40740741 1.         ... 0.46296296 0.7037037  0.7037037 ]\n",
      " ...\n",
      " [0.48148148 0.35185185 0.46296296 ... 1.         0.64814815 0.64814815]\n",
      " [0.61111111 0.59259259 0.7037037  ... 0.64814815 1.         0.96296296]\n",
      " [0.61111111 0.59259259 0.7037037  ... 0.64814815 0.96296296 1.        ]]\n",
      "\n",
      "Matrice de similarité (Occurrence - Manhattan):\n",
      "[[1.         0.44117647 0.52941176 ... 0.55882353 0.69117647 0.69117647]\n",
      " [0.44117647 1.         0.47058824 ... 0.41176471 0.63235294 0.63235294]\n",
      " [0.52941176 0.47058824 1.         ... 0.52941176 0.75       0.75      ]\n",
      " ...\n",
      " [0.55882353 0.41176471 0.52941176 ... 1.         0.69117647 0.69117647]\n",
      " [0.69117647 0.63235294 0.75       ... 0.69117647 1.         0.97058824]\n",
      " [0.69117647 0.63235294 0.75       ... 0.69117647 0.97058824 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "#Matrice de similarité\n",
    "def calculate_similarity_matrix(distance_matrix):\n",
    "    max_distance = np.max(distance_matrix)\n",
    "    return 1 - (distance_matrix / max_distance)\n",
    "# Calculer les matrices de similarité\n",
    "binary_similarity_manhattan = calculate_similarity_matrix(binary_distance_manhattan)\n",
    "occurrence_similarity_manhattan = calculate_similarity_matrix(occurrence_distance_manhattan)\n",
    "\n",
    "binary_similarity_euclidean = calculate_similarity_matrix(binary_distance_euclidean)\n",
    "occurrence_similarity_euclidean = calculate_similarity_matrix(occurrence_distance_euclidean)\n",
    "\n",
    "# Afficher les DataFrames des similarités\n",
    "print(\"Matrice de similarité (Binaire - Manhattan):\")\n",
    "print(binary_similarity_manhattan)\n",
    "\n",
    "print(\"\\nMatrice de similarité (Occurrence - Manhattan):\")\n",
    "print(occurrence_similarity_manhattan)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d457ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les 4 documents les plus similaires au document 1 :\n",
      "Document 14 avec similarité de 1.0000\n",
      "Document 15 avec similarité de 0.6111\n",
      "Document 16 avec similarité de 0.6111\n",
      "Document 36 avec similarité de 0.6111\n"
     ]
    }
   ],
   "source": [
    "def K_plus_proches_documents(doc_requete, k, similarity_matrix):\n",
    "    \"\"\"\n",
    "    Retourne les k premiers documents les plus similaires au document de requête.\n",
    "    \"\"\"\n",
    "    # Extraire les similarités du document de requête\n",
    "    similarites = similarity_matrix[doc_requete]\n",
    "    \n",
    "    # Créer une liste d'index et de similarités\n",
    "    similarites_idx = [(i, similarites[i]) for i in range(len(similarites)) if i != doc_requete]\n",
    "    \n",
    "    # de plus élevée à plus basse\n",
    "    similarites_idx.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Retourner les k premiers documents\n",
    "    return similarites_idx[:k]\n",
    "\n",
    "\n",
    "user_input = int(input(\"Entrez le numéro du document : \"))-1\n",
    "k = int(input(\"Entrez le nombre de documents similaires à retourner : \"))\n",
    "# par exemple je choisis: matrice similarite de manhattan\n",
    "k_plus_proches = K_plus_proches_documents(user_input, k, binary_similarity_manhattan)\n",
    "\n",
    "print(f\"Les {k} documents les plus similaires au document {user_input + 1} :\")\n",
    "for idx, sim in k_plus_proches:\n",
    "    print(f\"Document {idx + 1} avec similarité de {sim:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ec0f4e",
   "metadata": {},
   "source": [
    "## pour exemple de (cat,dog) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "189c23ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Matrice de Distance de Manhattan (Binaire) :\n",
      "      ph1_  ph2_  ph3_\n",
      "ph1_   0.0   4.0  11.0\n",
      "ph2_   4.0   0.0  11.0\n",
      "ph3_  11.0  11.0   0.0\n",
      "\n",
      "Matrice de Distance de Manhattan (Occurrences) :\n",
      "      ph1_  ph2_  ph3_\n",
      "ph1_   0.0   8.0  24.0\n",
      "ph2_   8.0   0.0  24.0\n",
      "ph3_  24.0  24.0   0.0\n",
      "\n",
      "Matrice de Distance Euclidienne (Binaire) :\n",
      "          ph1_      ph2_      ph3_\n",
      "ph1_  0.000000  2.000000  3.316625\n",
      "ph2_  2.000000  0.000000  3.316625\n",
      "ph3_  3.316625  3.316625  0.000000\n",
      "\n",
      "Matrice de Distance Euclidienne (Occurrences) :\n",
      "          ph1_      ph2_      ph3_\n",
      "ph1_  0.000000  4.000000  7.483315\n",
      "ph2_  4.000000  0.000000  7.483315\n",
      "ph3_  7.483315  7.483315  0.000000\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "# Phrases d'entrée\n",
    "ph1_ = \"The cat sat on the mat. The cat sat on the mat.\"\n",
    "ph2_ = \"The dog sat on the log. The dog sat on the log.\"\n",
    "ph3_ = \"Cats and dogs are great pets. Cats and dogs are great pets.\"\n",
    "\n",
    "\n",
    "# 1. Découper les phrases en phrases individuelles\n",
    "def split_into_sentences(text):\n",
    "    \"\"\"Divise le texte en phrases en utilisant la ponctuation.\"\"\"\n",
    "    sentences = re.split(r'\\.', text)\n",
    "    return [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "\n",
    "# 2. Prétraitement du texte pour obtenir les mots uniques\n",
    "def preprocess_text(sentences):\n",
    "    unique_tokens = set()  # Utilisation d'un ensemble pour éviter les doublons\n",
    "    for sentence in sentences:\n",
    "        # Nettoyer et tokeniser\n",
    "        tokens = re.findall(r'\\b\\w+\\b', sentence.lower())\n",
    "        unique_tokens.update(tokens)  # Ajouter les mots uniques\n",
    "    return sorted(unique_tokens)  # Retourner une liste triée\n",
    "\n",
    "# 3. Créer la matrice binaire et la matrice d'occurrences\n",
    "def create_matrices(sentences, unique_tokens):\n",
    "    binary_matrix = []\n",
    "    occurrence_matrix = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        # Tokeniser la phrase\n",
    "        tokens = re.findall(r'\\b\\w+\\b', sentence.lower())\n",
    "        \n",
    "        # Créer une ligne pour la matrice binaire\n",
    "        binary_row = [1 if token in tokens else 0 for token in unique_tokens]\n",
    "        binary_matrix.append(binary_row)\n",
    "        \n",
    "        # Créer une ligne pour la matrice d'occurrences\n",
    "        occurrence_row = [tokens.count(token) for token in unique_tokens]\n",
    "        occurrence_matrix.append(occurrence_row)\n",
    "\n",
    "    return np.array(binary_matrix), np.array(occurrence_matrix)\n",
    "\n",
    "# 4. Calculer la distance de Manhattan\n",
    "def calculate_manhattan_distance(matrix):\n",
    "    return squareform(pdist(matrix, metric='cityblock'))\n",
    "\n",
    "# 5. Calculer la distance euclidienne\n",
    "def calculate_euclidean_distance(matrix):\n",
    "    return squareform(pdist(matrix, metric='euclidean'))\n",
    "\n",
    "# Traitement des phrases\n",
    "sentences = [ph1_, ph2_, ph3_]  # Les documents à comparer\n",
    "tokenized_sentences = [split_into_sentences(doc) for doc in sentences]  # Diviser chaque doc en phrases\n",
    "all_sentences = [sentence for doc in tokenized_sentences for sentence in doc]  # Toutes les phrases ensemble\n",
    "\n",
    "# Prétraitement pour obtenir les tokens uniques\n",
    "unique_tokens = preprocess_text(all_sentences)\n",
    "\n",
    "# Création des matrices binaires et d'occurrences pour ph1_, ph2_, et ph3_\n",
    "binary_matrix, occurrence_matrix = create_matrices([ph1_, ph2_, ph3_], unique_tokens)\n",
    "\n",
    "# Calculer les distances de Manhattan\n",
    "binary_distance_manhattan = calculate_manhattan_distance(binary_matrix)\n",
    "occurrence_distance_manhattan = calculate_manhattan_distance(occurrence_matrix)\n",
    "\n",
    "# Calculer les distances euclidiennes\n",
    "binary_distance_euclidean = calculate_euclidean_distance(binary_matrix)\n",
    "occurrence_distance_euclidean = calculate_euclidean_distance(occurrence_matrix)\n",
    "\n",
    "# Créer des DataFrames pour afficher les distances\n",
    "binary_distance_manhattan_df = pd.DataFrame(binary_distance_manhattan, \n",
    "                                            columns=['ph1_', 'ph2_', 'ph3_'],\n",
    "                                            index=['ph1_', 'ph2_', 'ph3_'])\n",
    "\n",
    "occurrence_distance_manhattan_df = pd.DataFrame(occurrence_distance_manhattan, \n",
    "                                                columns=['ph1_', 'ph2_', 'ph3_'],\n",
    "                                                index=['ph1_', 'ph2_', 'ph3_'])\n",
    "\n",
    "binary_distance_euclidean_df = pd.DataFrame(binary_distance_euclidean, \n",
    "                                            columns=['ph1_', 'ph2_', 'ph3_'],\n",
    "                                            index=['ph1_', 'ph2_', 'ph3_'])\n",
    "\n",
    "occurrence_distance_euclidean_df = pd.DataFrame(occurrence_distance_euclidean, \n",
    "                                                columns=['ph1_', 'ph2_', 'ph3_'],\n",
    "                                                index=['ph1_', 'ph2_', 'ph3_'])\n",
    "\n",
    "# Afficher les résultats\n",
    "print(\"\\nMatrice de Distance de Manhattan (Binaire) :\")\n",
    "print(binary_distance_manhattan_df)\n",
    "\n",
    "print(\"\\nMatrice de Distance de Manhattan (Occurrences) :\")\n",
    "print(occurrence_distance_manhattan_df)\n",
    "\n",
    "print(\"\\nMatrice de Distance Euclidienne (Binaire) :\")\n",
    "print(binary_distance_euclidean_df)\n",
    "\n",
    "print(\"\\nMatrice de Distance Euclidienne (Occurrences) :\")\n",
    "print(occurrence_distance_euclidean_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2108b40c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Matrice de Distance de Manhattan (Binaire) :\n",
      "      ph1_  ph2_  ph3_\n",
      "ph1_   0.0   4.0  11.0\n",
      "ph2_   4.0   0.0  11.0\n",
      "ph3_  11.0  11.0   0.0\n",
      "\n",
      "Matrice de Distance de Manhattan (Occurrences) :\n",
      "      ph1_  ph2_  ph3_\n",
      "ph1_   0.0  16.0  48.0\n",
      "ph2_  16.0   0.0  48.0\n",
      "ph3_  48.0  48.0   0.0\n",
      "\n",
      "Matrice de Distance Euclidienne (Binaire) :\n",
      "          ph1_      ph2_      ph3_\n",
      "ph1_  0.000000  2.000000  3.316625\n",
      "ph2_  2.000000  0.000000  3.316625\n",
      "ph3_  3.316625  3.316625  0.000000\n",
      "\n",
      "Matrice de Distance Euclidienne (Occurrences) :\n",
      "          ph1_      ph2_      ph3_\n",
      "ph1_   0.00000   8.00000  14.96663\n",
      "ph2_   8.00000   0.00000  14.96663\n",
      "ph3_  14.96663  14.96663   0.00000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ph1_ = \"The cat sat on the mat. The cat sat on the mat.The cat sat on the mat. The cat sat on the mat.\"\n",
    "ph2_ = \"The dog sat on the log. The dog sat on the log.The dog sat on the log. The dog sat on the log.\"\n",
    "ph3_ = \"Cats and dogs are great pets. Cats and dogs are great pets.Cats and dogs are great pets. Cats and dogs are great pets.\"\n",
    "\n",
    "tokenized_sentences = [split_into_sentences(doc) for doc in sentences]  # Diviser chaque doc en phrases\n",
    "all_sentences = [sentence for doc in tokenized_sentences for sentence in doc]  \n",
    "\n",
    "# Prétraitement pour obtenir les tokens uniques\n",
    "unique_tokens = preprocess_text(all_sentences)\n",
    "\n",
    "# Création des matrices binaires et d'occurrences pour ph1_, ph2_, et ph3_\n",
    "binary_matrix, occurrence_matrix = create_matrices([ph1_, ph2_, ph3_], unique_tokens)\n",
    "\n",
    "# Calculer les distances de Manhattan\n",
    "binary_distance_manhattan = calculate_manhattan_distance(binary_matrix)\n",
    "occurrence_distance_manhattan = calculate_manhattan_distance(occurrence_matrix)\n",
    "\n",
    "# Calculer les distances euclidiennes\n",
    "binary_distance_euclidean = calculate_euclidean_distance(binary_matrix)\n",
    "occurrence_distance_euclidean = calculate_euclidean_distance(occurrence_matrix)\n",
    "\n",
    "# Créer des DataFrames pour afficher les distances\n",
    "binary_distance_manhattan_df = pd.DataFrame(binary_distance_manhattan, \n",
    "                                            columns=['ph1_', 'ph2_', 'ph3_'],\n",
    "                                            index=['ph1_', 'ph2_', 'ph3_'])\n",
    "\n",
    "occurrence_distance_manhattan_df = pd.DataFrame(occurrence_distance_manhattan, \n",
    "                                                columns=['ph1_', 'ph2_', 'ph3_'],\n",
    "                                                index=['ph1_', 'ph2_', 'ph3_'])\n",
    "\n",
    "binary_distance_euclidean_df = pd.DataFrame(binary_distance_euclidean, \n",
    "                                            columns=['ph1_', 'ph2_', 'ph3_'],\n",
    "                                            index=['ph1_', 'ph2_', 'ph3_'])\n",
    "\n",
    "occurrence_distance_euclidean_df = pd.DataFrame(occurrence_distance_euclidean, \n",
    "                                                columns=['ph1_', 'ph2_', 'ph3_'],\n",
    "                                                index=['ph1_', 'ph2_', 'ph3_'])\n",
    "\n",
    "# Afficher les résultats\n",
    "print(\"\\nMatrice de Distance de Manhattan (Binaire) :\")\n",
    "print(binary_distance_manhattan_df)\n",
    "\n",
    "print(\"\\nMatrice de Distance de Manhattan (Occurrences) :\")\n",
    "print(occurrence_distance_manhattan_df)\n",
    "\n",
    "print(\"\\nMatrice de Distance Euclidienne (Binaire) :\")\n",
    "print(binary_distance_euclidean_df)\n",
    "\n",
    "print(\"\\nMatrice de Distance Euclidienne (Occurrences) :\")\n",
    "print(occurrence_distance_euclidean_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ca1d0cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Matrice de Distance de Manhattan (Binaire) :\n",
      "      ph1_  ph2_  ph3_\n",
      "ph1_   0.0   4.0  11.0\n",
      "ph2_   4.0   0.0  11.0\n",
      "ph3_  11.0  11.0   0.0\n",
      "\n",
      "Matrice de Distance de Manhattan (Occurrences) :\n",
      "      ph1_  ph2_  ph3_\n",
      "ph1_   0.0   4.0  12.0\n",
      "ph2_   4.0   0.0  12.0\n",
      "ph3_  12.0  12.0   0.0\n",
      "\n",
      "Matrice de Distance Euclidienne (Binaire) :\n",
      "          ph1_      ph2_      ph3_\n",
      "ph1_  0.000000  2.000000  3.316625\n",
      "ph2_  2.000000  0.000000  3.316625\n",
      "ph3_  3.316625  3.316625  0.000000\n",
      "\n",
      "Matrice de Distance Euclidienne (Occurrences) :\n",
      "          ph1_      ph2_      ph3_\n",
      "ph1_  0.000000  2.000000  3.741657\n",
      "ph2_  2.000000  0.000000  3.741657\n",
      "ph3_  3.741657  3.741657  0.000000\n",
      "\n",
      "Matrice de Distance Euclidienne (Normalisée en Probabilités) :\n",
      "          ph1_      ph2_     ph3_\n",
      "ph1_  0.000000  0.333333  0.62361\n",
      "ph2_  0.333333  0.000000  0.62361\n",
      "ph3_  0.623610  0.623610  0.00000\n",
      "\n",
      "Matrice de Distance de Jaccard :\n",
      "          ph1_      ph2_  ph3_\n",
      "ph1_  0.000000  0.571429   1.0\n",
      "ph2_  0.571429  0.000000   1.0\n",
      "ph3_  1.000000  1.000000   0.0\n",
      "\n",
      "Matrice de Distance de Hamming :\n",
      "          ph1_      ph2_      ph3_\n",
      "ph1_  0.000000  0.307692  0.846154\n",
      "ph2_  0.307692  0.000000  0.846154\n",
      "ph3_  0.846154  0.846154  0.000000\n"
     ]
    }
   ],
   "source": [
    "###new ###################################\n",
    "# import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import pdist, squareform, hamming\n",
    "from sklearn.metrics import jaccard_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Phrases d'entrée\n",
    "ph1_ = \"The cat sat on the mat.\"\n",
    "ph2_ = \"The dog sat on the log.\"\n",
    "ph3_ = \"Cats and dogs are great pets.\"\n",
    "documents = [ph1_, ph2_, ph3_]\n",
    "\n",
    "# 1. Découper les phrases en phrases individuelles\n",
    "def split_into_sentences(text):\n",
    "    \"\"\"Divise le texte en phrases en utilisant la ponctuation.\"\"\"\n",
    "    sentences = re.split(r'\\.', text)\n",
    "    return [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "\n",
    "# 2. Prétraitement du texte pour obtenir les mots uniques\n",
    "def preprocess_text(sentences):\n",
    "    unique_tokens = set()  # Utilisation d'un ensemble pour éviter les doublons\n",
    "    for sentence in sentences:\n",
    "        # Nettoyer et tokeniser\n",
    "        tokens = re.findall(r'\\b\\w+\\b', sentence.lower())\n",
    "        unique_tokens.update(tokens)  # Ajouter les mots uniques\n",
    "    return sorted(unique_tokens)  # Retourner une liste triée\n",
    "\n",
    "# 3. Créer la matrice binaire et la matrice d'occurrences\n",
    "def create_matrices(sentences, unique_tokens):\n",
    "    binary_matrix = []\n",
    "    occurrence_matrix = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        # Tokeniser la phrase\n",
    "        tokens = re.findall(r'\\b\\w+\\b', sentence.lower())\n",
    "        \n",
    "        # Créer une ligne pour la matrice binaire\n",
    "        binary_row = [1 if token in tokens else 0 for token in unique_tokens]\n",
    "        binary_matrix.append(binary_row)\n",
    "        \n",
    "        # Créer une ligne pour la matrice d'occurrences\n",
    "        occurrence_row = [tokens.count(token) for token in unique_tokens]\n",
    "        occurrence_matrix.append(occurrence_row)\n",
    "\n",
    "    return np.array(binary_matrix), np.array(occurrence_matrix)\n",
    "\n",
    "# 4. Calculer la distance de Manhattan\n",
    "def calculate_manhattan_distance(matrix):\n",
    "    return squareform(pdist(matrix, metric='cityblock'))\n",
    "\n",
    "# 5. Calculer la distance euclidienne\n",
    "def calculate_euclidean_distance(matrix):\n",
    "    return squareform(pdist(matrix, metric='euclidean'))\n",
    "\n",
    "# 6. Normalisation en probabilités\n",
    "def normalize_matrix(matrix):\n",
    "    \"\"\"Normalise chaque ligne de la matrice en probabilités (fréquences relatives).\"\"\"\n",
    "    row_sums = matrix.sum(axis=1, keepdims=True)\n",
    "    return matrix / row_sums\n",
    "\n",
    "# Prétraitement : Obtenir les tokens uniques\n",
    "unique_tokens = preprocess_text(documents)\n",
    "\n",
    "# Création des matrices binaires et d'occurrences pour les phrases\n",
    "binary_matrix, occurrence_matrix = create_matrices(documents, unique_tokens)\n",
    "\n",
    "# Calculer les distances de Manhattan et Euclidienne\n",
    "binary_distance_manhattan = calculate_manhattan_distance(binary_matrix)\n",
    "occurrence_distance_manhattan = calculate_manhattan_distance(occurrence_matrix)\n",
    "binary_distance_euclidean = calculate_euclidean_distance(binary_matrix)\n",
    "occurrence_distance_euclidean = calculate_euclidean_distance(occurrence_matrix)\n",
    "\n",
    "# Normalisation de la matrice d'occurrences en probabilités\n",
    "normalized_matrix = normalize_matrix(occurrence_matrix)\n",
    "distance_euclidean_normalized = calculate_euclidean_distance(normalized_matrix)\n",
    "\n",
    "# Calculer la distance de Jaccard entre les phrases\n",
    "vectorizer = CountVectorizer(binary=True)\n",
    "X_binary = vectorizer.fit_transform(documents).toarray()\n",
    "\n",
    "jaccard_distances = np.zeros((len(documents), len(documents)))\n",
    "for i in range(len(documents)):\n",
    "    for j in range(len(documents)):\n",
    "        jaccard_distances[i, j] = 1 - jaccard_score(X_binary[i], X_binary[j])\n",
    "\n",
    "# Calculer la distance de Hamming entre les phrases\n",
    "hamming_distances = squareform(pdist(X_binary, metric='hamming'))\n",
    "\n",
    "# Création des DataFrames pour afficher les matrices de distances\n",
    "binary_distance_manhattan_df = pd.DataFrame(binary_distance_manhattan, \n",
    "                                            columns=['ph1_', 'ph2_', 'ph3_'],\n",
    "                                            index=['ph1_', 'ph2_', 'ph3_'])\n",
    "\n",
    "occurrence_distance_manhattan_df = pd.DataFrame(occurrence_distance_manhattan, \n",
    "                                                columns=['ph1_', 'ph2_', 'ph3_'],\n",
    "                                                index=['ph1_', 'ph2_', 'ph3_'])\n",
    "\n",
    "binary_distance_euclidean_df = pd.DataFrame(binary_distance_euclidean, \n",
    "                                            columns=['ph1_', 'ph2_', 'ph3_'],\n",
    "                                            index=['ph1_', 'ph2_', 'ph3_'])\n",
    "\n",
    "occurrence_distance_euclidean_df = pd.DataFrame(occurrence_distance_euclidean, \n",
    "                                                columns=['ph1_', 'ph2_', 'ph3_'],\n",
    "                                                index=['ph1_', 'ph2_', 'ph3_'])\n",
    "\n",
    "distance_euclidean_normalized_df = pd.DataFrame(distance_euclidean_normalized, \n",
    "                                                columns=['ph1_', 'ph2_', 'ph3_'],\n",
    "                                                index=['ph1_', 'ph2_', 'ph3_'])\n",
    "\n",
    "jaccard_distances_df = pd.DataFrame(jaccard_distances, \n",
    "                                    columns=['ph1_', 'ph2_', 'ph3_'],\n",
    "                                    index=['ph1_', 'ph2_', 'ph3_'])\n",
    "\n",
    "hamming_distances_df = pd.DataFrame(hamming_distances, \n",
    "                                    columns=['ph1_', 'ph2_', 'ph3_'],\n",
    "                                    index=['ph1_', 'ph2_', 'ph3_'])\n",
    "\n",
    "# Afficher les résultats\n",
    "print(\"\\nMatrice de Distance de Manhattan (Binaire) :\")\n",
    "print(binary_distance_manhattan_df)\n",
    "\n",
    "print(\"\\nMatrice de Distance de Manhattan (Occurrences) :\")\n",
    "print(occurrence_distance_manhattan_df)\n",
    "\n",
    "print(\"\\nMatrice de Distance Euclidienne (Binaire) :\")\n",
    "print(binary_distance_euclidean_df)\n",
    "\n",
    "print(\"\\nMatrice de Distance Euclidienne (Occurrences) :\")\n",
    "print(occurrence_distance_euclidean_df)\n",
    "\n",
    "print(\"\\nMatrice de Distance Euclidienne (Normalisée en Probabilités) :\")\n",
    "print(distance_euclidean_normalized_df)\n",
    "\n",
    "print(\"\\nMatrice de Distance de Jaccard :\")\n",
    "print(jaccard_distances_df)\n",
    "\n",
    "print(\"\\nMatrice de Distance de Hamming :\")\n",
    "print(hamming_distances_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ceb46ff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Matrice de Distance Bray-Curtis (Occurrences normalisées L1) :\n",
      "          ph1_      ph2_  ph3_\n",
      "ph1_  0.000000  0.333333   1.0\n",
      "ph2_  0.333333  0.000000   1.0\n",
      "ph3_  1.000000  1.000000   0.0\n",
      "\n",
      "Matrice de Distance Bray-Curtis (Occurrences normalisées L2) :\n",
      "          ph1_      ph2_  ph3_\n",
      "ph1_  0.000000  0.333333   1.0\n",
      "ph2_  0.333333  0.000000   1.0\n",
      "ph3_  1.000000  1.000000   0.0\n",
      "\n",
      "Matrice de Distance Bray-Curtis (Binaire normalisées L1) :\n",
      "      ph1_  ph2_  ph3_\n",
      "ph1_   0.0   0.4   1.0\n",
      "ph2_   0.4   0.0   1.0\n",
      "ph3_   1.0   1.0   0.0\n",
      "\n",
      "Matrice de Distance Kullback-Leibler (Occurrences normalisées L1) :\n",
      "           ph1_       ph2_       ph3_\n",
      "ph1_   0.000000   7.078030  21.465141\n",
      "ph2_   7.078030   0.000000  21.465141\n",
      "ph3_  21.234091  21.234091   0.000000\n",
      "\n",
      "Matrice de Distance Kullback-Leibler (Occurrences normalisées L2) :\n",
      "           ph1_       ph2_       ph3_\n",
      "ph1_   0.000000  15.546542  47.457924\n",
      "ph2_  15.546542   0.000000  47.457924\n",
      "ph3_  53.878968  53.878968   0.000000\n",
      "\n",
      "Matrice de Distance Kullback-Leibler (Binaire normalisées L1) :\n",
      "           ph1_       ph2_       ph3_\n",
      "ph1_   0.000000   8.566565  21.416413\n",
      "ph2_   8.566565   0.000000  21.416413\n",
      "ph3_  21.234091  21.234091   0.000000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.spatial.distance import braycurtis\n",
    "from scipy.special import kl_div\n",
    "\n",
    "# Phrases d'entrée\n",
    "ph1_ = \"The cat sat on the mat.\"\n",
    "ph2_ = \"The dog sat on the log.\"\n",
    "ph3_ = \"Cats and dogs are great pets.\"\n",
    "###The cat sat on the mat.  The dog sat on the log. Cats and dogs are great pets.\n",
    "# Documents\n",
    "documents = [ph1_, ph2_, ph3_]\n",
    "\n",
    "# Vectorisation des documents pour obtenir les matrices d'occurrence\n",
    "vectorizer_occ = CountVectorizer(binary=False)\n",
    "occurrence_matrix = vectorizer_occ.fit_transform(documents).toarray()\n",
    "\n",
    "# Vectorisation binaire (présence/absence de mots)\n",
    "vectorizer_binary = CountVectorizer(binary=True)\n",
    "binary_matrix = vectorizer_binary.fit_transform(documents).toarray()\n",
    "\n",
    "# Fonction de normalisation L1\n",
    "def normalize_L1(matrix):\n",
    "    return matrix / matrix.sum(axis=1, keepdims=True)\n",
    "\n",
    "# Fonction de normalisation L2\n",
    "def normalize_L2(matrix):\n",
    "    return matrix / np.sqrt((matrix ** 2).sum(axis=1, keepdims=True))\n",
    "\n",
    "# Calcul de la distance Bray-Curtis entre deux vecteurs\n",
    "def bray_curtis_distance(v1, v2):\n",
    "    return braycurtis(v1, v2)\n",
    "\n",
    "# Calcul de la distance Kullback-Leibler entre deux vecteurs\n",
    "def kullback_leibler_distance(v1, v2):\n",
    "    # Ajout de petites constantes pour éviter la division par zéro\n",
    "    v1 = np.clip(v1, 1e-10, None)\n",
    "    v2 = np.clip(v2, 1e-10, None)\n",
    "    return np.sum(kl_div(v1, v2))\n",
    "\n",
    "# Normalisation des matrices\n",
    "normalized_occ_matrix_L1 = normalize_L1(occurrence_matrix)\n",
    "normalized_occ_matrix_L2 = normalize_L2(occurrence_matrix)\n",
    "\n",
    "# Normalisation de la matrice binaire\n",
    "normalized_binary_matrix_L1 = normalize_L1(binary_matrix)\n",
    "\n",
    "# Initialisation des matrices de distance\n",
    "n_docs = len(documents)\n",
    "distance_matrix_bray_curtis_occ_L1 = np.zeros((n_docs, n_docs))\n",
    "distance_matrix_bray_curtis_occ_L2 = np.zeros((n_docs, n_docs))\n",
    "distance_matrix_bray_curtis_binary = np.zeros((n_docs, n_docs))\n",
    "\n",
    "# Matrices de distance pour Kullback-Leibler\n",
    "distance_matrix_kl_occ_L1 = np.zeros((n_docs, n_docs))\n",
    "distance_matrix_kl_occ_L2 = np.zeros((n_docs, n_docs))\n",
    "distance_matrix_kl_binary = np.zeros((n_docs, n_docs))\n",
    "\n",
    "# Calcul des distances entre tous les documents (Bray-Curtis et Kullback-Leibler)\n",
    "for i in range(n_docs):\n",
    "    for j in range(n_docs):\n",
    "        # Distance Bray-Curtis pour la matrice d'occurrence normalisée L1\n",
    "        distance_matrix_bray_curtis_occ_L1[i, j] = bray_curtis_distance(normalized_occ_matrix_L1[i], normalized_occ_matrix_L1[j])\n",
    "        \n",
    "        # Distance Bray-Curtis pour la matrice d'occurrence normalisée L2\n",
    "        distance_matrix_bray_curtis_occ_L2[i, j] = bray_curtis_distance(normalized_occ_matrix_L2[i], normalized_occ_matrix_L2[j])\n",
    "        \n",
    "        # Distance Bray-Curtis pour la matrice binaire\n",
    "        distance_matrix_bray_curtis_binary[i, j] = bray_curtis_distance(normalized_binary_matrix_L1[i], normalized_binary_matrix_L1[j])\n",
    "\n",
    "        # Distance Kullback-Leibler pour la matrice d'occurrence normalisée L1\n",
    "        distance_matrix_kl_occ_L1[i, j] = kullback_leibler_distance(normalized_occ_matrix_L1[i], normalized_occ_matrix_L1[j])\n",
    "        \n",
    "        # Distance Kullback-Leibler pour la matrice d'occurrence normalisée L2\n",
    "        distance_matrix_kl_occ_L2[i, j] = kullback_leibler_distance(normalized_occ_matrix_L2[i], normalized_occ_matrix_L2[j])\n",
    "        \n",
    "        # Distance Kullback-Leibler pour la matrice binaire\n",
    "        distance_matrix_kl_binary[i, j] = kullback_leibler_distance(normalized_binary_matrix_L1[i], normalized_binary_matrix_L1[j])\n",
    "\n",
    "# Création des DataFrames pour afficher les résultats\n",
    "distance_bray_curtis_occ_L1_df = pd.DataFrame(distance_matrix_bray_curtis_occ_L1, \n",
    "                                              columns=['ph1_', 'ph2_', 'ph3_'], \n",
    "                                              index=['ph1_', 'ph2_', 'ph3_'])\n",
    "\n",
    "distance_bray_curtis_occ_L2_df = pd.DataFrame(distance_matrix_bray_curtis_occ_L2, \n",
    "                                              columns=['ph1_', 'ph2_', 'ph3_'], \n",
    "                                              index=['ph1_', 'ph2_', 'ph3_'])\n",
    "\n",
    "distance_bray_curtis_binary_df = pd.DataFrame(distance_matrix_bray_curtis_binary, \n",
    "                                              columns=['ph1_', 'ph2_', 'ph3_'], \n",
    "                                              index=['ph1_', 'ph2_', 'ph3_'])\n",
    "\n",
    "distance_kl_occ_L1_df = pd.DataFrame(distance_matrix_kl_occ_L1, \n",
    "                                      columns=['ph1_', 'ph2_', 'ph3_'], \n",
    "                                      index=['ph1_', 'ph2_', 'ph3_'])\n",
    "\n",
    "distance_kl_occ_L2_df = pd.DataFrame(distance_matrix_kl_occ_L2, \n",
    "                                      columns=['ph1_', 'ph2_', 'ph3_'], \n",
    "                                      index=['ph1_', 'ph2_', 'ph3_'])\n",
    "\n",
    "distance_kl_binary_df = pd.DataFrame(distance_matrix_kl_binary, \n",
    "                                      columns=['ph1_', 'ph2_', 'ph3_'], \n",
    "                                      index=['ph1_', 'ph2_', 'ph3_'])\n",
    "\n",
    "# Affichage des matrices de distance\n",
    "print(\"\\nMatrice de Distance Bray-Curtis (Occurrences normalisées L1) :\")\n",
    "print(distance_bray_curtis_occ_L1_df)\n",
    "\n",
    "print(\"\\nMatrice de Distance Bray-Curtis (Occurrences normalisées L2) :\")\n",
    "print(distance_bray_curtis_occ_L2_df)\n",
    "\n",
    "print(\"\\nMatrice de Distance Bray-Curtis (Binaire normalisées L1) :\")\n",
    "print(distance_bray_curtis_binary_df)\n",
    "\n",
    "print(\"\\nMatrice de Distance Kullback-Leibler (Occurrences normalisées L1) :\")\n",
    "print(distance_kl_occ_L1_df)\n",
    "\n",
    "print(\"\\nMatrice de Distance Kullback-Leibler (Occurrences normalisées L2) :\")\n",
    "print(distance_kl_occ_L2_df)\n",
    "\n",
    "print(\"\\nMatrice de Distance Kullback-Leibler (Binaire normalisées L1) :\")\n",
    "print(distance_kl_binary_df)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "70018ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Matrice d'Occurrences :\n",
      "[[0 0 4 0 0 0 0 0 4 4 0 4 8]\n",
      " [0 0 0 0 4 0 0 4 0 4 0 4 8]\n",
      " [4 4 0 4 0 4 4 0 0 0 4 0 0]]\n",
      "\n",
      "Matrice Normalisée par la Norme L2 :\n",
      "[[0.         0.         0.35355339 0.         0.         0.\n",
      "  0.         0.         0.35355339 0.35355339 0.         0.35355339\n",
      "  0.70710678]\n",
      " [0.         0.         0.         0.         0.35355339 0.\n",
      "  0.         0.35355339 0.         0.35355339 0.         0.35355339\n",
      "  0.70710678]\n",
      " [0.40824829 0.40824829 0.         0.40824829 0.         0.40824829\n",
      "  0.40824829 0.         0.         0.         0.40824829 0.\n",
      "  0.        ]]\n",
      "\n",
      "Matrice de Distance Euclidienne (Normalisée par la Norme L2) :\n",
      "          ph1_      ph2_      ph3_\n",
      "ph1_  0.000000  0.707107  1.414214\n",
      "ph2_  0.707107  0.000000  1.414214\n",
      "ph3_  1.414214  1.414214  0.000000\n"
     ]
    }
   ],
   "source": [
    "def normalize_l2(matrix):\n",
    "    \"\"\"Normalise chaque ligne de la matrice par sa norme L2.\"\"\"\n",
    "    norms = np.linalg.norm(matrix, axis=1, keepdims=True)  # Calculer la norme L2 pour chaque ligne\n",
    "    normalized_matrix = matrix / norms  # Diviser chaque ligne par sa norme L2\n",
    "    return normalized_matrix\n",
    "\n",
    "# 5. Calculer la distance euclidienne sur la matrice normalisée\n",
    "def calculate_euclidean_distance(matrix):\n",
    "    return squareform(pdist(matrix, metric='euclidean'))\n",
    "\n",
    "# Documents\n",
    "documents = [ph1_, ph2_, ph3_]\n",
    "unique_tokens = preprocess_text(documents)\n",
    "\n",
    "# Normalisation de la matrice par la norme L2\n",
    "normalized_matrix_l2 = normalize_l2(occurrence_matrix)\n",
    "\n",
    "# Calculer la distance euclidienne sur la matrice normalisée (L2)\n",
    "distance_euclidean_l2 = calculate_euclidean_distance(normalized_matrix_l2)\n",
    "\n",
    "# Créer un DataFrame pour afficher les distances\n",
    "distance_euclidean_l2_df = pd.DataFrame(distance_euclidean_l2, \n",
    "                                        columns=['ph1_', 'ph2_', 'ph3_'],\n",
    "                                        index=['ph1_', 'ph2_', 'ph3_'])\n",
    "\n",
    "\n",
    "print(\"\\nMatrice d'Occurrences :\")\n",
    "print(occurrence_matrix)\n",
    "\n",
    "print(\"\\nMatrice Normalisée par la Norme L2 :\")\n",
    "print(normalized_matrix_l2)\n",
    "\n",
    "print(\"\\nMatrice de Distance Euclidienne (Normalisée par la Norme L2) :\")\n",
    "print(distance_euclidean_l2_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a1d42ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Matrice d'Occurrences :\n",
      "[[0 0 4 0 0 0 0 0 4 4 0 4 8]\n",
      " [0 0 0 0 4 0 0 4 0 4 0 4 8]\n",
      " [4 4 0 4 0 4 4 0 0 0 4 0 0]]\n",
      "\n",
      "Matrice Normalisée (Probabilités) :\n",
      "[[0.         0.         0.16666667 0.         0.         0.\n",
      "  0.         0.         0.16666667 0.16666667 0.         0.16666667\n",
      "  0.33333333]\n",
      " [0.         0.         0.         0.         0.16666667 0.\n",
      "  0.         0.16666667 0.         0.16666667 0.         0.16666667\n",
      "  0.33333333]\n",
      " [0.16666667 0.16666667 0.         0.16666667 0.         0.16666667\n",
      "  0.16666667 0.         0.         0.         0.16666667 0.\n",
      "  0.        ]]\n",
      "\n",
      "Matrice de Distance Euclidienne (Normalisée en Probabilités) :\n",
      "          ph1_      ph2_     ph3_\n",
      "ph1_  0.000000  0.333333  0.62361\n",
      "ph2_  0.333333  0.000000  0.62361\n",
      "ph3_  0.623610  0.623610  0.00000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ph1_ = \"The cat sat on the mat. The cat sat on the mat.\"\n",
    "ph2_ = \"The dog sat on the log. The dog sat on the log.\"\n",
    "ph3_ = \"Cats and dogs are great pets.Cats and dogs are great pets.\"\n",
    "\n",
    "# Documents\n",
    "documents = [ph1_, ph2_, ph3_]\n",
    "\n",
    "# Prétraitement : Obtenir les tokens uniques\n",
    "unique_tokens = preprocess_text(documents)\n",
    "\n",
    "# Normalisation de la matrice en probabilités\n",
    "normalized_matrix = normalize_matrix(occurrence_matrix)\n",
    "\n",
    "# Calculer la distance euclidienne\n",
    "distance_euclidean = calculate_euclidean_distance(normalized_matrix)\n",
    "\n",
    "# Créer un DataFrame pour afficher les distances\n",
    "distance_euclidean_df = pd.DataFrame(distance_euclidean, \n",
    "                                     columns=['ph1_', 'ph2_', 'ph3_'],\n",
    "                                     index=['ph1_', 'ph2_', 'ph3_'])\n",
    "\n",
    "# Afficher les résultats\n",
    "print(\"\\nMatrice d'Occurrences :\")\n",
    "print(occurrence_matrix)\n",
    "\n",
    "print(\"\\nMatrice Normalisée (Probabilités) :\")\n",
    "print(normalized_matrix)\n",
    "\n",
    "print(\"\\nMatrice de Distance Euclidienne (Normalisée en Probabilités) :\")\n",
    "print(distance_euclidean_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "46e5c567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Matrice d'Occurrences :\n",
      "[[0 0 4 0 0 0 0 0 4 4 0 4 8]\n",
      " [0 0 0 0 4 0 0 4 0 4 0 4 8]\n",
      " [4 4 0 4 0 4 4 0 0 0 4 0 0]]\n",
      "\n",
      "Matrice Normalisée par la Norme L2 :\n",
      "[[0.         0.         0.35355339 0.         0.         0.\n",
      "  0.         0.         0.35355339 0.35355339 0.         0.35355339\n",
      "  0.70710678]\n",
      " [0.         0.         0.         0.         0.35355339 0.\n",
      "  0.         0.35355339 0.         0.35355339 0.         0.35355339\n",
      "  0.70710678]\n",
      " [0.40824829 0.40824829 0.         0.40824829 0.         0.40824829\n",
      "  0.40824829 0.         0.         0.         0.40824829 0.\n",
      "  0.        ]]\n",
      "\n",
      "Matrice de Distance Euclidienne (Normalisée par la Norme L2) :\n",
      "          ph1_      ph2_      ph3_\n",
      "ph1_  0.000000  0.707107  1.414214\n",
      "ph2_  0.707107  0.000000  1.414214\n",
      "ph3_  1.414214  1.414214  0.000000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Documents\n",
    "documents = [ph1_, ph2_, ph3_]\n",
    "\n",
    "# Prétraitement : Obtenir les tokens uniques\n",
    "unique_tokens = preprocess_text(documents)\n",
    "\n",
    "# Normalisation de la matrice par la norme L2\n",
    "normalized_matrix_l2 = normalize_l2(occurrence_matrix)\n",
    "\n",
    "# Calculer la distance euclidienne sur la matrice normalisée (L2)\n",
    "distance_euclidean_l2 = calculate_euclidean_distance(normalized_matrix_l2)\n",
    "\n",
    "# Créer un DataFrame pour afficher les distances\n",
    "distance_euclidean_l2_df = pd.DataFrame(distance_euclidean_l2, \n",
    "                                        columns=['ph1_', 'ph2_', 'ph3_'],\n",
    "                                        index=['ph1_', 'ph2_', 'ph3_'])\n",
    "\n",
    "# Afficher les résultats\n",
    "print(\"\\nMatrice d'Occurrences :\")\n",
    "print(occurrence_matrix)\n",
    "\n",
    "print(\"\\nMatrice Normalisée par la Norme L2 :\")\n",
    "print(normalized_matrix_l2)\n",
    "\n",
    "print(\"\\nMatrice de Distance Euclidienne (Normalisée par la Norme L2) :\")\n",
    "print(distance_euclidean_l2_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ebe7e49c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Matrice d'Occurrences :\n",
      "[[0 0 4 0 0 0 0 0 4 4 0 4 8]\n",
      " [0 0 0 0 4 0 0 4 0 4 0 4 8]\n",
      " [4 4 0 4 0 4 4 0 0 0 4 0 0]]\n",
      "\n",
      "Matrice Normalisée par la Norme L1 :\n",
      "[[0.         0.         0.16666667 0.         0.         0.\n",
      "  0.         0.         0.16666667 0.16666667 0.         0.16666667\n",
      "  0.33333333]\n",
      " [0.         0.         0.         0.         0.16666667 0.\n",
      "  0.         0.16666667 0.         0.16666667 0.         0.16666667\n",
      "  0.33333333]\n",
      " [0.16666667 0.16666667 0.         0.16666667 0.         0.16666667\n",
      "  0.16666667 0.         0.         0.         0.16666667 0.\n",
      "  0.        ]]\n",
      "\n",
      "Matrice de Distance de Manhattan (Normalisée par la Norme L1) :\n",
      "          ph1_      ph2_  ph3_\n",
      "ph1_  0.000000  0.666667   2.0\n",
      "ph2_  0.666667  0.000000   2.0\n",
      "ph3_  2.000000  2.000000   0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Phrases d'entrée\n",
    "ph1_ = \"The cat sat on the mat. The cat sat on the mat.\"\n",
    "ph2_ = \"The dog sat on the log. The dog sat on the log.\"\n",
    "ph3_ = \"Cats and dogs are great pets. Cats and dogs are great pets.\"\n",
    "\n",
    "\n",
    "# 4. Normalisation par la norme L1\n",
    "def normalize_l1(matrix):\n",
    "    \"\"\"Normalise chaque ligne de la matrice par sa norme L1.\"\"\"\n",
    "    norms = np.sum(np.abs(matrix), axis=1, keepdims=True)  # Calculer la norme L1 pour chaque ligne\n",
    "    normalized_matrix = matrix / norms  # Diviser chaque ligne par sa norme L1\n",
    "    return normalized_matrix\n",
    "\n",
    "# 5. Calculer la distance de Manhattan sur la matrice normalisée\n",
    "def calculate_manhattan_distance(matrix):\n",
    "    return squareform(pdist(matrix, metric='cityblock'))\n",
    "\n",
    "# Documents\n",
    "documents = [ph1_, ph2_, ph3_]\n",
    "\n",
    "# Prétraitement : Obtenir les tokens uniques\n",
    "unique_tokens = preprocess_text(documents)\n",
    "\n",
    "\n",
    "# Normalisation de la matrice par la norme L1\n",
    "normalized_matrix_l1 = normalize_l1(occurrence_matrix)\n",
    "\n",
    "# Calculer la distance de Manhattan sur la matrice normalisée (L1)\n",
    "distance_manhattan_l1 = calculate_manhattan_distance(normalized_matrix_l1)\n",
    "\n",
    "# Créer un DataFrame pour afficher les distances\n",
    "distance_manhattan_l1_df = pd.DataFrame(distance_manhattan_l1, \n",
    "                                        columns=['ph1_', 'ph2_', 'ph3_'],\n",
    "                                        index=['ph1_', 'ph2_', 'ph3_'])\n",
    "\n",
    "# Afficher les résultats\n",
    "print(\"\\nMatrice d'Occurrences :\")\n",
    "print(occurrence_matrix)\n",
    "\n",
    "print(\"\\nMatrice Normalisée par la Norme L1 :\")\n",
    "print(normalized_matrix_l1)\n",
    "\n",
    "print(\"\\nMatrice de Distance de Manhattan (Normalisée par la Norme L1) :\")\n",
    "print(distance_manhattan_l1_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c643c447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Matrice d'Occurrences :\n",
      "[[0 0 1 0 0 0 0 0 1 1 0 1 2]\n",
      " [0 0 0 0 1 0 0 1 0 1 0 1 2]\n",
      " [1 1 0 1 0 1 1 0 0 0 1 0 0]]\n",
      "\n",
      "Matrice Normalisée par la Norme L1 :\n",
      "[[0.         0.         0.16666667 0.         0.         0.\n",
      "  0.         0.         0.16666667 0.16666667 0.         0.16666667\n",
      "  0.33333333]\n",
      " [0.         0.         0.         0.         0.16666667 0.\n",
      "  0.         0.16666667 0.         0.16666667 0.         0.16666667\n",
      "  0.33333333]\n",
      " [0.16666667 0.16666667 0.         0.16666667 0.         0.16666667\n",
      "  0.16666667 0.         0.         0.         0.16666667 0.\n",
      "  0.        ]]\n",
      "\n",
      "Matrice de Distance de Manhattan (Normalisée par la Norme L1) :\n",
      "          ph1_      ph2_  ph3_\n",
      "ph1_  0.000000  0.666667   2.0\n",
      "ph2_  0.666667  0.000000   2.0\n",
      "ph3_  2.000000  2.000000   0.0\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "# Phrases d'entrée\n",
    "ph1_ = \"The cat sat on the mat. \"\n",
    "ph2_ = \"The dog sat on the log. \"\n",
    "ph3_ = \"Cats and dogs are great pets. \"\n",
    "\n",
    "\n",
    "# Documents\n",
    "documents = [ph1_, ph2_, ph3_]\n",
    "\n",
    "# Prétraitement : Obtenir les tokens uniques\n",
    "unique_tokens = preprocess_text(documents)\n",
    "\n",
    "# Normalisation de la matrice par la norme L1\n",
    "normalized_matrix_l1 = normalize_l1(occurrence_matrix)\n",
    "\n",
    "# Calculer la distance de Manhattan sur la matrice normalisée (L1)\n",
    "distance_manhattan_l1 = calculate_manhattan_distance(normalized_matrix_l1)\n",
    "\n",
    "# Créer un DataFrame pour afficher les distances\n",
    "distance_manhattan_l1_df = pd.DataFrame(distance_manhattan_l1, \n",
    "                                        columns=['ph1_', 'ph2_', 'ph3_'],\n",
    "                                        index=['ph1_', 'ph2_', 'ph3_'])\n",
    "\n",
    "# Afficher les résultats\n",
    "print(\"\\nMatrice d'Occurrences :\")\n",
    "print(occurrence_matrix)\n",
    "\n",
    "print(\"\\nMatrice Normalisée par la Norme L1 :\")\n",
    "print(normalized_matrix_l1)\n",
    "\n",
    "print(\"\\nMatrice de Distance de Manhattan (Normalisée par la Norme L1) :\")\n",
    "print(distance_manhattan_l1_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7d5dca4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Matrice d'Occurrences :\n",
      "[[0 0 1 0 0 0 0 0 1 1 0 1 2]\n",
      " [0 0 0 0 1 0 0 1 0 1 0 1 2]\n",
      " [1 1 0 1 0 1 1 0 0 0 1 0 0]]\n",
      "\n",
      "Matrice Normalisée par la Norme L1 :\n",
      "[[0.         0.         0.16666667 0.         0.         0.\n",
      "  0.         0.         0.16666667 0.16666667 0.         0.16666667\n",
      "  0.33333333]\n",
      " [0.         0.         0.         0.         0.16666667 0.\n",
      "  0.         0.16666667 0.         0.16666667 0.         0.16666667\n",
      "  0.33333333]\n",
      " [0.16666667 0.16666667 0.         0.16666667 0.         0.16666667\n",
      "  0.16666667 0.         0.         0.         0.16666667 0.\n",
      "  0.        ]]\n",
      "\n",
      "Matrice de Distance de Manhattan (Normalisée par la Norme L1) :\n",
      "          ph1_      ph2_  ph3_\n",
      "ph1_  0.000000  0.666667   2.0\n",
      "ph2_  0.666667  0.000000   2.0\n",
      "ph3_  2.000000  2.000000   0.0\n"
     ]
    }
   ],
   "source": [
    "ph1_ = \"The cat sat on the mat. The cat sat on the mat.The cat sat on the mat. The cat sat on the mat.\"\n",
    "ph2_ = \"The dog sat on the log. The dog sat on the log.The dog sat on the log. The dog sat on the log.\"\n",
    "ph3_ = \"Cats and dogs are great pets. Cats and dogs are great pets.Cats and dogs are great pets. Cats and dogs are great pets.\"\n",
    "# Documents\n",
    "documents = [ph1_, ph2_, ph3_]\n",
    "\n",
    "# Prétraitement : Obtenir les tokens uniques\n",
    "unique_tokens = preprocess_text(documents)\n",
    "\n",
    "\n",
    "\n",
    "# Normalisation de la matrice par la norme L1\n",
    "normalized_matrix_l1 = normalize_l1(occurrence_matrix)\n",
    "\n",
    "# Calculer la distance de Manhattan sur la matrice normalisée (L1)\n",
    "distance_manhattan_l1 = calculate_manhattan_distance(normalized_matrix_l1)\n",
    "\n",
    "# Créer un DataFrame pour afficher les distances\n",
    "distance_manhattan_l1_df = pd.DataFrame(distance_manhattan_l1, \n",
    "                                        columns=['ph1_', 'ph2_', 'ph3_'],\n",
    "                                        index=['ph1_', 'ph2_', 'ph3_'])\n",
    "\n",
    "# Afficher les résultats\n",
    "print(\"\\nMatrice d'Occurrences :\")\n",
    "print(occurrence_matrix)\n",
    "\n",
    "print(\"\\nMatrice Normalisée par la Norme L1 :\")\n",
    "print(normalized_matrix_l1)\n",
    "\n",
    "print(\"\\nMatrice de Distance de Manhattan (Normalisée par la Norme L1) :\")\n",
    "print(distance_manhattan_l1_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "348cc1c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Matrice d'Occurrences :\n",
      "[[0 0 1 0 0 0 0 0 1 1 0 1 2]\n",
      " [0 0 0 0 1 0 0 1 0 1 0 1 2]\n",
      " [1 1 0 1 0 1 1 0 0 0 1 0 0]]\n",
      "\n",
      "Matrice Normalisée (Probabilités) :\n",
      "[[0.         0.         0.16666667 0.         0.         0.\n",
      "  0.         0.         0.16666667 0.16666667 0.         0.16666667\n",
      "  0.33333333]\n",
      " [0.         0.         0.         0.         0.16666667 0.\n",
      "  0.         0.16666667 0.         0.16666667 0.         0.16666667\n",
      "  0.33333333]\n",
      " [0.16666667 0.16666667 0.         0.16666667 0.         0.16666667\n",
      "  0.16666667 0.         0.         0.         0.16666667 0.\n",
      "  0.        ]]\n",
      "\n",
      "Matrice de Distance Euclidienne (Normalisée en Probabilités) :\n",
      "          ph1_      ph2_     ph3_\n",
      "ph1_  0.000000  0.333333  0.62361\n",
      "ph2_  0.333333  0.000000  0.62361\n",
      "ph3_  0.623610  0.623610  0.00000\n"
     ]
    }
   ],
   "source": [
    "# Documents\n",
    "documents = [ph1_, ph2_, ph3_]\n",
    "\n",
    "# Prétraitement : Obtenir les tokens uniques\n",
    "unique_tokens = preprocess_text(documents)\n",
    "\n",
    "\n",
    "# Normalisation de la matrice en probabilités\n",
    "normalized_matrix = normalize_matrix(occurrence_matrix)\n",
    "\n",
    "# Calculer la distance euclidienne\n",
    "distance_euclidean = calculate_euclidean_distance(normalized_matrix)\n",
    "\n",
    "# Créer un DataFrame pour afficher les distances\n",
    "distance_euclidean_df = pd.DataFrame(distance_euclidean, \n",
    "                                     columns=['ph1_', 'ph2_', 'ph3_'],\n",
    "                                     index=['ph1_', 'ph2_', 'ph3_'])\n",
    "\n",
    "# Afficher les résultats\n",
    "print(\"\\nMatrice d'Occurrences :\")\n",
    "print(occurrence_matrix)\n",
    "\n",
    "print(\"\\nMatrice Normalisée (Probabilités) :\")\n",
    "print(normalized_matrix)\n",
    "\n",
    "print(\"\\nMatrice de Distance Euclidienne (Normalisée en Probabilités) :\")\n",
    "print(distance_euclidean_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "863a8c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Matrice d'Occurrences :\n",
      "[[0 0 1 0 0 0 0 0 1 1 0 1 2]\n",
      " [0 0 0 0 1 0 0 1 0 1 0 1 2]\n",
      " [1 1 0 1 0 1 1 0 0 0 1 0 0]]\n",
      "\n",
      "Matrice Normalisée par la Norme L2 :\n",
      "[[0.         0.         0.35355339 0.         0.         0.\n",
      "  0.         0.         0.35355339 0.35355339 0.         0.35355339\n",
      "  0.70710678]\n",
      " [0.         0.         0.         0.         0.35355339 0.\n",
      "  0.         0.35355339 0.         0.35355339 0.         0.35355339\n",
      "  0.70710678]\n",
      " [0.40824829 0.40824829 0.         0.40824829 0.         0.40824829\n",
      "  0.40824829 0.         0.         0.         0.40824829 0.\n",
      "  0.        ]]\n",
      "\n",
      "Matrice de Distance Euclidienne (Normalisée par la Norme L2) :\n",
      "          ph1_      ph2_      ph3_\n",
      "ph1_  0.000000  0.707107  1.414214\n",
      "ph2_  0.707107  0.000000  1.414214\n",
      "ph3_  1.414214  1.414214  0.000000\n"
     ]
    }
   ],
   "source": [
    "# Documents\n",
    "documents = [ph1_, ph2_, ph3_]\n",
    "\n",
    "# Prétraitement : Obtenir les tokens uniques\n",
    "unique_tokens = preprocess_text(documents)\n",
    "\n",
    "\n",
    "# Normalisation de la matrice par la norme L2\n",
    "normalized_matrix_l2 = normalize_l2(occurrence_matrix)\n",
    "\n",
    "# Calculer la distance euclidienne sur la matrice normalisée (L2)\n",
    "distance_euclidean_l2 = calculate_euclidean_distance(normalized_matrix_l2)\n",
    "\n",
    "# Créer un DataFrame pour afficher les distances\n",
    "distance_euclidean_l2_df = pd.DataFrame(distance_euclidean_l2, \n",
    "                                        columns=['ph1_', 'ph2_', 'ph3_'],\n",
    "                                        index=['ph1_', 'ph2_', 'ph3_'])\n",
    "\n",
    "# Afficher les résultats\n",
    "print(\"\\nMatrice d'Occurrences :\")\n",
    "print(occurrence_matrix)\n",
    "\n",
    "print(\"\\nMatrice Normalisée par la Norme L2 :\")\n",
    "print(normalized_matrix_l2)\n",
    "\n",
    "print(\"\\nMatrice de Distance Euclidienne (Normalisée par la Norme L2) :\")\n",
    "print(distance_euclidean_l2_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d775a086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Matrice d'Occurrences :\n",
      "[[0 0 1 0 0 0 0 0 1 1 0 1 2]\n",
      " [0 0 0 0 1 0 0 1 0 1 0 1 2]\n",
      " [1 1 0 1 0 1 1 0 0 0 1 0 0]]\n",
      "\n",
      "Matrice Normalisée par la Norme L2 :\n",
      "[[0.         0.         0.35355339 0.         0.         0.\n",
      "  0.         0.         0.35355339 0.35355339 0.         0.35355339\n",
      "  0.70710678]\n",
      " [0.         0.         0.         0.         0.35355339 0.\n",
      "  0.         0.35355339 0.         0.35355339 0.         0.35355339\n",
      "  0.70710678]\n",
      " [0.40824829 0.40824829 0.         0.40824829 0.         0.40824829\n",
      "  0.40824829 0.         0.         0.         0.40824829 0.\n",
      "  0.        ]]\n",
      "\n",
      "Matrice de Distance de Manhattan (Normalisée par la Norme L2) :\n",
      "          ph1_      ph2_     ph3_\n",
      "ph1_  0.000000  1.414214  4.57081\n",
      "ph2_  1.414214  0.000000  4.57081\n",
      "ph3_  4.570810  4.570810  0.00000\n"
     ]
    }
   ],
   "source": [
    "# Documents\n",
    "documents = [ph1_, ph2_, ph3_]\n",
    "\n",
    "# Prétraitement : Obtenir les tokens uniques\n",
    "unique_tokens = preprocess_text(documents)\n",
    "\n",
    "\n",
    "\n",
    "# Normalisation de la matrice par la norme L2\n",
    "normalized_matrix_l2 = normalize_l2(occurrence_matrix)\n",
    "\n",
    "# Calculer la distance de Manhattan sur la matrice normalisée (L2)\n",
    "distance_manhattan_l2 = calculate_manhattan_distance(normalized_matrix_l2)\n",
    "\n",
    "# Créer un DataFrame pour afficher les distances\n",
    "distance_manhattan_l2_df = pd.DataFrame(distance_manhattan_l2, \n",
    "                                        columns=['ph1_', 'ph2_', 'ph3_'],\n",
    "                                        index=['ph1_', 'ph2_', 'ph3_'])\n",
    "\n",
    "# Afficher les résultats\n",
    "print(\"\\nMatrice d'Occurrences :\")\n",
    "print(occurrence_matrix)\n",
    "\n",
    "print(\"\\nMatrice Normalisée par la Norme L2 :\")\n",
    "print(normalized_matrix_l2)\n",
    "\n",
    "print(\"\\nMatrice de Distance de Manhattan (Normalisée par la Norme L2) :\")\n",
    "print(distance_manhattan_l2_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4197d264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Matrice de Distance de Cosinus (Normalisation par Probabilité) :\n",
      "      ph1_  ph2_  ph3_\n",
      "ph1_  0.00  0.25   1.0\n",
      "ph2_  0.25  0.00   1.0\n",
      "ph3_  1.00  1.00   0.0\n",
      "\n",
      "Matrice de Distance de Cosinus (Normalisation par la Norme L2) :\n",
      "      ph1_  ph2_  ph3_\n",
      "ph1_  0.00  0.25   1.0\n",
      "ph2_  0.25  0.00   1.0\n",
      "ph3_  1.00  1.00   0.0\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "# Phrases d'entrée\n",
    "ph1_ = \"The cat sat on the mat. \"\n",
    "ph2_ = \"The dog sat on the log.\"\n",
    "ph3_ = \"Cats and dogs are great pets. \"\n",
    "\n",
    "# 1. Découper les phrases en mots\n",
    "def split_into_words(text):\n",
    "    \"\"\"Divise le texte en mots en supprimant la ponctuation et en utilisant des espaces.\"\"\"\n",
    "    return re.findall(r'\\b\\w+\\b', text.lower())\n",
    "\n",
    "# 2. Prétraitement du texte pour obtenir les mots uniques\n",
    "def preprocess_text(documents):\n",
    "    unique_tokens = set()\n",
    "    for doc in documents:\n",
    "        tokens = split_into_words(doc)\n",
    "        unique_tokens.update(tokens)\n",
    "    return sorted(unique_tokens)  # Retourne une liste triée des mots uniques\n",
    "\n",
    "# 3. Créer la matrice d'occurrences\n",
    "def create_occurrence_matrix(documents, unique_tokens):\n",
    "    occurrence_matrix = []\n",
    "    \n",
    "    for doc in documents:\n",
    "        tokens = split_into_words(doc)\n",
    "        occurrence_row = [tokens.count(token) for token in unique_tokens]\n",
    "        occurrence_matrix.append(occurrence_row)\n",
    "\n",
    "    return np.array(occurrence_matrix)\n",
    "\n",
    "# 4. Normalisation par la probabilité (probabilité normale)\n",
    "def normalize_probability(matrix):\n",
    "    \"\"\"Normalise la matrice d'occurrences en transformant les fréquences en probabilités.\"\"\"\n",
    "    row_sums = matrix.sum(axis=1, keepdims=True)  # Calculer la somme des occurrences pour chaque document\n",
    "    probability_matrix = matrix / row_sums  # Diviser chaque élément par la somme de la ligne correspondante\n",
    "    return probability_matrix\n",
    "\n",
    "# 5. Normalisation par la norme L2\n",
    "def normalize_l2(matrix):\n",
    "    \"\"\"Normalise chaque ligne de la matrice par sa norme L2.\"\"\"\n",
    "    norms = np.linalg.norm(matrix, ord=2, axis=1, keepdims=True)  # Calculer la norme L2 pour chaque ligne\n",
    "    normalized_matrix = matrix / norms  # Diviser chaque ligne par sa norme L2\n",
    "    return normalized_matrix\n",
    "\n",
    "# 6. Calculer la distance de Cosinus\n",
    "def calculate_cosine_distance(matrix):\n",
    "    \"\"\"Calcule la distance de Cosinus entre les documents.\"\"\"\n",
    "    return squareform(pdist(matrix, metric='cosine'))\n",
    "\n",
    "# Documents\n",
    "documents = [ph1_, ph2_, ph3_]\n",
    "\n",
    "# Prétraitement : Obtenir les tokens uniques\n",
    "unique_tokens = preprocess_text(documents)\n",
    "\n",
    "# Créer la matrice d'occurrences\n",
    "occurrence_matrix = create_occurrence_matrix(documents, unique_tokens)\n",
    "\n",
    "# 7. Normalisation par la probabilité (normalisation en probabilités)\n",
    "probability_matrix = normalize_probability(occurrence_matrix)\n",
    "\n",
    "# 8. Normalisation par la norme L2\n",
    "normalized_matrix_l2 = normalize_l2(occurrence_matrix)\n",
    "\n",
    "# 9. Calcul des distances de Cosinus\n",
    "distance_cosine_probability = calculate_cosine_distance(probability_matrix)\n",
    "distance_cosine_l2 = calculate_cosine_distance(normalized_matrix_l2)\n",
    "\n",
    "# Créer des DataFrames pour afficher les distances\n",
    "distance_cosine_probability_df = pd.DataFrame(distance_cosine_probability, \n",
    "                                              columns=['ph1_', 'ph2_', 'ph3_'],\n",
    "                                              index=['ph1_', 'ph2_', 'ph3_'])\n",
    "\n",
    "distance_cosine_l2_df = pd.DataFrame(distance_cosine_l2, \n",
    "                                      columns=['ph1_', 'ph2_', 'ph3_'],\n",
    "                                      index=['ph1_', 'ph2_', 'ph3_'])\n",
    "\n",
    "# Afficher les résultats\n",
    "print(\"\\nMatrice de Distance de Cosinus (Normalisation par Probabilité) :\")\n",
    "print(distance_cosine_probability_df)\n",
    "\n",
    "print(\"\\nMatrice de Distance de Cosinus (Normalisation par la Norme L2) :\")\n",
    "print(distance_cosine_l2_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e1ba1dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Distances pour la matrice d'occurrence (probabilité) ---\n",
      "Normalisation : manhattan\n",
      "          ph1_      ph2_      ph3_\n",
      "ph1_  0.000000  1.733333  1.700000\n",
      "ph2_  1.733333  0.000000  1.722222\n",
      "ph3_  1.700000  1.722222  0.000000\n",
      "Normalisation : euclidean\n",
      "          ph1_      ph2_      ph3_\n",
      "ph1_  0.000000  0.295804  0.299176\n",
      "ph2_  0.295804  0.000000  0.303020\n",
      "ph3_  0.299176  0.303020  0.000000\n",
      "Normalisation : cosine\n",
      "          ph1_      ph2_      ph3_\n",
      "ph1_  0.000000  0.776393  0.800000\n",
      "ph2_  0.776393  0.000000  0.739125\n",
      "ph3_  0.800000  0.739125  0.000000\n",
      "--- Distances pour la matrice d'occurrence (norme L2) ---\n",
      "Normalisation : manhattan\n",
      "          ph1_      ph2_      ph3_\n",
      "ph1_  0.000000  7.358256  7.155418\n",
      "ph2_  7.358256  0.000000  6.911042\n",
      "ph3_  7.155418  6.911042  0.000000\n",
      "Normalisation : euclidean\n",
      "          ph1_      ph2_      ph3_\n",
      "ph1_  0.000000  1.246109  1.264911\n",
      "ph2_  1.246109  0.000000  1.215833\n",
      "ph3_  1.264911  1.215833  0.000000\n",
      "Normalisation : cosine\n",
      "          ph1_      ph2_      ph3_\n",
      "ph1_  0.000000  0.776393  0.800000\n",
      "ph2_  0.776393  0.000000  0.739125\n",
      "ph3_  0.800000  0.739125  0.000000\n",
      "--- Distances pour la matrice binaire (probabilité) ---\n",
      "Normalisation : manhattan\n",
      "          ph1_      ph2_      ph3_\n",
      "ph1_  0.000000  1.714286  1.700000\n",
      "ph2_  1.714286  0.000000  1.714286\n",
      "ph3_  1.700000  1.714286  0.000000\n",
      "Normalisation : euclidean\n",
      "          ph1_      ph2_      ph3_\n",
      "ph1_  0.000000  0.288675  0.301954\n",
      "ph2_  0.288675  0.000000  0.299392\n",
      "ph3_  0.301954  0.299392  0.000000\n",
      "Normalisation : cosine\n",
      "          ph1_      ph2_      ph3_\n",
      "ph1_  0.000000  0.853615  0.837302\n",
      "ph2_  0.853615  0.000000  0.841223\n",
      "ph3_  0.837302  0.841223  0.000000\n",
      "--- Distances pour la matrice binaire (norme L2) ---\n",
      "Normalisation : manhattan\n",
      "          ph1_      ph2_      ph3_\n",
      "ph1_  0.000000  7.745404  7.253601\n",
      "ph2_  7.745404  0.000000  7.396374\n",
      "ph3_  7.253601  7.396374  0.000000\n",
      "Normalisation : euclidean\n",
      "          ph1_      ph2_      ph3_\n",
      "ph1_  0.000000  1.306610  1.294065\n",
      "ph2_  1.306610  0.000000  1.297091\n",
      "ph3_  1.294065  1.297091  0.000000\n",
      "Normalisation : cosine\n",
      "          ph1_      ph2_      ph3_\n",
      "ph1_  0.000000  0.853615  0.837302\n",
      "ph2_  0.853615  0.000000  0.841223\n",
      "ph3_  0.837302  0.841223  0.000000\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "# Trois premières phrases du discours de Jacques Chirac\n",
    "ph1_ = \"La confiance que vous venez de me témoigner, je veux y répondre en m'engageant dans l'action avec détermination.\"\n",
    "ph2_ = \"Mes chers compatriotes de métropole, d'outre-mer et de l'étranger,Nous venons de vivre un temps de grave inquiétude pour la Nation.\"\n",
    "ph3_=\"Mais ce soir, dans un grand élan la France a réaffirmé son attachement aux valeurs de la République.\"\n",
    "\n",
    "# 1. Découper les phrases en mots\n",
    "def split_into_words(text):\n",
    "    \"\"\"Divise le texte en mots en supprimant la ponctuation et en utilisant des espaces.\"\"\"\n",
    "    return re.findall(r'\\b\\w+\\b', text.lower())\n",
    "\n",
    "# 2. Prétraitement du texte pour obtenir les mots uniques\n",
    "def preprocess_text(documents):\n",
    "    unique_tokens = set()\n",
    "    for doc in documents:\n",
    "        tokens = split_into_words(doc)\n",
    "        unique_tokens.update(tokens)\n",
    "    return sorted(unique_tokens)  # Retourne une liste triée des mots uniques\n",
    "\n",
    "# 3. Créer la matrice d'occurrences\n",
    "def create_occurrence_matrix(documents, unique_tokens):\n",
    "    occurrence_matrix = []\n",
    "    \n",
    "    for doc in documents:\n",
    "        tokens = split_into_words(doc)\n",
    "        occurrence_row = [tokens.count(token) for token in unique_tokens]\n",
    "        occurrence_matrix.append(occurrence_row)\n",
    "\n",
    "    return np.array(occurrence_matrix)\n",
    "\n",
    "# 4. Créer la matrice binaire\n",
    "def create_binary_matrix(documents, unique_tokens):\n",
    "    binary_matrix = []\n",
    "    \n",
    "    for doc in documents:\n",
    "        tokens = split_into_words(doc)\n",
    "        binary_row = [1 if token in tokens else 0 for token in unique_tokens]\n",
    "        binary_matrix.append(binary_row)\n",
    "\n",
    "    return np.array(binary_matrix)\n",
    "\n",
    "# 5. Normalisation par la probabilité\n",
    "def normalize_probability(matrix):\n",
    "    row_sums = matrix.sum(axis=1, keepdims=True)\n",
    "    probability_matrix = matrix / row_sums\n",
    "    return probability_matrix\n",
    "\n",
    "# 6. Normalisation par la norme L2\n",
    "def normalize_l2(matrix):\n",
    "    norms = np.linalg.norm(matrix, ord=2, axis=1, keepdims=True)\n",
    "    normalized_matrix = matrix / norms\n",
    "    return normalized_matrix\n",
    "\n",
    "# 7. Calculer les distances (Manhattan, Euclidienne, Cosinus)\n",
    "def calculate_distances(matrix, metric):\n",
    "    return squareform(pdist(matrix, metric=metric))\n",
    "\n",
    "# Documents\n",
    "documents = [ph1_, ph2_, ph3_]\n",
    "\n",
    "# Prétraitement : Obtenir les tokens uniques\n",
    "unique_tokens = preprocess_text(documents)\n",
    "\n",
    "# Créer les matrices d'occurrence et binaire\n",
    "occurrence_matrix = create_occurrence_matrix(documents, unique_tokens)\n",
    "binary_matrix = create_binary_matrix(documents, unique_tokens)\n",
    "\n",
    "# Normalisation\n",
    "occurrence_prob_matrix = normalize_probability(occurrence_matrix)\n",
    "occurrence_l2_matrix = normalize_l2(occurrence_matrix)\n",
    "\n",
    "binary_prob_matrix = normalize_probability(binary_matrix)\n",
    "binary_l2_matrix = normalize_l2(binary_matrix)\n",
    "\n",
    "# Calcul des distances\n",
    "distances = {}\n",
    "\n",
    "# Distances pour matrice d'occurrence normalisée par probabilité et norme L2\n",
    "distances['occurrence_prob'] = {\n",
    "    'manhattan': calculate_distances(occurrence_prob_matrix, 'cityblock'),\n",
    "    'euclidean': calculate_distances(occurrence_prob_matrix, 'euclidean'),\n",
    "    'cosine': calculate_distances(occurrence_prob_matrix, 'cosine')\n",
    "}\n",
    "distances['occurrence_l2'] = {\n",
    "    'manhattan': calculate_distances(occurrence_l2_matrix, 'cityblock'),\n",
    "    'euclidean': calculate_distances(occurrence_l2_matrix, 'euclidean'),\n",
    "    'cosine': calculate_distances(occurrence_l2_matrix, 'cosine')\n",
    "}\n",
    "\n",
    "# Distances pour matrice binaire normalisée par probabilité et norme L2\n",
    "distances['binary_prob'] = {\n",
    "    'manhattan': calculate_distances(binary_prob_matrix, 'cityblock'),\n",
    "    'euclidean': calculate_distances(binary_prob_matrix, 'euclidean'),\n",
    "    'cosine': calculate_distances(binary_prob_matrix, 'cosine')\n",
    "}\n",
    "distances['binary_l2'] = {\n",
    "    'manhattan': calculate_distances(binary_l2_matrix, 'cityblock'),\n",
    "    'euclidean': calculate_distances(binary_l2_matrix, 'euclidean'),\n",
    "    'cosine': calculate_distances(binary_l2_matrix, 'cosine')\n",
    "}\n",
    "\n",
    "def print_distance_matrices(distances, matrix_type):\n",
    "    print(f\"--- Distances pour la matrice {matrix_type} ---\")\n",
    "    for normalization, matrix in distances.items():  # Si `distances` est un dictionnaire de matrices\n",
    "        if isinstance(matrix, dict):  # Vérifie si `matrix` est un dictionnaire avec types de distances\n",
    "            print(f\"Normalisation : {normalization}\")\n",
    "            for dist_type, mat in matrix.items():\n",
    "                df = pd.DataFrame(mat, columns=['ph1_', 'ph2_', 'ph3_'], index=['ph1_', 'ph2_', 'ph3_'])\n",
    "                print(f\"Distance {dist_type.capitalize()} :\")\n",
    "                print(df)\n",
    "        else:\n",
    "            # Si la mtrice n'est pas un dictionnaire\n",
    "            df = pd.DataFrame(matrix, columns=['ph1_', 'ph2_', 'ph3_'], index=['ph1_', 'ph2_', 'ph3_'])\n",
    "            print(f\"Normalisation : {normalization}\")\n",
    "            print(df)\n",
    "\n",
    "# Affichage des distances\n",
    "print_distance_matrices(distances['occurrence_prob'], \"d'occurrence (probabilité)\")\n",
    "print_distance_matrices(distances['occurrence_l2'], \"d'occurrence (norme L2)\")\n",
    "print_distance_matrices(distances['binary_prob'], \"binaire (probabilité)\")\n",
    "print_distance_matrices(distances['binary_l2'], \"binaire (norme L2)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6dc683f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF Binaire:\n",
      "    and  are  cat  cats  dog  dogs  great  log  mat  on  pets  sat  the\n",
      "D1    0    0    1     0    0     0      0    0    1   1     0    1    1\n",
      "D2    0    0    0     0    1     0      0    1    0   1     0    1    1\n",
      "D3    1    1    0     1    0     1      1    0    0   0     1    0    0\n",
      "\n",
      "TF Occurrence:\n",
      "    and  are  cat  cats  dog  dogs  great  log  mat  on  pets  sat  the\n",
      "D1    0    0    1     0    0     0      0    0    1   1     0    1    2\n",
      "D2    0    0    0     0    1     0      0    1    0   1     0    1    2\n",
      "D3    1    1    0     1    0     1      1    0    0   0     1    0    0\n",
      "\n",
      "TF Occurrence Normalisé:\n",
      "         and       are       cat      cats       dog      dogs     great  \\\n",
      "D1  0.000000  0.000000  0.166667  0.000000  0.000000  0.000000  0.000000   \n",
      "D2  0.000000  0.000000  0.000000  0.000000  0.166667  0.000000  0.000000   \n",
      "D3  0.166667  0.166667  0.000000  0.166667  0.000000  0.166667  0.166667   \n",
      "\n",
      "         log       mat        on      pets       sat       the  \n",
      "D1  0.000000  0.166667  0.166667  0.000000  0.166667  0.333333  \n",
      "D2  0.166667  0.000000  0.166667  0.000000  0.166667  0.333333  \n",
      "D3  0.000000  0.000000  0.000000  0.166667  0.000000  0.000000  \n",
      "\n",
      "Document Frequency (DF):\n",
      "and      1\n",
      "are      1\n",
      "cat      1\n",
      "cats     1\n",
      "dog      1\n",
      "dogs     1\n",
      "great    1\n",
      "log      1\n",
      "mat      1\n",
      "on       2\n",
      "pets     1\n",
      "sat      2\n",
      "the      2\n",
      "dtype: int64\n",
      "\n",
      "IDF (log10) pour chaque terme:\n",
      "and      0.477121\n",
      "are      0.477121\n",
      "cat      0.477121\n",
      "cats     0.477121\n",
      "dog      0.477121\n",
      "dogs     0.477121\n",
      "great    0.477121\n",
      "log      0.477121\n",
      "mat      0.477121\n",
      "on       0.176091\n",
      "pets     0.477121\n",
      "sat      0.176091\n",
      "the      0.176091\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from math import log10\n",
    "\n",
    "# Corpus de documents\n",
    "documents = [\n",
    "    \"The cat sat on the mat.\",\n",
    "    \"The dog sat on the log.\",\n",
    "    \"Cats and dogs are great pets.\"\n",
    "]\n",
    "\n",
    "# Prétraitement du texte\n",
    "vectorizer = CountVectorizer()  # Utiliser sans 'binary=True' pour avoir les occurrences\n",
    "X_occ = vectorizer.fit_transform(documents)\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "\n",
    "# TF binaire\n",
    "vectorizer_binary = CountVectorizer(binary=True)  # Pour TF binaire\n",
    "X_binary = vectorizer_binary.fit_transform(documents)\n",
    "tf_binary = pd.DataFrame(X_binary.toarray(), columns=terms, index=['D1', 'D2', 'D3'])\n",
    "\n",
    "# TF occurrence\n",
    "tf_occ = pd.DataFrame(X_occ.toarray(), columns=terms, index=['D1', 'D2', 'D3'])\n",
    "\n",
    "# TF occurrence normalisé (diviser chaque terme par le total des termes dans le document)\n",
    "tf_occ_normalized = tf_occ.div(tf_occ.sum(axis=1), axis=0)\n",
    "\n",
    "# Calcul de DF (Document Frequency) : Nombre de documents contenant chaque terme\n",
    "df = (tf_occ > 0).sum(axis=0)\n",
    "\n",
    "# Calcul de IDF (Inverse Document Frequency) avec log10\n",
    "N = len(documents)  # Nombre total de documents\n",
    "idf = np.log10(N / df)\n",
    "\n",
    "# Affichage des matrices\n",
    "print(\"TF Binaire:\")\n",
    "print(tf_binary)\n",
    "print(\"\\nTF Occurrence:\")\n",
    "print(tf_occ)\n",
    "print(\"\\nTF Occurrence Normalisé:\")\n",
    "print(tf_occ_normalized)\n",
    "print(\"\\nDocument Frequency (DF):\")\n",
    "print(df)\n",
    "print(\"\\nIDF (log10) pour chaque terme:\")\n",
    "print(idf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7d77f86c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Binaire:\n",
      "         and       are       cat      cats       dog      dogs     great  \\\n",
      "D1  0.000000  0.000000  0.477121  0.000000  0.000000  0.000000  0.000000   \n",
      "D2  0.000000  0.000000  0.000000  0.000000  0.477121  0.000000  0.000000   \n",
      "D3  0.477121  0.477121  0.000000  0.477121  0.000000  0.477121  0.477121   \n",
      "\n",
      "         log       mat        on      pets       sat       the  \n",
      "D1  0.000000  0.477121  0.176091  0.000000  0.176091  0.176091  \n",
      "D2  0.477121  0.000000  0.176091  0.000000  0.176091  0.176091  \n",
      "D3  0.000000  0.000000  0.000000  0.477121  0.000000  0.000000  \n",
      "\n",
      "TF-IDF Occurrence:\n",
      "         and       are       cat      cats       dog      dogs     great  \\\n",
      "D1  0.000000  0.000000  0.477121  0.000000  0.000000  0.000000  0.000000   \n",
      "D2  0.000000  0.000000  0.000000  0.000000  0.477121  0.000000  0.000000   \n",
      "D3  0.477121  0.477121  0.000000  0.477121  0.000000  0.477121  0.477121   \n",
      "\n",
      "         log       mat        on      pets       sat       the  \n",
      "D1  0.000000  0.477121  0.176091  0.000000  0.176091  0.352183  \n",
      "D2  0.477121  0.000000  0.176091  0.000000  0.176091  0.352183  \n",
      "D3  0.000000  0.000000  0.000000  0.477121  0.000000  0.000000  \n",
      "\n",
      "TF-IDF Occurrence Normalisé:\n",
      "        and      are      cat     cats      dog     dogs    great      log  \\\n",
      "D1  0.00000  0.00000  0.07952  0.00000  0.00000  0.00000  0.00000  0.00000   \n",
      "D2  0.00000  0.00000  0.00000  0.00000  0.07952  0.00000  0.00000  0.07952   \n",
      "D3  0.07952  0.07952  0.00000  0.07952  0.00000  0.07952  0.07952  0.00000   \n",
      "\n",
      "        mat        on     pets       sat       the  \n",
      "D1  0.07952  0.029349  0.00000  0.029349  0.058697  \n",
      "D2  0.00000  0.029349  0.00000  0.029349  0.058697  \n",
      "D3  0.00000  0.000000  0.07952  0.000000  0.000000  \n"
     ]
    }
   ],
   "source": [
    "# Calcul des matrices TF-IDF\n",
    "tfidf_binary = tf_binary * idf.values\n",
    "tfidf_occ = tf_occ * idf.valuese\n",
    "tfidf_occ_normalized = tf_occ_normalized * idf.values\n",
    "\n",
    "# Affichage des matrices TF-IDF\n",
    "print(\"TF-IDF Binaire:\")\n",
    "print(pd.DataFrame(tfidf_binary, columns=terms, index=['D1', 'D2', 'D3']))\n",
    "print(\"\\nTF-IDF Occurrence:\")\n",
    "print(pd.DataFrame(tfidf_occ, columns=terms, index=['D1', 'D2', 'D3']))\n",
    "print(\"\\nTF-IDF Occurrence Normalisé:\")\n",
    "print(pd.DataFrame(tfidf_occ_normalized, columns=terms, index=['D1', 'D2', 'D3']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a832432d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Distance L1 (Binaire):\n",
      "[[0.         1.90848502 4.34524381]\n",
      " [1.90848502 0.         4.34524381]\n",
      " [4.34524381 4.34524381 0.        ]]\n",
      "\n",
      "Distance L1 (Occurrence):\n",
      "[[0.         1.90848502 4.52133507]\n",
      " [1.90848502 0.         4.52133507]\n",
      " [4.52133507 4.52133507 0.        ]]\n",
      "\n",
      "Distance L1 (Occurrence Normalisé):\n",
      "[[0.         0.31808084 0.75355585]\n",
      " [0.31808084 0.         0.75355585]\n",
      " [0.75355585 0.75355585 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# Distance L1\n",
    "distance_l1_binary = cdist(tfidf_binary, tfidf_binary, metric='cityblock')\n",
    "distance_l1_occ = cdist(tfidf_occ, tfidf_occ, metric='cityblock')\n",
    "distance_l1_occ_normalized = cdist(tfidf_occ_normalized, tfidf_occ_normalized, metric='cityblock')\n",
    "\n",
    "print(\"\\nDistance L1 (Binaire):\")\n",
    "print(distance_l1_binary)\n",
    "\n",
    "print(\"\\nDistance L1 (Occurrence):\")\n",
    "print(distance_l1_occ)\n",
    "\n",
    "print(\"\\nDistance L1 (Occurrence Normalisé):\")\n",
    "print(distance_l1_occ_normalized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7c712ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Distance L2 (Binaire):\n",
      "[[0.         0.95424251 1.38353964]\n",
      " [0.95424251 0.         1.38353964]\n",
      " [1.38353964 1.38353964 0.        ]]\n",
      "\n",
      "Distance L2 (Occurrence):\n",
      "[[0.         0.95424251 1.41675909]\n",
      " [0.95424251 0.         1.41675909]\n",
      " [1.41675909 1.41675909 0.        ]]\n",
      "\n",
      "Distance L2 (Occurrence Normalisé):\n",
      "[[0.         0.15904042 0.23612652]\n",
      " [0.15904042 0.         0.23612652]\n",
      " [0.23612652 0.23612652 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Distance L2\n",
    "distance_l2_binary = cdist(tfidf_binary, tfidf_binary, metric='euclidean')\n",
    "distance_l2_occ = cdist(tfidf_occ, tfidf_occ, metric='euclidean')\n",
    "distance_l2_occ_normalized = cdist(tfidf_occ_normalized, tfidf_occ_normalized, metric='euclidean')\n",
    "\n",
    "print(\"\\nDistance L2 (Binaire):\")\n",
    "print(distance_l2_binary)\n",
    "\n",
    "print(\"\\nDistance L2 (Occurrence):\")\n",
    "print(distance_l2_occ)\n",
    "\n",
    "print(\"\\nDistance L2 (Occurrence Normalisé):\")\n",
    "print(distance_l2_occ_normalized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1454f288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Distance Cosinus (Binaire):\n",
      "[[0.00000000e+00 8.30344598e-01 1.00000000e+00]\n",
      " [8.30344598e-01 0.00000000e+00 1.00000000e+00]\n",
      " [1.00000000e+00 1.00000000e+00 1.11022302e-16]]\n",
      "\n",
      "Distance Cosinus (Occurrence):\n",
      "[[0.00000000e+00 7.09905324e-01 1.00000000e+00]\n",
      " [7.09905324e-01 0.00000000e+00 1.00000000e+00]\n",
      " [1.00000000e+00 1.00000000e+00 1.11022302e-16]]\n",
      "\n",
      "Distance Cosinus (Occurrence Normalisé):\n",
      "[[0.         0.70990532 1.        ]\n",
      " [0.70990532 0.         1.        ]\n",
      " [1.         1.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Distance Cosinus\n",
    "distance_cosine_binary = cdist(tfidf_binary, tfidf_binary, metric='cosine')\n",
    "distance_cosine_occ = cdist(tfidf_occ, tfidf_occ, metric='cosine')\n",
    "distance_cosine_occ_normalized = cdist(tfidf_occ_normalized, tfidf_occ_normalized, metric='cosine')\n",
    "\n",
    "print(\"\\nDistance Cosinus (Binaire):\")\n",
    "print(distance_cosine_binary)\n",
    "\n",
    "print(\"\\nDistance Cosinus (Occurrence):\")\n",
    "print(distance_cosine_occ)\n",
    "\n",
    "print(\"\\nDistance Cosinus (Occurrence Normalisé):\")\n",
    "print(distance_cosine_occ_normalized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7f3a8e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Distance Bray-Curtis (Binaire):\n",
      "[[0.        0.6436641 1.       ]\n",
      " [0.6436641 0.        1.       ]\n",
      " [1.        1.        0.       ]]\n",
      "\n",
      "Distance Bray-Curtis (Occurrence):\n",
      "[[0.         0.57532749 1.        ]\n",
      " [0.57532749 0.         1.        ]\n",
      " [1.         1.         0.        ]]\n",
      "\n",
      "Distance Bray-Curtis (Occurrence Normalisé):\n",
      "[[0.         0.57532749 1.        ]\n",
      " [0.57532749 0.         1.        ]\n",
      " [1.         1.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Distance Bray-Curtis\n",
    "distance_bray_curtis_binary = cdist(tfidf_binary, tfidf_binary, metric='braycurtis')\n",
    "distance_bray_curtis_occ = cdist(tfidf_occ, tfidf_occ, metric='braycurtis')\n",
    "distance_bray_curtis_occ_normalized = cdist(tfidf_occ_normalized, tfidf_occ_normalized, metric='braycurtis')\n",
    "\n",
    "print(\"\\nDistance Bray-Curtis (Binaire):\")\n",
    "print(distance_bray_curtis_binary)\n",
    "\n",
    "print(\"\\nDistance Bray-Curtis (Occurrence):\")\n",
    "print(distance_bray_curtis_occ)\n",
    "\n",
    "print(\"\\nDistance Bray-Curtis (Occurrence Normalisé):\")\n",
    "print(distance_bray_curtis_occ_normalized)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
